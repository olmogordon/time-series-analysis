{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "937b93d3-097e-470b-aa18-0f3774caa9c1",
   "metadata": {},
   "source": [
    "# Hierarchical Dynamic Factor Model for GDP Forecasting: Implementation Plan  \n",
    "\n",
    "## 1. Architecture Overview  \n",
    "\n",
    "The architecture implements a hierarchical forecasting system that integrates information from multiple time frequencies to predict quarterly GDP growth. The system processes data through a cascade of Dynamic Factor Models (DFMs) that successively integrate information from higher to lower frequencies:  \n",
    "  \n",
    "- Daily Data → Weekly Data → Monthly Data → Quarterly GDP  \n",
    "  \n",
    "We will implement two versions of the model:  \n",
    "  \n",
    "1. Pure Hierarchical: Using direct factor weighting for the final GDP prediction  \n",
    "2. Hierarchical + MIDAS: Using Mixed-Data Sampling for optimal temporal weighting at the final stage  \n",
    "  \n",
    "### Key Components  \n",
    "  \n",
    "#### 1. Multi-frequency Data Processing Pipeline  \n",
    "  \n",
    "- Specialized preprocessing for daily, weekly, monthly, and quarterly data  \n",
    "- Feature engineering with technical indicators (SMA, RSI, ROC)  \n",
    "- Temporal alignment mechanism between frequency domains  \n",
    "  \n",
    "#### 2. Hierarchical Dynamic Factor Model System  \n",
    "  \n",
    "- Series of DFMs that extract factors at each frequency level  \n",
    "- Autoregressive modeling of factors at each stage  \n",
    "- Information flow from higher to lower frequencies  \n",
    "  \n",
    "#### 3. GDP Prediction Module  \n",
    "  \n",
    "- Pure hierarchical version with direct factor weighting  \n",
    "- Hybrid version with MIDAS-style weighting  \n",
    "- Autoregressive modeling of GDP growth  \n",
    "  \n",
    "#### 4. Evaluation and Diagnostic Framework  \n",
    "  \n",
    "- Out-of-sample forecasting performance evaluation  \n",
    "- Comparative analysis of hierarchical vs. hybrid approaches  \n",
    "- Economic interpretation of factor contributions  \n",
    "\n",
    "\n",
    "\n",
    "## 2. Data Processing Framework  \n",
    "  \n",
    "### Daily Data Processing  \n",
    "  \n",
    "- Preprocessing of 7 daily time series (Oil, Gold, Copper/Gold, Lumber/Gold, S&P 500, 10Y-03M, 10Y-02Y)  \n",
    "- Technical indicators: SMA(5,20,60,200), RSI(14,21), ROC(1,5,20,60)  \n",
    "- Enhanced metrics for each technical indicator  \n",
    "- Dimension reduction through dynamic factor model  \n",
    "  \n",
    "### Weekly Data Processing  \n",
    "  \n",
    "- Preprocessing of 2 weekly series (Unemployment Claims, Financial Conditions)  \n",
    "- Technical indicators: SMA(4,12,26), RSI(8), ROC(1,4,12,26)  \n",
    "- Integration with daily factors  \n",
    "- Dimension reduction through dynamic factor model  \n",
    "  \n",
    "### Monthly Data Processing  \n",
    "  \n",
    "- Preprocessing of 7 monthly series (CPI, Unemployment, Interest Rates, Housing, etc.)  \n",
    "- Technical indicators: SMA(3,6,12), RSI(6), ROC(1,3,6,12)  \n",
    "- Integration with weekly factors  \n",
    "- Dimension reduction through dynamic factor model\n",
    "  \n",
    "### Quarterly Data Processing  \n",
    "  \n",
    "- Preprocessing of GDP growth series  \n",
    "- Autoregressive modeling  \n",
    "- Integration with monthly factors  \n",
    "- Final prediction model  \n",
    "  \n",
    "## 3. Temporal Alignment Strategy  \n",
    "The system uses a state space alignment strategy for connecting factors across frequencies:  \n",
    "  \n",
    "1. Factor Projection Method: Uses the DFM's state evolution equations to project factors to required observation dates  \n",
    "2. End-of-Period Anchoring: Anchors alignment to release dates of economic data  \n",
    "3. Information Flow Control: Ensures no future information leaks into the model  \n",
    "  \n",
    "This approach is based on research by Bańbura et al. (2011) showing 15-25% improvement in forecast accuracy compared to simple aggregation methods.  \n",
    "\n",
    "## 4. Dynamic Factor Model Implementation\n",
    "Each DFM will:  \n",
    "  \n",
    "1. Extract common factors from input data  \n",
    "2. Model the autoregressive structure of factors  \n",
    "3. Pass both factor values and their dynamics to the next level  \n",
    "  \n",
    "The DFM implementation follows Doz, Giannone, and Reichlin (2011), using:  \n",
    "  \n",
    "- EM algorithm for parameter estimation  \n",
    "- Kalman filtering for optimal factor extraction  \n",
    "- State space representation for dynamic modeling  \n",
    "  \n",
    "## 5. MIDAS Implementation (Hybrid Version)  \n",
    "The MIDAS component:  \n",
    "  \n",
    "1. Optimally weights monthly factors for quarterly GDP prediction  \n",
    "2. Uses exponential Almon lag polynomial weighting functions  \n",
    "3. Combines with autoregressive GDP components  \n",
    "  \n",
    "This approach is based on Ghysels et al. (2004) showing superior performance for mixed-frequency forecasting.  \n",
    "  \n",
    "## 6. Measurement and Evaluation  \n",
    "The system evaluation includes:  \n",
    "  \n",
    "1. Out-of-sample forecast accuracy metrics (RMSE, MAE)  \n",
    "2. Rolling window forecast evaluation  \n",
    "3. Comparative analysis of both model versions  \n",
    "4. Statistical significance testing of forecast improvements    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7ecf1d-1df5-4a50-993e-d10f0b9ae039",
   "metadata": {},
   "source": [
    "# Module 1: Enhanced Data Preprocessor for Multi-Frequency Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aefb3cce-a679-4fac-86b3-75bbbee064aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import copy\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "\n",
    "class MultiFrequencyPreprocessor:\n",
    "    \"\"\"\n",
    "    Enhanced data preprocessor for multi-frequency economic data.\n",
    "    \n",
    "    This class handles different time frequencies (daily, weekly, monthly, quarterly)\n",
    "    and ensures proper alignment and processing for hierarchical modeling.\n",
    "    \n",
    "    Attributes:\n",
    "        data_folder (str): Path to the folder containing CSV files\n",
    "        available_files (list): List of available CSV files\n",
    "        data_config (dict): Configuration for data loading and preprocessing\n",
    "        frequency_data (dict): Dictionary containing data for each frequency\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_folder):\n",
    "        \"\"\"\n",
    "        Initialize the MultiFrequencyPreprocessor with the folder containing CSV files.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_folder: str\n",
    "            Path to the folder containing CSV files\n",
    "        \"\"\"\n",
    "        self.data_folder = data_folder\n",
    "        self.available_files = self._get_available_files()\n",
    "        self.data_config = {}\n",
    "        self.frequency_data = {\n",
    "            'daily': None,\n",
    "            'weekly': None,\n",
    "            'monthly': None, \n",
    "            'quarterly': None\n",
    "        }\n",
    "        self.start_date = None\n",
    "        self.end_date = None\n",
    "        \n",
    "        # Dictionaries to store processed data and factors\n",
    "        self.processed_data = {}\n",
    "        self.factors = {}\n",
    "        \n",
    "        print(f\"Found {len(self.available_files)} files in {data_folder}\")\n",
    "        \n",
    "    def _get_available_files(self):\n",
    "        \"\"\"List all CSV files in the data folder.\"\"\"\n",
    "        # Normalize path to handle both forward and backward slashes\n",
    "        norm_path = os.path.normpath(self.data_folder)\n",
    "        files = glob.glob(os.path.join(norm_path, '*.csv'))\n",
    "        return [os.path.basename(f) for f in files]\n",
    "    \n",
    "    def set_config(self, data_config):\n",
    "        \"\"\"\n",
    "        Set the configuration for data loading and preprocessing.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_config: dict\n",
    "            Configuration dictionary with the following structure:\n",
    "            {\n",
    "                'daily': {\n",
    "                    'file_name.csv': {\n",
    "                        'columns': ['column1'],\n",
    "                        'transformations': {'column1': 'pct_change'},\n",
    "                        'start_date': '1980-01-01'  # Optional\n",
    "                    },\n",
    "                    ...\n",
    "                },\n",
    "                'weekly': {...},\n",
    "                'monthly': {...},\n",
    "                'quarterly': {...}\n",
    "            }\n",
    "        \"\"\"\n",
    "        self.data_config = data_config\n",
    "        \n",
    "    def set_date_range(self, start_date=None, end_date=None):\n",
    "        \"\"\"\n",
    "        Set the global date range for data processing.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        start_date: str or datetime\n",
    "            Start date for data processing (format: 'YYYY-MM-DD')\n",
    "        end_date: str or datetime\n",
    "            End date for data processing (format: 'YYYY-MM-DD')\n",
    "        \"\"\"\n",
    "        if start_date:\n",
    "            self.start_date = pd.to_datetime(start_date) if isinstance(start_date, str) else start_date\n",
    "        if end_date:\n",
    "            self.end_date = pd.to_datetime(end_date) if isinstance(end_date, str) else end_date\n",
    "    \n",
    "    def _load_csv(self, file_name, frequency):\n",
    "        \"\"\"\n",
    "        Load a CSV file and parse the date column.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        file_name: str\n",
    "            Name of the CSV file\n",
    "        frequency: str\n",
    "            Data frequency ('daily', 'weekly', 'monthly', 'quarterly')\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            Loaded dataframe with date index\n",
    "        \"\"\"\n",
    "        # Normalize path\n",
    "        norm_path = os.path.normpath(self.data_folder)\n",
    "        file_path = os.path.join(norm_path, file_name)\n",
    "        \n",
    "        try:\n",
    "            # First try standard CSV loading\n",
    "            df = pd.read_csv(file_path, parse_dates=[0], index_col=0)\n",
    "            \n",
    "            # Check if index is datetime\n",
    "            if not pd.api.types.is_datetime64_any_dtype(df.index):\n",
    "                # Convert index to datetime\n",
    "                df.index = pd.to_datetime(df.index)\n",
    "                \n",
    "            # Apply frequency-specific processing\n",
    "            if frequency == 'daily':\n",
    "                # For daily data, ensure the index is business days\n",
    "                df = df.asfreq('B', method='ffill')\n",
    "            elif frequency == 'weekly':\n",
    "                # For weekly data, use end of week\n",
    "                df = df.asfreq('W-FRI', method='ffill')\n",
    "            elif frequency == 'monthly':\n",
    "                # For monthly data, use end of month\n",
    "                df = df.asfreq('M', method='ffill')\n",
    "            elif frequency == 'quarterly':\n",
    "                # For quarterly data, use end of quarter\n",
    "                df = df.asfreq('Q', method='ffill')\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_name}: {e}\")\n",
    "            \n",
    "            # Try alternative approach\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                date_col = df.columns[0]\n",
    "                \n",
    "                # Try different date formats\n",
    "                try:\n",
    "                    df[date_col] = pd.to_datetime(df[date_col])\n",
    "                except:\n",
    "                    for date_format in ['%Y-%m-%d', '%d/%m/%Y', '%m/%d/%Y', '%Y/%m/%d']:\n",
    "                        try:\n",
    "                            df[date_col] = pd.to_datetime(df[date_col], format=date_format)\n",
    "                            break\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                \n",
    "                df.set_index(date_col, inplace=True)\n",
    "                \n",
    "                # Apply frequency-specific processing\n",
    "                if frequency == 'daily':\n",
    "                    df = df.asfreq('B', method='ffill')\n",
    "                elif frequency == 'weekly':\n",
    "                    df = df.asfreq('W-FRI', method='ffill')\n",
    "                elif frequency == 'monthly':\n",
    "                    df = df.asfreq('M', method='ffill')\n",
    "                elif frequency == 'quarterly':\n",
    "                    df = df.asfreq('Q', method='ffill')\n",
    "                \n",
    "                return df\n",
    "                \n",
    "            except Exception as nested_e:\n",
    "                print(f\"Failed to load {file_name} after multiple attempts: {nested_e}\")\n",
    "                raise\n",
    "    \n",
    "    def _apply_transformation(self, df, column, transformation):\n",
    "        \"\"\"\n",
    "        Apply the specified transformation to a column.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df: pd.DataFrame\n",
    "            DataFrame containing the column\n",
    "        column: str\n",
    "            Column name to transform\n",
    "        transformation: str or list\n",
    "            Transformation type ('raw', 'pct_change', 'log_return', 'diff') or list\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list of tuples\n",
    "            List of (column_name, transformed_series) tuples\n",
    "        \"\"\"\n",
    "        if column not in df.columns:\n",
    "            print(f\"Warning: Column {column} not found in DataFrame\")\n",
    "            return []\n",
    "        \n",
    "        # Handle list of transformations\n",
    "        if isinstance(transformation, list):\n",
    "            result = []\n",
    "            for t in transformation:\n",
    "                column_name = f\"{column}_{t}\"\n",
    "                series = self._apply_single_transformation(df, column, t)\n",
    "                result.append((column_name, series))\n",
    "            return result\n",
    "        else:\n",
    "            # Handle single transformation\n",
    "            column_name = f\"{column}_{transformation}\" if transformation != 'raw' else column\n",
    "            series = self._apply_single_transformation(df, column, transformation)\n",
    "            return [(column_name, series)]\n",
    "    \n",
    "    def _apply_single_transformation(self, df, column, transformation):\n",
    "        \"\"\"\n",
    "        Apply a single transformation to a column with robust handling of edge cases.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df: pd.DataFrame\n",
    "            DataFrame containing the column\n",
    "        column: str\n",
    "            Column name to transform\n",
    "        transformation: str\n",
    "            Transformation type ('raw', 'pct_change', 'log_return', 'diff', 'yoy')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.Series\n",
    "            Transformed series\n",
    "        \n",
    "        References:\n",
    "        -----------\n",
    "        - Stock, J. H., & Watson, M. W. (2002). Forecasting using principal components from a\n",
    "        large number of predictors. Journal of the American Statistical Association, 97, 1167-1179.\n",
    "        - Marcellino, M., Stock, J. H., & Watson, M. W. (2003). Macroeconomic forecasting in the\n",
    "        euro area: Country specific versus area-wide information. European Economic Review, 47, 1-18.\n",
    "        \"\"\"\n",
    "        if transformation == 'raw':\n",
    "            return df[column]\n",
    "        elif transformation == 'pct_change':\n",
    "            # Calculate percentage change with correct usage\n",
    "            # First ffill to avoid warning about fill_method\n",
    "            pct = df[column].ffill().pct_change() * 100\n",
    "            # Fill first value with 0 for continuity\n",
    "            if len(pct) > 0:\n",
    "                pct.iloc[0] = 0\n",
    "            return pct\n",
    "        elif transformation == 'log_return':\n",
    "            # Calculate log return (continuously compounded return)\n",
    "            # Log returns provide better statistical properties for economic forecasting\n",
    "            # Reference: Campbell, J. Y., Lo, A. W., & MacKinlay, A. C. (1997). The Econometrics of Financial Markets.\n",
    "            log_ret = (np.log(df[column]) - np.log(df[column].shift(1))) * 100\n",
    "            # Fill first value with 0 for continuity\n",
    "            if len(log_ret) > 0:\n",
    "                log_ret.iloc[0] = 0\n",
    "            return log_ret\n",
    "        elif transformation == 'diff':\n",
    "            # Calculate first difference\n",
    "            diff = df[column].diff()\n",
    "            # Fill first value with 0 for continuity\n",
    "            if len(diff) > 0:\n",
    "                diff.iloc[0] = 0\n",
    "            return diff\n",
    "        elif transformation == 'yoy':\n",
    "            # Calculate year-over-year percentage change\n",
    "            # Reference: Zarnowitz, V., & Ozyildirim, A. (2006). Time series decomposition and measurement\n",
    "            # of business cycles, trends and growth cycles. Journal of Monetary Economics, 53, 1717-1739.\n",
    "            yoy = df[column].ffill().pct_change(periods=12) * 100\n",
    "            # Forward fill NaN values\n",
    "            yoy = yoy.ffill()\n",
    "            return yoy\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown transformation: {transformation}\")\n",
    "\n",
    "    def _calculate_ratios(self, data_dict, ratio_config):\n",
    "        \"\"\"\n",
    "        Calculate financial ratios from base time series.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_dict: dict\n",
    "            Dictionary of DataFrames\n",
    "        ratio_config: dict\n",
    "            Configuration for ratio calculation\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary with ratio DataFrames added\n",
    "        \"\"\"\n",
    "        result_dict = data_dict.copy()\n",
    "        \n",
    "        for ratio_name, config in ratio_config.items():\n",
    "            try:\n",
    "                numerator_key = config['numerator']\n",
    "                denominator_key = config['denominator']\n",
    "                transformations = config.get('transformations', ['raw'])\n",
    "                \n",
    "                # Get the component series\n",
    "                if numerator_key in data_dict and denominator_key in data_dict:\n",
    "                    numerator = data_dict[numerator_key].iloc[:, 0]  # Assume first column\n",
    "                    denominator = data_dict[denominator_key].iloc[:, 0]  # Assume first column\n",
    "                    \n",
    "                    # Calculate the ratio\n",
    "                    ratio = numerator / denominator\n",
    "                    ratio_df = pd.DataFrame({f\"{ratio_name}_raw\": ratio})\n",
    "                    \n",
    "                    # Apply transformations\n",
    "                    for transform in transformations:\n",
    "                        if transform != 'raw':\n",
    "                            transformed_series = self._apply_single_transformation(ratio_df, f\"{ratio_name}_raw\", transform)\n",
    "                            ratio_df[f\"{ratio_name}_{transform}\"] = transformed_series\n",
    "                    \n",
    "                    # Add to result\n",
    "                    result_dict[ratio_name] = ratio_df\n",
    "                    print(f\"Created ratio: {ratio_name} with {len(ratio_df)} observations\")\n",
    "                else:\n",
    "                    print(f\"Warning: Could not create ratio {ratio_name}. Missing component series.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating ratio {ratio_name}: {e}\")\n",
    "                \n",
    "        return result_dict\n",
    "                \n",
    "    def process_frequency_data(self, frequency):\n",
    "        \"\"\"\n",
    "        Process data for a specific frequency.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        frequency: str\n",
    "            Data frequency ('daily', 'weekly', 'monthly', 'quarterly')\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            Processed DataFrame for the specified frequency\n",
    "        \"\"\"\n",
    "        if frequency not in self.data_config:\n",
    "            print(f\"No configuration found for {frequency} data\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Processing {frequency} data...\")\n",
    "        freq_config = self.data_config[frequency]\n",
    "        \n",
    "        # Load and transform individual files\n",
    "        data_dict = {}\n",
    "        for file_name, config in freq_config.get('files', {}).items():\n",
    "            if file_name not in self.available_files:\n",
    "                print(f\"Warning: {file_name} not found, skipping\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Load CSV file\n",
    "                df = self._load_csv(file_name, frequency)\n",
    "                \n",
    "                # Apply date filtering if specified\n",
    "                if 'start_date' in config:\n",
    "                    df = df[df.index >= pd.to_datetime(config['start_date'])]\n",
    "                elif self.start_date:\n",
    "                    df = df[df.index >= self.start_date]\n",
    "                \n",
    "                if self.end_date:\n",
    "                    df = df[df.index <= self.end_date]\n",
    "                \n",
    "                # Apply transformations\n",
    "                transformed_columns = []\n",
    "                for column in config['columns']:\n",
    "                    # Get transformation type\n",
    "                    transformation = config['transformations'].get(column, 'raw')\n",
    "                    # Apply transformation\n",
    "                    results = self._apply_transformation(df, column, transformation)\n",
    "                    # Store results\n",
    "                    for col_name, series in results:\n",
    "                        # Create descriptive name: filename_column_transformation\n",
    "                        file_prefix = file_name.split('.')[0] # Remove extension\n",
    "                        prefixed_name = f\"{file_prefix}_{col_name}\"\n",
    "                        transformed_columns.append((prefixed_name, series))\n",
    "                \n",
    "                # Create DataFrame from transformed columns\n",
    "                if transformed_columns:\n",
    "                    processed_df = pd.DataFrame({name: series for name, series in transformed_columns})\n",
    "                    processed_df.index = df.index\n",
    "                    # Store in data dictionary\n",
    "                    key = file_name.split('.')[0] # Use filename without extension\n",
    "                    data_dict[key] = processed_df\n",
    "                    print(f\"Processed {file_name}: {len(processed_df)} observations, {len(processed_df.columns)} features\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        # Calculate ratios if configured\n",
    "        if 'ratios' in freq_config:\n",
    "            data_dict = self._calculate_ratios(data_dict, freq_config['ratios'])\n",
    "        \n",
    "        # Merge all DataFrames\n",
    "        if data_dict:\n",
    "            merged_df = None\n",
    "            for _, df in data_dict.items():\n",
    "                if merged_df is None:\n",
    "                    merged_df = df.copy()\n",
    "                else:\n",
    "                    merged_df = merged_df.join(df, how='outer')\n",
    "            \n",
    "            # Handle missing values\n",
    "            if merged_df is not None:\n",
    "                # Forward fill for continuity (using proper method)\n",
    "                merged_df = merged_df.ffill()\n",
    "                # Then backward fill any remaining NaNs at the beginning\n",
    "                merged_df = merged_df.bfill()\n",
    "                \n",
    "                # Store in processed data dictionary\n",
    "                self.processed_data[frequency] = merged_df\n",
    "                print(f\"Final {frequency} dataset: {len(merged_df)} observations, {len(merged_df.columns)} features\")\n",
    "                return merged_df\n",
    "            else:\n",
    "                print(f\"No valid data found for {frequency} frequency\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"No data processed for {frequency} frequency\")\n",
    "            return None\n",
    "    \n",
    "    def process_all_frequencies(self):\n",
    "        \"\"\"\n",
    "        Process data for all configured frequencies.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of processed DataFrames for each frequency\n",
    "        \"\"\"\n",
    "        for frequency in self.data_config.keys():\n",
    "            self.process_frequency_data(frequency)\n",
    "        \n",
    "        return self.processed_data\n",
    "    \n",
    "    def align_to_dates(self, source_df, target_dates, method='last'):\n",
    "        \"\"\"\n",
    "        Align source DataFrame to target dates using specified method.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        source_df: pd.DataFrame\n",
    "            Source DataFrame to align\n",
    "        target_dates: pd.DatetimeIndex\n",
    "            Target dates to align to\n",
    "        method: str\n",
    "            Method for alignment ('last', 'nearest', 'linear')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            Aligned DataFrame\n",
    "        \n",
    "        References:\n",
    "        -----------\n",
    "        - Ghysels, E., Santa-Clara, P., & Valkanov, R. (2004). The MIDAS touch: Mixed data \n",
    "          sampling regression models. Working paper, University of North Carolina.\n",
    "        - Marcellino, M., & Schumacher, C. (2010). Factor MIDAS for nowcasting and forecasting \n",
    "          with ragged-edge data: A model comparison for German GDP. Oxford Bulletin of Economics \n",
    "          and Statistics, 72, 518-550.\n",
    "        \"\"\"\n",
    "        # Initialize aligned DataFrame with the same columns as source_df\n",
    "        aligned_df = pd.DataFrame(index=target_dates, columns=source_df.columns)\n",
    "        \n",
    "        if method == 'last':\n",
    "            # For each target date, find the last available observation\n",
    "            for date in target_dates:\n",
    "                prev_data = source_df[source_df.index <= date]\n",
    "                if not prev_data.empty:\n",
    "                    # Get the last row as a Series and assign values column by column\n",
    "                    last_row = prev_data.iloc[-1]\n",
    "                    for col in source_df.columns:\n",
    "                        aligned_df.loc[date, col] = last_row[col]\n",
    "        \n",
    "        elif method == 'nearest':\n",
    "            # For each target date, find the nearest observation\n",
    "            for date in target_dates:\n",
    "                # Calculate absolute difference in days\n",
    "                source_dates = source_df.index\n",
    "                if len(source_dates) > 0:\n",
    "                    # Convert to numpy arrays for vectorized operations\n",
    "                    days_diff = np.abs((source_dates - date).days.values)\n",
    "                    nearest_idx = np.argmin(days_diff)\n",
    "                    # Assign values column by column\n",
    "                    nearest_row = source_df.iloc[nearest_idx]\n",
    "                    for col in source_df.columns:\n",
    "                        aligned_df.loc[date, col] = nearest_row[col]\n",
    "        \n",
    "        elif method == 'linear':\n",
    "            # This method can be implemented directly with pandas reindex\n",
    "            aligned_df = source_df.reindex(index=sorted(list(source_df.index) + list(target_dates)))\n",
    "            # Apply linear interpolation\n",
    "            aligned_df = aligned_df.interpolate(method='linear')\n",
    "            # Extract only the target dates\n",
    "            aligned_df = aligned_df.reindex(target_dates)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown alignment method: {method}\")\n",
    "        \n",
    "        # Handle any remaining NaNs by forward filling, then backward filling\n",
    "        aligned_df = aligned_df.ffill().bfill()\n",
    "        \n",
    "        return aligned_df\n",
    "    \n",
    "    def generate_hierarchical_dataset(self, target_frequency='quarterly'):\n",
    "        \"\"\"\n",
    "        Generate hierarchical dataset with higher-frequency data aligned to lower frequency.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        target_frequency: str\n",
    "            Target frequency for alignment ('quarterly', 'monthly', 'weekly')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of aligned datasets for hierarchical modeling\n",
    "        \"\"\"\n",
    "        hierarchical_data = {}\n",
    "        \n",
    "        # Define frequency hierarchy\n",
    "        freq_hierarchy = {\n",
    "            'quarterly': ['monthly', 'weekly', 'daily'],\n",
    "            'monthly': ['weekly', 'daily'],\n",
    "            'weekly': ['daily']\n",
    "        }\n",
    "        \n",
    "        # Get target dates\n",
    "        if target_frequency not in self.processed_data:\n",
    "            raise ValueError(f\"No processed data found for {target_frequency} frequency\")\n",
    "            \n",
    "        target_dates = self.processed_data[target_frequency].index\n",
    "        hierarchical_data[target_frequency] = self.processed_data[target_frequency]\n",
    "        \n",
    "        # Align higher frequency data to target dates\n",
    "        for higher_freq in freq_hierarchy.get(target_frequency, []):\n",
    "            if higher_freq in self.processed_data:\n",
    "                aligned_df = self.align_to_dates(\n",
    "                    self.processed_data[higher_freq], \n",
    "                    target_dates,\n",
    "                    method='last'  # Use last available observation\n",
    "                )\n",
    "                hierarchical_data[f\"{higher_freq}_aligned\"] = aligned_df\n",
    "        \n",
    "        return hierarchical_data\n",
    "    \n",
    "    def plot_data_overview(self, frequency=None):\n",
    "        \"\"\"\n",
    "        Plot an overview of the processed data to help with visualization.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        frequency: str or None\n",
    "            Frequency to plot, or None to plot all\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            Matplotlib figure object\n",
    "        \"\"\"\n",
    "        if frequency:\n",
    "            if frequency not in self.processed_data:\n",
    "                raise ValueError(f\"No processed data found for {frequency} frequency\")\n",
    "            frequencies = [frequency]\n",
    "        else:\n",
    "            frequencies = list(self.processed_data.keys())\n",
    "        \n",
    "        n_freqs = len(frequencies)\n",
    "        fig, axes = plt.subplots(n_freqs, 1, figsize=(15, 6*n_freqs))\n",
    "        \n",
    "        if n_freqs == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, freq in enumerate(frequencies):\n",
    "            df = self.processed_data[freq]\n",
    "            \n",
    "            # Select a subset of columns if there are too many\n",
    "            max_cols = 10\n",
    "            if len(df.columns) > max_cols:\n",
    "                # Choose evenly spaced columns\n",
    "                indices = np.linspace(0, len(df.columns)-1, max_cols, dtype=int)\n",
    "                plot_cols = [df.columns[i] for i in indices]\n",
    "            else:\n",
    "                plot_cols = df.columns\n",
    "            \n",
    "            # Plot each column\n",
    "            for col in plot_cols:\n",
    "                axes[i].plot(df.index, df[col], label=col)\n",
    "            \n",
    "            axes[i].set_title(f\"{freq.capitalize()} Data Overview\")\n",
    "            axes[i].set_xlabel('Date')\n",
    "            axes[i].set_ylabel('Value')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            axes[i].legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7313098b-f5f6-42f6-8876-935d0ed64349",
   "metadata": {},
   "source": [
    "# Module 2: Technical Indicators for Multi-Frequency Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ac6aae3-f3b9-4a85-98a1-9d8bb5b4f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiFrequencyTechnicalIndicators:\n",
    "    \"\"\"\n",
    "    Technical indicators calculation for multi-frequency economic data.\n",
    "    \n",
    "    This class implements SMA, RSI, and ROC with frequency-appropriate parameters\n",
    "    and enhanced metrics for economic time series.\n",
    "    \n",
    "    References:\n",
    "    -----------\n",
    "    - Neely, C. J., Rapach, D. E., Tu, J., & Zhou, G. (2014). Forecasting the equity risk premium: \n",
    "      The role of technical indicators. Management Science, 60(7), 1772-1791.\n",
    "    - Brock, W., Lakonishok, J., & LeBaron, B. (1992). Simple Technical Trading Rules and the \n",
    "      Stochastic Properties of Stock Returns. Journal of Finance, 47(5), 1731-1764.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_frequency_params(frequency):\n",
    "        \"\"\"\n",
    "        Get appropriate technical indicator parameters for each frequency.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        frequency: str\n",
    "            Data frequency ('daily', 'weekly', 'monthly', 'quarterly')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of parameters for each indicator type\n",
    "            \n",
    "        References:\n",
    "        -----------\n",
    "        - Neely, C. J., Rapach, D. E., Tu, J., & Zhou, G. (2014). Forecasting the equity risk premium: \n",
    "          The role of technical indicators. Management Science, 60(7), 1772-1791.\n",
    "        - Brock, W., Lakonishok, J., & LeBaron, B. (1992). Simple Technical Trading Rules and the \n",
    "          Stochastic Properties of Stock Returns. Journal of Finance, 47(5), 1731-1764.\n",
    "        \"\"\"\n",
    "        if frequency == 'daily':\n",
    "            # For daily data - standard financial parameters\n",
    "            # Reference: Brock et al. (1992), Neely et al. (2014)\n",
    "            return {\n",
    "                'sma': [5, 20, 60, 200],  # Short, medium, quarter, year\n",
    "                'rsi': [14, 21],          # Standard and extended\n",
    "                'roc': [1, 5, 20, 60]     # Daily, weekly, monthly, quarter\n",
    "            }\n",
    "        elif frequency == 'weekly':\n",
    "            # For weekly data - adjusted to weekly scale\n",
    "            # Reference: Marshall et al. (2008)\n",
    "            return {\n",
    "                'sma': [4, 12, 26, 52],   # Month, quarter, half-year, year\n",
    "                'rsi': [8, 12],           # ~1.5-2 months\n",
    "                'roc': [1, 4, 13, 26]     # Week, month, quarter, half-year\n",
    "            }\n",
    "        elif frequency == 'monthly':\n",
    "            # For monthly data - adjusted to monthly scale\n",
    "            # Reference: Fama & French (1988)\n",
    "            return {\n",
    "                'sma': [3, 6, 12, 24],    # Quarter, half-year, year, two years\n",
    "                'rsi': [6, 9],            # Half-year, three quarters\n",
    "                'roc': [1, 3, 6, 12]      # Month, quarter, half-year, year\n",
    "            }\n",
    "        elif frequency == 'quarterly':\n",
    "            # For quarterly data - adjusted to quarterly scale\n",
    "            # Reference: Stock & Watson (2002)\n",
    "            return {\n",
    "                'sma': [2, 4, 8, 12],     # Half-year, year, two years, three years\n",
    "                'rsi': [4, 6],            # Year, year and half\n",
    "                'roc': [1, 2, 4, 8]       # Quarter, half-year, year, two years\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown frequency: {frequency}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _calculate_trend_direction(series, periods=1):\n",
    "        \"\"\"\n",
    "        Calculate trend direction for a series with proper handling of zeros and NaNs.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        series: pandas.Series\n",
    "            Series to calculate trend direction for\n",
    "        periods: int\n",
    "            Number of periods to look back\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.Series\n",
    "            Series containing trend direction values:\n",
    "            1 for rising, -1 for falling, 0 for no change\n",
    "        \"\"\"\n",
    "        # Calculate direction safely\n",
    "        diff = series.diff(periods)\n",
    "        \n",
    "        # Initialize direction series\n",
    "        direction = pd.Series(0, index=series.index)\n",
    "        \n",
    "        # Positive direction\n",
    "        direction[diff > 0] = 1\n",
    "        \n",
    "        # Negative direction\n",
    "        direction[diff < 0] = -1\n",
    "        \n",
    "        # For zero-diff values, carry forward previous direction to avoid flicker\n",
    "        # but only where series values are valid\n",
    "        zero_mask = (diff == 0) & series.notna()\n",
    "        if zero_mask.any():\n",
    "            # Forward-fill only zero-diff positions\n",
    "            direction_filled = direction.copy()\n",
    "            direction_filled[zero_mask] = np.nan\n",
    "            direction_filled = direction_filled.ffill()\n",
    "            \n",
    "            # Update direction where diff was zero\n",
    "            direction[zero_mask] = direction_filled[zero_mask]\n",
    "            \n",
    "        return direction\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_sma(df, column, windows=None, include_trend=True, include_crossovers=True):\n",
    "        \"\"\"\n",
    "        Calculate Simple Moving Averages with enhanced metrics.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df: pandas.DataFrame\n",
    "            DataFrame containing the data\n",
    "        column: str\n",
    "            Column name to calculate SMA for\n",
    "        windows: list\n",
    "            List of window sizes for SMA calculation\n",
    "        include_trend: bool\n",
    "            Whether to include trend direction\n",
    "        include_crossovers: bool\n",
    "            Whether to include crossover signals\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with SMA values and enhanced metrics\n",
    "            \n",
    "        References:\n",
    "        -----------\n",
    "        - Brock, W., Lakonishok, J., & LeBaron, B. (1992). Simple Technical Trading Rules and \n",
    "          the Stochastic Properties of Stock Returns. Journal of Finance, 47(5), 1731-1764.\n",
    "        - Neely, C. J., Rapach, D. E., Tu, J., & Zhou, G. (2014). Forecasting the equity risk \n",
    "          premium: The role of technical indicators. Management Science, 60(7), 1772-1791.\n",
    "        \"\"\"\n",
    "        if windows is None:\n",
    "            # Default parameters - will be overridden by frequency-specific ones\n",
    "            windows = [5, 20, 60, 200]\n",
    "        \n",
    "        result_df = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # Calculate SMAs for each window\n",
    "        for window in windows:\n",
    "            # Calculate SMA with proper min_periods\n",
    "            min_periods = max(1, window // 4)\n",
    "            sma = df[column].rolling(window=window, min_periods=min_periods).mean()\n",
    "            sma_name = f\"{column}_SMA_{window}\"\n",
    "            result_df[sma_name] = sma\n",
    "            \n",
    "            # Calculate percentage difference from SMA\n",
    "            valid_mask = (sma != 0) & sma.notna() & df[column].notna()\n",
    "            pct_diff = pd.Series(index=df.index, dtype=float)\n",
    "            pct_diff[valid_mask] = (df[column][valid_mask] - sma[valid_mask]) / sma[valid_mask] * 100\n",
    "            result_df[f\"{sma_name}_pct_diff\"] = pct_diff\n",
    "            \n",
    "            # Calculate trend if requested\n",
    "            if include_trend:\n",
    "                trend = MultiFrequencyTechnicalIndicators._calculate_trend_direction(sma)\n",
    "                result_df[f\"{sma_name}_trend\"] = trend\n",
    "        \n",
    "        # Calculate crossovers if requested and we have at least two windows\n",
    "        if include_crossovers and len(windows) >= 2:\n",
    "            # Sort windows to ensure correct fast/slow designation\n",
    "            sorted_windows = sorted(windows)\n",
    "            \n",
    "            # Calculate crossovers between adjacent SMAs\n",
    "            for i in range(len(sorted_windows) - 1):\n",
    "                fast_window = sorted_windows[i]\n",
    "                slow_window = sorted_windows[i+1]\n",
    "                \n",
    "                fast_sma = result_df[f\"{column}_SMA_{fast_window}\"]\n",
    "                slow_sma = result_df[f\"{column}_SMA_{slow_window}\"]\n",
    "                \n",
    "                # Calculate difference between fast and slow SMAs\n",
    "                diff = fast_sma - slow_sma\n",
    "                \n",
    "                # Calculate crossover signal\n",
    "                crossover = pd.Series(0, index=df.index)\n",
    "                \n",
    "                # Find where diff changes sign\n",
    "                diff_sign = np.sign(diff)\n",
    "                sign_change = diff_sign.diff().fillna(0)\n",
    "                \n",
    "                # 1 for bullish crossover (fast crosses above slow)\n",
    "                crossover[sign_change > 0] = 1\n",
    "                \n",
    "                # -1 for bearish crossover (fast crosses below slow)\n",
    "                crossover[sign_change < 0] = -1\n",
    "                \n",
    "                crossover_name = f\"{column}_SMA_{fast_window}_{slow_window}_crossover\"\n",
    "                result_df[crossover_name] = crossover\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_rsi(df, column, windows=None, include_trend=True, include_zones=True):\n",
    "        \"\"\"\n",
    "        Calculate Relative Strength Index with enhanced metrics.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df: pandas.DataFrame\n",
    "            DataFrame containing the data\n",
    "        column: str\n",
    "            Column name to calculate RSI for\n",
    "        windows: list\n",
    "            List of window sizes for RSI calculation\n",
    "        include_trend: bool\n",
    "            Whether to include trend direction\n",
    "        include_zones: bool\n",
    "            Whether to include overbought/oversold zone indicators\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with RSI values and enhanced metrics\n",
    "            \n",
    "        References:\n",
    "        -----------\n",
    "        - Wilder, J. W. (1978). New Concepts in Technical Trading Systems. \n",
    "          Trend Research, Greensboro, NC.\n",
    "        - Chong, T. T. L., & Ng, W. K. (2008). Technical analysis and the London stock exchange: \n",
    "          Testing the MACD and RSI rules using the FT30. Applied Economics Letters, 15(14), 1111-1114.\n",
    "        \"\"\"\n",
    "        if windows is None:\n",
    "            # Default parameters - will be overridden by frequency-specific ones\n",
    "            windows = [14, 21]\n",
    "        \n",
    "        result_df = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        for window in windows:\n",
    "            # Calculate price changes\n",
    "            delta = df[column].diff()\n",
    "            \n",
    "            # Create separate gain and loss series with proper dtype\n",
    "            gain = pd.Series(0.0, index=delta.index)  # Use float dtype\n",
    "            loss = pd.Series(0.0, index=delta.index)  # Use float dtype\n",
    "            \n",
    "            # Set values for gain and loss series using .loc for proper assignment\n",
    "            gain.loc[delta > 0] = delta[delta > 0]\n",
    "            loss.loc[delta < 0] = -delta[delta < 0]  # Make losses positive\n",
    "            \n",
    "            # First values are NaN\n",
    "            gain.iloc[0] = 0.0\n",
    "            loss.iloc[0] = 0.0\n",
    "            \n",
    "            # Calculate RSI using Wilder's method\n",
    "            # First calculate simple averages for initial periods\n",
    "            avg_gain = gain.rolling(window=window, min_periods=1).mean()\n",
    "            avg_loss = loss.rolling(window=window, min_periods=1).mean()\n",
    "            \n",
    "            # Then use the Wilder's smoothing method\n",
    "            for i in range(window, len(gain)):\n",
    "                avg_gain.iloc[i] = (avg_gain.iloc[i-1] * (window-1) + gain.iloc[i]) / window\n",
    "                avg_loss.iloc[i] = (avg_loss.iloc[i-1] * (window-1) + loss.iloc[i]) / window\n",
    "            \n",
    "            # Calculate RS and RSI\n",
    "            # Avoid division by zero with epsilon\n",
    "            epsilon = np.finfo(float).eps\n",
    "            rs = avg_gain / avg_loss.replace(0, epsilon)\n",
    "            rsi = 100 - (100 / (1 + rs))\n",
    "            \n",
    "            # Ensure RSI is within [0, 100] bounds\n",
    "            rsi = np.clip(rsi, 0, 100)\n",
    "            rsi_name = f\"{column}_RSI_{window}\"\n",
    "            result_df[rsi_name] = rsi\n",
    "            \n",
    "            # Calculate trend if requested\n",
    "            if include_trend:\n",
    "                trend = MultiFrequencyTechnicalIndicators._calculate_trend_direction(rsi)\n",
    "                result_df[f\"{rsi_name}_trend\"] = trend\n",
    "            \n",
    "            # Add overbought/oversold indicators if requested\n",
    "            if include_zones:\n",
    "                # Overbought zone (RSI > 70)\n",
    "                result_df[f\"{rsi_name}_overbought\"] = (rsi > 70).astype(int)\n",
    "                # Oversold zone (RSI < 30)\n",
    "                result_df[f\"{rsi_name}_oversold\"] = (rsi < 30).astype(int)\n",
    "                \n",
    "                # Initialize divergence column\n",
    "                result_df[f\"{rsi_name}_divergence\"] = 0\n",
    "                \n",
    "                # Calculate divergence between price and RSI\n",
    "                # Instead of using chained assignment, we'll create and assign a complete array\n",
    "                divergence_window = max(5, window // 3)\n",
    "                divergence_values = np.zeros(len(df))\n",
    "                \n",
    "                # Process in batches to improve performance\n",
    "                batch_size = 1000  # Process in batches\n",
    "                \n",
    "                for start_idx in range(divergence_window, len(df), batch_size):\n",
    "                    end_idx = min(start_idx + batch_size, len(df))\n",
    "                    \n",
    "                    for i in range(start_idx, end_idx):\n",
    "                        # Get windows for analysis\n",
    "                        price_window = df[column].iloc[i-divergence_window:i+1]\n",
    "                        rsi_window = rsi.iloc[i-divergence_window:i+1]\n",
    "                        \n",
    "                        # Skip if windows contain NaN\n",
    "                        if price_window.isna().any() or rsi_window.isna().any():\n",
    "                            continue\n",
    "                        \n",
    "                        # Check for bearish divergence\n",
    "                        # Price higher high but RSI lower high\n",
    "                        if (price_window.iloc[-1] > price_window.iloc[:-1].max() and \n",
    "                            rsi_window.iloc[-1] < rsi_window.iloc[:-1].max()):\n",
    "                            divergence_values[i] = -1  # Bearish\n",
    "                            \n",
    "                        # Check for bullish divergence\n",
    "                        # Price lower low but RSI higher low\n",
    "                        elif (price_window.iloc[-1] < price_window.iloc[:-1].min() and \n",
    "                              rsi_window.iloc[-1] > rsi_window.iloc[:-1].min()):\n",
    "                            divergence_values[i] = 1  # Bullish\n",
    "                \n",
    "                # Assign the complete divergence array at once (avoids chained assignment)\n",
    "                result_df.loc[:, f\"{rsi_name}_divergence\"] = divergence_values\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_roc(df, column, windows=None, include_trend=True, include_signal=True):\n",
    "        \"\"\"\n",
    "        Calculate Rate of Change with enhanced metrics.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df: pandas.DataFrame\n",
    "            DataFrame containing the data\n",
    "        column: str\n",
    "            Column name to calculate ROC for\n",
    "        windows: list\n",
    "            List of window sizes for ROC calculation\n",
    "        include_trend: bool\n",
    "            Whether to include trend direction\n",
    "        include_signal: bool\n",
    "            Whether to include signal line\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with ROC values and enhanced metrics\n",
    "            \n",
    "        References:\n",
    "        -----------\n",
    "        - Moskowitz, T. J., Ooi, Y. H., & Pedersen, L. H. (2012). Time series momentum. \n",
    "          Journal of Financial Economics, 104(2), 228-250.\n",
    "        - Menkhoff, L., Sarno, L., Schmeling, M., & Schrimpf, A. (2012). Currency momentum \n",
    "          strategies. Journal of Financial Economics, 106(3), 660-684.\n",
    "        \"\"\"\n",
    "        if windows is None:\n",
    "            # Default parameters - will be overridden by frequency-specific ones\n",
    "            windows = [1, 5, 20, 60]\n",
    "        \n",
    "        result_df = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        for window in windows:\n",
    "            # Calculate ROC (percentage change over the specified window)\n",
    "            roc = df[column].pct_change(periods=window) * 100\n",
    "            \n",
    "            # Fill first value with 0 for continuity\n",
    "            roc.iloc[:window] = 0\n",
    "            \n",
    "            roc_name = f\"{column}_ROC_{window}\"\n",
    "            result_df[roc_name] = roc\n",
    "            \n",
    "            # Calculate trend if requested\n",
    "            if include_trend:\n",
    "                trend = MultiFrequencyTechnicalIndicators._calculate_trend_direction(roc)\n",
    "                result_df[f\"{roc_name}_trend\"] = trend\n",
    "            \n",
    "            # Calculate signal line if requested\n",
    "            if include_signal:\n",
    "                # Signal line is typically a moving average of the ROC\n",
    "                signal_window = max(5, window // 4)\n",
    "                signal = roc.rolling(window=signal_window, min_periods=1).mean()\n",
    "                result_df[f\"{roc_name}_signal\"] = signal\n",
    "                \n",
    "                # Calculate crossover signal\n",
    "                crossover = pd.Series(0, index=df.index)\n",
    "                \n",
    "                # ROC crossing above signal line = bullish\n",
    "                crossover[(roc.shift(1) <= signal.shift(1)) & (roc > signal)] = 1\n",
    "                \n",
    "                # ROC crossing below signal line = bearish\n",
    "                crossover[(roc.shift(1) >= signal.shift(1)) & (roc < signal)] = -1\n",
    "                \n",
    "                result_df[f\"{roc_name}_crossover\"] = crossover\n",
    "                \n",
    "                # Calculate histogram (difference between ROC and signal)\n",
    "                histogram = roc - signal\n",
    "                result_df[f\"{roc_name}_histogram\"] = histogram\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_indicators(df, frequency='daily'):\n",
    "        \"\"\"\n",
    "        Apply all technical indicators with frequency-appropriate parameters.\n",
    "        Parameters:\n",
    "        -----------\n",
    "        df: pandas.DataFrame\n",
    "            DataFrame containing the data\n",
    "        frequency: str\n",
    "            Data frequency ('daily', 'weekly', 'monthly', 'quarterly')\n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with all technical indicators\n",
    "        \"\"\"\n",
    "        # Get frequency-specific parameters\n",
    "        params = MultiFrequencyTechnicalIndicators.get_frequency_params(frequency)\n",
    "        \n",
    "        # Store all indicators in a dictionary first to avoid DataFrame fragmentation\n",
    "        all_indicators = {}\n",
    "        \n",
    "        # Process each column in the DataFrame\n",
    "        for column in df.columns:\n",
    "            try:\n",
    "                # Calculate SMA\n",
    "                sma_df = MultiFrequencyTechnicalIndicators.calculate_sma(\n",
    "                    df, column, windows=params['sma'],\n",
    "                    include_trend=True, include_crossovers=True\n",
    "                )\n",
    "                \n",
    "                # Calculate RSI\n",
    "                rsi_df = MultiFrequencyTechnicalIndicators.calculate_rsi(\n",
    "                    df, column, windows=params['rsi'],\n",
    "                    include_trend=True, include_zones=True\n",
    "                )\n",
    "                \n",
    "                # Calculate ROC\n",
    "                roc_df = MultiFrequencyTechnicalIndicators.calculate_roc(\n",
    "                    df, column, windows=params['roc'],\n",
    "                    include_trend=True, include_signal=True\n",
    "                )\n",
    "                \n",
    "                # Combine all indicators into the dictionary\n",
    "                for col in sma_df.columns:\n",
    "                    all_indicators[f\"{column}_{col}\"] = sma_df[col]\n",
    "                for col in rsi_df.columns:\n",
    "                    all_indicators[f\"{column}_{col}\"] = rsi_df[col]\n",
    "                for col in roc_df.columns:\n",
    "                    all_indicators[f\"{column}_{col}\"] = roc_df[col]\n",
    "                    \n",
    "                print(f\"Applied indicators to {column}: {len(sma_df.columns) + len(rsi_df.columns) + len(roc_df.columns)} new features\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error applying indicators to {column}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        # Create the result DataFrame in one go to avoid fragmentation\n",
    "        result_df = pd.DataFrame(all_indicators, index=df.index)\n",
    "        \n",
    "        # Handle NaN values properly\n",
    "        if result_df.isna().any().any():\n",
    "            # Use proper forward fill and backward fill\n",
    "            result_df = result_df.ffill().bfill()\n",
    "            # If still have NaNs, fill with zeros\n",
    "            result_df = result_df.fillna(0)\n",
    "        \n",
    "        return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf2960b-c4d5-4e0f-ba3c-429f57daa44c",
   "metadata": {},
   "source": [
    "# Module 3: Dynamic Factor Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63a94e84-cf32-41ae-86a5-631f9a670a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.base.model import Model\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.tsa.statespace.tools import (\n",
    "    constrain_stationary_univariate, unconstrain_stationary_univariate\n",
    ")\n",
    "from statsmodels.tsa.tsatools import lagmat\n",
    "from scipy.optimize import minimize\n",
    "from numpy.linalg import pinv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "class DynamicFactorModel:\n",
    "    \"\"\"\n",
    "    Implementation of Dynamic Factor Model with EM algorithm for parameter estimation.\n",
    "    \n",
    "    This implementation follows the methodology of:\n",
    "    Doz, C., Giannone, D., & Reichlin, L. (2011). A two-step estimator for large \n",
    "    approximate dynamic factor models based on Kalman filtering. Journal of \n",
    "    Econometrics, 164(1), 188-205.\n",
    "    \n",
    "    Attributes:\n",
    "        n_factors (int): Number of factors to extract\n",
    "        max_iter (int): Maximum number of EM iterations\n",
    "        tol (float): Tolerance for convergence\n",
    "        ar_lags (int): Number of autoregressive lags for factors\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_factors=5, ar_lags=1, max_iter=100, tol=1e-4, random_state=None):\n",
    "        \"\"\"\n",
    "        Initialize the Dynamic Factor Model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_factors: int\n",
    "            Number of factors to extract\n",
    "        ar_lags: int\n",
    "            Number of autoregressive lags for factors\n",
    "        max_iter: int\n",
    "            Maximum number of EM iterations\n",
    "        tol: float\n",
    "            Tolerance for convergence\n",
    "        random_state: int or None\n",
    "            Random state for initialization\n",
    "        \"\"\"\n",
    "        self.n_factors = n_factors\n",
    "        self.ar_lags = ar_lags\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Model parameters to be estimated\n",
    "        self.loadings = None      # Factor loadings (Lambda)\n",
    "        self.ar_coefs = None      # Autoregressive coefficients (A)\n",
    "        self.Q = None             # State innovations covariance\n",
    "        self.R = None             # Observation innovations covariance\n",
    "        \n",
    "        # Kalman filter/smoother outputs\n",
    "        self.factors = None       # Smoothed factor estimates\n",
    "        self.factor_cov = None    # Factor covariance\n",
    "        \n",
    "        # Fit statistics\n",
    "        self.log_likelihood = None\n",
    "        self.n_obs = None\n",
    "        self.n_vars = None\n",
    "        \n",
    "        # Initialize random number generator\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "    \n",
    "    def _init_parameters(self, X):\n",
    "        \"\"\"\n",
    "        Initialize model parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: ndarray\n",
    "            Data matrix (time × variables)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            Initial parameter estimates\n",
    "            \n",
    "        References:\n",
    "        -----------\n",
    "        - Bańbura, M., & Modugno, M. (2014). Maximum likelihood estimation of factor models on \n",
    "          datasets with arbitrary pattern of missing data. Journal of Applied Econometrics, 29(1), \n",
    "          133-160.\n",
    "        \"\"\"\n",
    "        T, n_vars = X.shape\n",
    "        n_factors = self.n_factors\n",
    "        ar_lags = self.ar_lags\n",
    "        \n",
    "        # Standardize data\n",
    "        X_std = (X - np.nanmean(X, axis=0)) / np.nanstd(X, axis=0)\n",
    "        \n",
    "        # Handle missing values\n",
    "        X_filled = X_std.copy()\n",
    "        for j in range(n_vars):\n",
    "            mask = np.isnan(X_filled[:, j])\n",
    "            if mask.any():\n",
    "                # Forward fill, then backward fill\n",
    "                valid_indices = np.where(~mask)[0]\n",
    "                if len(valid_indices) > 0:\n",
    "                    for i in range(T):\n",
    "                        if mask[i]:\n",
    "                            # Find nearest valid index\n",
    "                            dists = np.abs(valid_indices - i)\n",
    "                            nearest_idx = valid_indices[np.argmin(dists)]\n",
    "                            X_filled[i, j] = X_filled[nearest_idx, j]\n",
    "                else:\n",
    "                    # If all values are missing, fill with zeros\n",
    "                    X_filled[:, j] = 0\n",
    "        \n",
    "        # 1. Initialize loadings using Principal Component Analysis\n",
    "        U, s, Vt = np.linalg.svd(X_filled, full_matrices=False)\n",
    "        loadings = Vt[:n_factors].T * np.sqrt(n_vars)\n",
    "        \n",
    "        # 2. Initialize factors\n",
    "        factors = U[:, :n_factors] * s[:n_factors] / np.sqrt(n_vars)\n",
    "        \n",
    "        # 3. Initialize AR coefficients for factors\n",
    "        ar_coefs = np.zeros((n_factors, n_factors * ar_lags))\n",
    "        \n",
    "        if T > ar_lags:\n",
    "            # For each factor, estimate an AR model\n",
    "            for i in range(n_factors):\n",
    "                factor_series = factors[:, i]\n",
    "                # Create lagged matrix\n",
    "                y = factor_series[ar_lags:]\n",
    "                X_ar = np.column_stack([factor_series[ar_lags-j-1:-j-1] for j in range(ar_lags)])\n",
    "                \n",
    "                # OLS estimation\n",
    "                beta = np.linalg.lstsq(X_ar, y, rcond=None)[0]\n",
    "                \n",
    "                # Store coefficients\n",
    "                ar_coefs[i, i::n_factors] = beta\n",
    "        \n",
    "        # Constrain AR coefficients to ensure stationarity\n",
    "        for i in range(n_factors):\n",
    "            coefs = ar_coefs[i, i::n_factors]\n",
    "            if len(coefs) > 0:  # Check if we have any AR coefficients\n",
    "                stable_coefs = constrain_stationary_univariate(coefs)\n",
    "                ar_coefs[i, i::n_factors] = stable_coefs\n",
    "        \n",
    "        # 4. Initialize state and observation innovations covariance\n",
    "        # Fit factor model and get residuals\n",
    "        fitted = factors @ loadings.T\n",
    "        residuals = X_filled - fitted\n",
    "        \n",
    "        # R: Observation innovations covariance (diagonal)\n",
    "        R = np.diag(np.nanvar(residuals, axis=0))\n",
    "        \n",
    "        # Q: State innovations covariance (diagonal)\n",
    "        Q = np.eye(n_factors) * 0.1\n",
    "        \n",
    "        return loadings, ar_coefs, Q, R\n",
    "    \n",
    "    def _kalman_filter(self, Y, loadings, ar_coefs, Q, R):\n",
    "        \"\"\"\n",
    "        Run Kalman filter for the dynamic factor model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        Y: ndarray\n",
    "            Data matrix (time × variables)\n",
    "        loadings: ndarray\n",
    "            Factor loadings\n",
    "        ar_coefs: ndarray\n",
    "            Autoregressive coefficients\n",
    "        Q: ndarray\n",
    "            State innovations covariance\n",
    "        R: ndarray\n",
    "            Observation innovations covariance\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            Filtered states, log-likelihood, and related quantities\n",
    "            \n",
    "        References:\n",
    "        -----------\n",
    "        - Durbin, J., & Koopman, S. J. (2012). Time Series Analysis by State Space Methods: \n",
    "          Second Edition. Oxford University Press.\n",
    "        \"\"\"\n",
    "        T, n_vars = Y.shape\n",
    "        n_factors = self.n_factors\n",
    "        ar_lags = self.ar_lags\n",
    "        \n",
    "        # State dimension\n",
    "        n_states = n_factors * ar_lags\n",
    "        \n",
    "        # Initialize state matrices - only store current and next state to save memory\n",
    "        alpha_curr = np.zeros(n_states)  # Current filtered state\n",
    "        P_curr = np.eye(n_states) * 10   # Current filtered state covariance (diffuse prior)\n",
    "        \n",
    "        # For smoothing, we need to store all states and their covariances\n",
    "        # Use more efficient storage - only store what we need\n",
    "        alpha = np.zeros((T+1, n_states))  # Filtered states\n",
    "        P = np.zeros((T+1, n_states, n_states))  # Filtered state covariance\n",
    "        alpha[0] = alpha_curr\n",
    "        P[0] = P_curr\n",
    "        \n",
    "        # For speed, pre-allocate and reuse memory for these matrices\n",
    "        v_full = np.zeros(n_vars)  # Full prediction error vector\n",
    "        F_full = np.zeros((n_vars, n_vars))  # Full prediction error covariance\n",
    "        K_full = np.zeros((n_states, n_vars))  # Full Kalman gain\n",
    "        \n",
    "        # Innovation vectors and covariance matrices - only store what's needed for smoothing\n",
    "        v = np.zeros((T, n_vars))  # Prediction errors\n",
    "        F = np.zeros((T, n_vars, n_vars))  # Prediction error covariance\n",
    "        K = np.zeros((T, n_states, n_vars))  # Kalman gain\n",
    "        \n",
    "        # Log-likelihood components\n",
    "        loglik = 0\n",
    "        \n",
    "        # Create transition matrix (companion form for VAR)\n",
    "        transition = np.zeros((n_states, n_states))\n",
    "        transition[:n_factors, :] = ar_coefs\n",
    "        for i in range(1, ar_lags):\n",
    "            transition[i*n_factors:(i+1)*n_factors, (i-1)*n_factors:i*n_factors] = np.eye(n_factors)\n",
    "        \n",
    "        # Create selection matrix for innovations\n",
    "        selection = np.zeros((n_states, n_factors))\n",
    "        selection[:n_factors, :] = np.eye(n_factors)\n",
    "        \n",
    "        # Create design matrix for observation equation\n",
    "        design = np.zeros((n_vars, n_states))\n",
    "        design[:, :n_factors] = loadings\n",
    "        \n",
    "        # Calculate state innovation covariance matrix once\n",
    "        Q_state = selection @ Q @ selection.T\n",
    "        \n",
    "        # Run the Kalman filter\n",
    "        for t in range(T):\n",
    "            # Forecast step\n",
    "            alpha_pred = transition @ alpha_curr\n",
    "            P_pred = transition @ P_curr @ transition.T + Q_state\n",
    "            \n",
    "            # Get observation for current time step\n",
    "            y_t = Y[t]\n",
    "            \n",
    "            # Handle missing data\n",
    "            mask = ~np.isnan(y_t)\n",
    "            if np.any(mask):\n",
    "                # Adapt matrices for missing data\n",
    "                y_observed = y_t[mask]\n",
    "                design_observed = design[mask]\n",
    "                R_observed = R[mask][:, mask]\n",
    "                \n",
    "                # Calculate prediction error\n",
    "                v_t = y_observed - design_observed @ alpha_pred\n",
    "                \n",
    "                # Calculate prediction error covariance\n",
    "                F_t = design_observed @ P_pred @ design_observed.T + R_observed\n",
    "                \n",
    "                # Symmetrize F_t for numerical stability\n",
    "                F_t = (F_t + F_t.T) / 2\n",
    "                \n",
    "                # Calculate Kalman gain\n",
    "                try:\n",
    "                    # Try Cholesky decomposition first (faster, more stable)\n",
    "                    L = np.linalg.cholesky(F_t)\n",
    "                    L_inv = np.linalg.inv(L)\n",
    "                    F_inv = L_inv.T @ L_inv\n",
    "                except np.linalg.LinAlgError:\n",
    "                    # Fall back to pseudo-inverse if Cholesky fails\n",
    "                    # Use more stable SVD-based pseudoinverse\n",
    "                    F_inv = np.linalg.pinv(F_t, rcond=1e-12)\n",
    "                \n",
    "                K_t = P_pred @ design_observed.T @ F_inv\n",
    "                \n",
    "                # Update step\n",
    "                alpha_curr = alpha_pred + K_t @ v_t\n",
    "                P_curr = P_pred - K_t @ design_observed @ P_pred\n",
    "                \n",
    "                # Store for smoothing\n",
    "                v_full[:] = np.nan  # Reset to NaN\n",
    "                v_full[mask] = v_t\n",
    "                v[t] = v_full\n",
    "                \n",
    "                F_full[:] = 0  # Reset to zeros\n",
    "                F_full[mask][:, mask] = F_t\n",
    "                F[t] = F_full\n",
    "                \n",
    "                K_full[:] = 0  # Reset to zeros\n",
    "                K_full[:, mask] = K_t\n",
    "                K[t] = K_full\n",
    "                \n",
    "                # Update log-likelihood if we have valid observation\n",
    "                # log L = -0.5 * (k*log(2π) + log|F_t| + v_t'F_t^(-1)v_t)\n",
    "                sign, logdet = np.linalg.slogdet(F_t)\n",
    "                if sign > 0:  # Ensure positive determinant\n",
    "                    loglik -= 0.5 * (len(y_observed) * np.log(2 * np.pi) + logdet + v_t.T @ F_inv @ v_t)\n",
    "            else:\n",
    "                # If all data is missing, just use predicted values\n",
    "                alpha_curr = alpha_pred\n",
    "                P_curr = P_pred\n",
    "            \n",
    "            # Store current state and covariance for smoothing\n",
    "            alpha[t+1] = alpha_curr\n",
    "            P[t+1] = P_curr\n",
    "        \n",
    "        return alpha, P, v, F, K, loglik\n",
    "    \n",
    "    def _kalman_smoother(self, alpha_filtered, P_filtered, v, F, K, transition):\n",
    "        \"\"\"\n",
    "        Run Kalman smoother for the dynamic factor model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        alpha_filtered: ndarray\n",
    "            Filtered states\n",
    "        P_filtered: ndarray\n",
    "            Filtered state covariance\n",
    "        v: ndarray\n",
    "            Prediction errors\n",
    "        F: ndarray\n",
    "            Prediction error covariance\n",
    "        K: ndarray\n",
    "            Kalman gain\n",
    "        transition: ndarray\n",
    "            Transition matrix\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            Smoothed states and covariance\n",
    "            \n",
    "        References:\n",
    "        -----------\n",
    "        - Durbin, J., & Koopman, S. J. (2012). Time Series Analysis by State Space Methods: \n",
    "          Second Edition. Oxford University Press.\n",
    "        \"\"\"\n",
    "        T = len(alpha_filtered) - 1\n",
    "        n_states = alpha_filtered.shape[1]\n",
    "        \n",
    "        # Smoothed states and covariance\n",
    "        alpha_smoothed = np.zeros_like(alpha_filtered)\n",
    "        P_smoothed = np.zeros_like(P_filtered)\n",
    "        \n",
    "        # Initialize with final filtered values\n",
    "        alpha_smoothed[T] = alpha_filtered[T]\n",
    "        P_smoothed[T] = P_filtered[T]\n",
    "        \n",
    "        # Run the Kalman smoother backwards\n",
    "        for t in range(T-1, -1, -1):\n",
    "            # Calculate smoothing quantities\n",
    "            L = transition - K[t] @ F[t] @ transition\n",
    "            \n",
    "            # Smoothing recursion\n",
    "            alpha_smoothed[t] = alpha_filtered[t] + L.T @ (alpha_smoothed[t+1] - transition @ alpha_filtered[t])\n",
    "            P_smoothed[t] = P_filtered[t] + L.T @ (P_smoothed[t+1] - P_filtered[t+1]) @ L\n",
    "        \n",
    "        return alpha_smoothed, P_smoothed\n",
    "    \n",
    "    def _em_step(self, Y, loadings, ar_coefs, Q, R):\n",
    "        \"\"\"\n",
    "        Perform one step of the EM algorithm.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        Y: ndarray\n",
    "            Data matrix (time × variables)\n",
    "        loadings: ndarray\n",
    "            Factor loadings\n",
    "        ar_coefs: ndarray\n",
    "            Autoregressive coefficients\n",
    "        Q: ndarray\n",
    "            State innovations covariance\n",
    "        R: ndarray\n",
    "            Observation innovations covariance\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            Updated parameters and log-likelihood\n",
    "            \n",
    "        References:\n",
    "        -----------\n",
    "        - Shumway, R. H., & Stoffer, D. S. (1982). An approach to time series smoothing and \n",
    "          forecasting using the EM algorithm. Journal of Time Series Analysis, 3(4), 253-264.\n",
    "        - Doz, C., Giannone, D., & Reichlin, L. (2011). A two-step estimator for large \n",
    "          approximate dynamic factor models based on Kalman filtering. Journal of \n",
    "          Econometrics, 164(1), 188-205.\n",
    "        \"\"\"\n",
    "        T, n_vars = Y.shape\n",
    "        n_factors = self.n_factors\n",
    "        ar_lags = self.ar_lags\n",
    "        n_states = n_factors * ar_lags\n",
    "        \n",
    "        # Create transition matrix\n",
    "        transition = np.zeros((n_states, n_states))\n",
    "        transition[:n_factors, :] = ar_coefs\n",
    "        for i in range(1, ar_lags):\n",
    "            transition[i*n_factors:(i+1)*n_factors, (i-1)*n_factors:i*n_factors] = np.eye(n_factors)\n",
    "        \n",
    "        # Create design matrix\n",
    "        design = np.zeros((n_vars, n_states))\n",
    "        design[:, :n_factors] = loadings\n",
    "        \n",
    "        # Create selection matrix\n",
    "        selection = np.zeros((n_states, n_factors))\n",
    "        selection[:n_factors, :] = np.eye(n_factors)\n",
    "        \n",
    "        # 1. E-step: Run Kalman filter and smoother\n",
    "        alpha_filtered, P_filtered, v, F, K, loglik = self._kalman_filter(Y, loadings, ar_coefs, Q, R)\n",
    "        alpha_smoothed, P_smoothed = self._kalman_smoother(alpha_filtered, P_filtered, v, F, K, transition)\n",
    "        \n",
    "        # Process in batches to reduce memory usage\n",
    "        batch_size = min(100, T//4)  # Use reasonable batch size\n",
    "        \n",
    "        # Initialize parameter update accumulators\n",
    "        loadings_numerator = np.zeros((n_vars, n_factors))\n",
    "        loadings_denominator = np.zeros((n_factors, n_factors))\n",
    "        ar_numerator = np.zeros((n_factors, n_factors * ar_lags))\n",
    "        ar_denominator = np.zeros((n_factors * ar_lags, n_factors * ar_lags))\n",
    "        Q_new = np.zeros((n_factors, n_factors))\n",
    "        R_new = np.zeros((n_vars, n_vars))\n",
    "        obs_count = np.zeros(n_vars)\n",
    "        \n",
    "        # Process data in batches to update parameters\n",
    "        for batch_start in range(0, T, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, T)\n",
    "            \n",
    "            # Update loadings (Lambda) batch\n",
    "            for t in range(batch_start, batch_end):\n",
    "                y_t = Y[t]\n",
    "                mask = ~np.isnan(y_t)\n",
    "                if np.any(mask):\n",
    "                    alpha_t = alpha_smoothed[t][:n_factors]\n",
    "                    P_t = P_smoothed[t][:n_factors, :n_factors]\n",
    "                    loadings_numerator[mask] += np.outer(y_t[mask], alpha_t)\n",
    "                    loadings_denominator += P_t + np.outer(alpha_t, alpha_t)\n",
    "            \n",
    "            # Update AR coefficients (A) batch\n",
    "            for t in range(batch_start + 1, batch_end):\n",
    "                alpha_t = alpha_smoothed[t][:n_factors]\n",
    "                alpha_lag = alpha_smoothed[t-1][:n_factors*ar_lags]\n",
    "                \n",
    "                # For cross-covariance, we need a simplification\n",
    "                # Instead of storing all P_cross matrices, compute directly\n",
    "                P_t = P_smoothed[t][:n_factors, :n_factors]\n",
    "                P_lag = P_smoothed[t-1][:n_factors*ar_lags, :n_factors*ar_lags]\n",
    "                \n",
    "                # Simplified P_cross approximation - works well in practice\n",
    "                P_cross = np.zeros((n_factors, n_factors*ar_lags))\n",
    "                if t > 0:\n",
    "                    # Use a simplified approach that still captures the cross-covariance\n",
    "                    P_cross = transition[:n_factors, :] @ P_lag\n",
    "                \n",
    "                ar_numerator += np.outer(alpha_t, alpha_lag) + P_cross\n",
    "                ar_denominator += P_lag + np.outer(alpha_lag, alpha_lag)\n",
    "            \n",
    "            # Update Q batch\n",
    "            for t in range(batch_start + 1, batch_end):\n",
    "                alpha_t = alpha_smoothed[t][:n_factors]\n",
    "                alpha_pred = transition @ alpha_smoothed[t-1][:n_states]\n",
    "                alpha_pred = alpha_pred[:n_factors]  # Only first n_factors states are directly affected\n",
    "                \n",
    "                P_t = P_smoothed[t][:n_factors, :n_factors]\n",
    "                P_pred = transition @ P_smoothed[t-1] @ transition.T\n",
    "                P_pred = P_pred[:n_factors, :n_factors]  # Only use factor block\n",
    "                \n",
    "                # Error in state prediction\n",
    "                pred_error = alpha_t - alpha_pred\n",
    "                \n",
    "                # Update Q - simplified to avoid storing P_cross\n",
    "                P_cross_approx = transition[:n_factors, :] @ P_smoothed[t-1][:, :n_factors]\n",
    "                \n",
    "                Q_new += P_t - P_cross_approx - P_cross_approx.T + np.outer(pred_error, pred_error)\n",
    "            \n",
    "            # Update R batch\n",
    "            for t in range(batch_start, batch_end):\n",
    "                y_t = Y[t]\n",
    "                mask = ~np.isnan(y_t)\n",
    "                obs_count[mask] += 1\n",
    "                \n",
    "                if np.any(mask):\n",
    "                    alpha_t = alpha_smoothed[t][:n_factors]\n",
    "                    P_t = P_smoothed[t][:n_factors, :n_factors]\n",
    "                    \n",
    "                    # Predicted observation\n",
    "                    y_pred = loadings[mask] @ alpha_t\n",
    "                    \n",
    "                    # Observation prediction error\n",
    "                    obs_error = y_t[mask] - y_pred\n",
    "                    \n",
    "                    # Update R (diagonal elements only)\n",
    "                    for i, idx in enumerate(np.where(mask)[0]):\n",
    "                        R_new[idx, idx] += obs_error[i]**2 + loadings[idx] @ P_t @ loadings[idx].T\n",
    "        \n",
    "        # Finalize parameter updates\n",
    "        # Update loadings\n",
    "        new_loadings = loadings_numerator @ np.linalg.pinv(loadings_denominator)\n",
    "        \n",
    "        # Update AR coefficients\n",
    "        new_ar_coefs = ar_numerator @ np.linalg.pinv(ar_denominator)\n",
    "        \n",
    "        # Constrain AR coefficients to ensure stationarity\n",
    "        for i in range(n_factors):\n",
    "            coefs = new_ar_coefs[i, i::n_factors]\n",
    "            if len(coefs) > 0:  # Check if we have any AR coefficients\n",
    "                try:\n",
    "                    stable_coefs = constrain_stationary_univariate(coefs)\n",
    "                    new_ar_coefs[i, i::n_factors] = stable_coefs\n",
    "                except:\n",
    "                    # If constraint fails, keep old coefficients\n",
    "                    pass\n",
    "        \n",
    "        # Update Q\n",
    "        Q_new /= (T - 1)\n",
    "        \n",
    "        # Ensure Q is positive definite\n",
    "        Q_new = (Q_new + Q_new.T) / 2  # Make symmetric\n",
    "        eigvals, eigvecs = np.linalg.eigh(Q_new)\n",
    "        eigvals = np.maximum(eigvals, 1e-6)  # Ensure positive eigenvalues\n",
    "        Q_new = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
    "        \n",
    "        # Update R\n",
    "        for i in range(n_vars):\n",
    "            if obs_count[i] > 0:\n",
    "                R_new[i, i] /= obs_count[i]\n",
    "            else:\n",
    "                # Keep old value if no observations\n",
    "                R_new[i, i] = R[i, i]\n",
    "        \n",
    "        # Ensure R is positive definite\n",
    "        R_diag = np.diag(R_new)\n",
    "        R_diag = np.maximum(R_diag, 1e-6)  # Ensure positive values\n",
    "        R_new = np.diag(R_diag)\n",
    "        \n",
    "        return new_loadings, new_ar_coefs, Q_new, R_new, loglik, alpha_smoothed, P_smoothed\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the Dynamic Factor Model using EM algorithm.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: DataFrame or ndarray\n",
    "            Data matrix (time × variables)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self\n",
    "            Fitted model instance\n",
    "        \"\"\"\n",
    "        # Convert to numpy array if DataFrame\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.column_names = X.columns\n",
    "            self.index = X.index\n",
    "            X = X.values\n",
    "        else:\n",
    "            self.column_names = [f\"Var{i}\" for i in range(X.shape[1])]\n",
    "            self.index = np.arange(X.shape[0])\n",
    "        \n",
    "        # Store dimensions\n",
    "        self.n_obs, self.n_vars = X.shape\n",
    "        \n",
    "        # Initialize parameters\n",
    "        loadings, ar_coefs, Q, R = self._init_parameters(X)\n",
    "        \n",
    "        # EM iterations\n",
    "        prev_loglik = -np.inf\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            # Perform one EM step\n",
    "            loadings, ar_coefs, Q, R, loglik, factors, factor_cov = self._em_step(X, loadings, ar_coefs, Q, R)\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.abs(loglik - prev_loglik) < self.tol:\n",
    "                print(f\"EM algorithm converged after {iteration+1} iterations\")\n",
    "                break\n",
    "            \n",
    "            prev_loglik = loglik\n",
    "            \n",
    "            if (iteration + 1) % 10 == 0:\n",
    "                print(f\"Iteration {iteration+1}, Log-likelihood: {loglik:.4f}\")\n",
    "        \n",
    "        # Store final parameters\n",
    "        self.loadings = loadings\n",
    "        self.ar_coefs = ar_coefs\n",
    "        self.Q = Q\n",
    "        self.R = R\n",
    "        self.log_likelihood = loglik\n",
    "        \n",
    "        # Extract factors (first n_factors states from smoothed estimates)\n",
    "        self.factors = factors[:, :self.n_factors]\n",
    "        self.factor_cov = factor_cov[:, :self.n_factors, :self.n_factors]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X=None):\n",
    "        \"\"\"\n",
    "        Extract factors from data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: DataFrame or ndarray, optional\n",
    "            New data to transform. If None, use the data used for fitting.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        ndarray\n",
    "            Extracted factors\n",
    "        \"\"\"\n",
    "        if X is None:\n",
    "            # Return factors estimated during fitting\n",
    "            return self.factors\n",
    "        \n",
    "        # Convert to numpy array if DataFrame\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        \n",
    "        # Check if X has the same number of variables\n",
    "        if X.shape[1] != self.n_vars:\n",
    "            raise ValueError(f\"X has {X.shape[1]} variables, but the model was \"\n",
    "                            f\"fitted with {self.n_vars} variables\")\n",
    "        \n",
    "        # Run Kalman filter on new data\n",
    "        alpha, P, _, _, _, _ = self._kalman_filter(X, self.loadings, self.ar_coefs, self.Q, self.R)\n",
    "        \n",
    "        # Extract factors (first n_factors states)\n",
    "        return alpha[1:, :self.n_factors]  # Use filtered estimates (shifted by 1)\n",
    "    \n",
    "    def get_factor_loadings(self):\n",
    "        \"\"\"\n",
    "        Get factor loadings as a DataFrame.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            Factor loadings\n",
    "        \"\"\"\n",
    "        if self.loadings is None:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        factor_names = [f\"Factor{i+1}\" for i in range(self.n_factors)]\n",
    "        return pd.DataFrame(self.loadings, index=self.column_names, columns=factor_names)\n",
    "    \n",
    "    def get_factors(self):\n",
    "        \"\"\"\n",
    "        Get extracted factors as a DataFrame.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            Extracted factors\n",
    "        \"\"\"\n",
    "        if self.factors is None:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        factor_names = [f\"Factor{i+1}\" for i in range(self.n_factors)]\n",
    "        return pd.DataFrame(self.factors, index=self.index, columns=factor_names)\n",
    "    \n",
    "    def forecast(self, steps=1):\n",
    "        \"\"\"\n",
    "        Forecast factors for future time periods.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        steps: int\n",
    "            Number of steps ahead to forecast\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            Forecasted factors\n",
    "        \"\"\"\n",
    "        if self.factors is None:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        # Get the last state\n",
    "        n_states = self.n_factors * self.ar_lags\n",
    "        last_state = np.zeros(n_states)\n",
    "        \n",
    "        # Fill in most recent observations\n",
    "        for i in range(min(self.ar_lags, len(self.factors))):\n",
    "            if i < len(self.factors):\n",
    "                last_state[i*self.n_factors:(i+1)*self.n_factors] = self.factors[-i-1]\n",
    "        \n",
    "        # Create transition matrix\n",
    "        transition = np.zeros((n_states, n_states))\n",
    "        transition[:self.n_factors, :] = self.ar_coefs\n",
    "        for i in range(1, self.ar_lags):\n",
    "            transition[i*self.n_factors:(i+1)*self.n_factors, (i-1)*self.n_factors:i*self.n_factors] = np.eye(self.n_factors)\n",
    "        \n",
    "        # Iterate through forecast horizon\n",
    "        forecasts = np.zeros((steps, self.n_factors))\n",
    "        current_state = last_state\n",
    "        \n",
    "        for t in range(steps):\n",
    "            # Forecast next state\n",
    "            next_state = transition @ current_state\n",
    "            \n",
    "            # Store the forecast\n",
    "            forecasts[t] = next_state[:self.n_factors]\n",
    "            \n",
    "            # Update current state\n",
    "            current_state = next_state\n",
    "        \n",
    "        # Create forecast index\n",
    "        last_date = self.index[-1]\n",
    "        if isinstance(last_date, pd.Timestamp):\n",
    "            # Infer frequency\n",
    "            freq = pd.infer_freq(self.index)\n",
    "            if freq is None:\n",
    "                # Try to infer from last few observations\n",
    "                freq = pd.infer_freq(self.index[-5:])\n",
    "            \n",
    "            if freq is None:\n",
    "                # Fallback to day frequency\n",
    "                freq = 'D'\n",
    "            \n",
    "            forecast_index = pd.date_range(start=last_date, periods=steps+1, freq=freq)[1:]\n",
    "        else:\n",
    "            forecast_index = np.arange(self.index[-1] + 1, self.index[-1] + steps + 1)\n",
    "        \n",
    "        # Create factor names\n",
    "        factor_names = [f\"Factor{i+1}\" for i in range(self.n_factors)]\n",
    "        \n",
    "        return pd.DataFrame(forecasts, index=forecast_index, columns=factor_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c1005-44a0-46fc-8cb7-ab8de848cc94",
   "metadata": {},
   "source": [
    "# Module 4: MIDAS Implementation for GDP Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0942433-eb64-4e1b-be02-9e822be8578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import Ridge\n",
    "import warnings\n",
    "\n",
    "class MIDASRegressor:\n",
    "    \"\"\"\n",
    "    Mixed-Data Sampling (MIDAS) regression for mixed-frequency time series.\n",
    "    \n",
    "    This implementation allows for flexible weighting functions to combine \n",
    "    high-frequency data for predicting low-frequency targets.\n",
    "    \n",
    "    References:\n",
    "    -----------\n",
    "    - Ghysels, E., Santa-Clara, P., & Valkanov, R. (2004). The MIDAS touch: Mixed data \n",
    "      sampling regression models. Working paper, University of North Carolina.\n",
    "    - Ghysels, E., Sinko, A., & Valkanov, R. (2007). MIDAS regressions: Further results \n",
    "      and new directions. Econometric Reviews, 26(1), 53-90.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, weight_function='exponential_almon', max_lags=12, \n",
    "                 n_weight_params=2, ar_lags=4, regularization=0.0, \n",
    "                 max_iter=1000, tol=1e-6, random_state=None):\n",
    "        \"\"\"\n",
    "        Initialize MIDAS regressor.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        weight_function: str or callable\n",
    "            Weighting function. Options are:\n",
    "            - 'exponential_almon': Exponential Almon lag polynomial\n",
    "            - 'beta': Beta function\n",
    "            - A custom callable function with signature w(lag, params)\n",
    "        max_lags: int\n",
    "            Maximum number of lags for high-frequency variables\n",
    "        n_weight_params: int\n",
    "            Number of parameters in the weighting function\n",
    "        ar_lags: int\n",
    "            Number of autoregressive lags for the target variable\n",
    "        regularization: float\n",
    "            Ridge regularization parameter\n",
    "        max_iter: int\n",
    "            Maximum number of iterations for optimization\n",
    "        tol: float\n",
    "            Tolerance for optimization convergence\n",
    "        random_state: int or None\n",
    "            Random state for initialization\n",
    "        \"\"\"\n",
    "        self.max_lags = max_lags\n",
    "        self.n_weight_params = n_weight_params\n",
    "        self.ar_lags = ar_lags\n",
    "        self.regularization = regularization\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Set weighting function\n",
    "        if weight_function == 'exponential_almon':\n",
    "            self.weight_function = self._exponential_almon_weights\n",
    "        elif weight_function == 'beta':\n",
    "            self.weight_function = self._beta_weights\n",
    "        elif callable(weight_function):\n",
    "            self.weight_function = weight_function\n",
    "        else:\n",
    "            raise ValueError(\"weight_function must be 'exponential_almon', 'beta', or a callable\")\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.weight_params = None\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.fit_intercept = True\n",
    "        \n",
    "        # Initialize random number generator\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "    \n",
    "    def _exponential_almon_weights(self, lag, params):\n",
    "        \"\"\"\n",
    "        Exponential Almon lag polynomial weighting function.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        lag: ndarray\n",
    "            Lag indices (0, 1, ..., max_lags-1)\n",
    "        params: ndarray\n",
    "            Parameters for the weighting function\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        ndarray\n",
    "            Weights for each lag\n",
    "            \n",
    "        References:\n",
    "        -----------\n",
    "        - Ghysels, E., Sinko, A., & Valkanov, R. (2007). MIDAS regressions: Further results \n",
    "          and new directions. Econometric Reviews, 26(1), 53-90.\n",
    "        \"\"\"\n",
    "        if len(params) < 2:\n",
    "            # Need at least two parameters\n",
    "            params = np.array([params[0], 0.0])\n",
    "        \n",
    "        # Normalize lags to [0, 1]\n",
    "        x = lag / (self.max_lags - 1) if self.max_lags > 1 else 0\n",
    "        \n",
    "        # Calculate weights\n",
    "        exponent = params[0] * x + params[1] * x**2\n",
    "        weights = np.exp(exponent)\n",
    "        \n",
    "        # Normalize weights to sum to 1\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def _beta_weights(self, lag, params):\n",
    "        \"\"\"\n",
    "        Beta function weighting.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        lag: ndarray\n",
    "            Lag indices (0, 1, ..., max_lags-1)\n",
    "        params: ndarray\n",
    "            Parameters for the weighting function\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        ndarray\n",
    "            Weights for each lag\n",
    "            \n",
    "        References:\n",
    "        -----------\n",
    "        - Ghysels, E., Sinko, A., & Valkanov, R. (2007). MIDAS regressions: Further results \n",
    "          and new directions. Econometric Reviews, 26(1), 53-90.\n",
    "        \"\"\"\n",
    "        if len(params) < 2:\n",
    "            # Need at least two parameters\n",
    "            params = np.array([1.0, 5.0])\n",
    "        \n",
    "        # Ensure parameters are positive\n",
    "        a = np.abs(params[0])\n",
    "        b = np.abs(params[1])\n",
    "        \n",
    "        # Normalize lags to [0, 1]\n",
    "        x = lag / (self.max_lags - 1) if self.max_lags > 1 else 0\n",
    "        \n",
    "        # Calculate weights using beta function\n",
    "        weights = x**(a-1) * (1-x)**(b-1)\n",
    "        \n",
    "        # Handle potential numerical issues\n",
    "        weights = np.nan_to_num(weights, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        # Normalize weights to sum to 1\n",
    "        if weights.sum() > 0:\n",
    "            weights = weights / weights.sum()\n",
    "        else:\n",
    "            # If all weights are zero, use uniform weights\n",
    "            weights = np.ones_like(weights) / len(weights)\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def _aggregate_high_frequency(self, X_hf, weight_params):\n",
    "        \"\"\"\n",
    "        Aggregate high-frequency variables using weighting function.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_hf: list of ndarrays\n",
    "            List of high-frequency variables, each with shape (n_samples, max_lags)\n",
    "        weight_params: ndarray\n",
    "            Parameters for the weighting function\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        ndarray\n",
    "            Aggregated high-frequency variables, shape (n_samples, n_hf_variables)\n",
    "        \"\"\"\n",
    "        n_samples = X_hf[0].shape[0]\n",
    "        n_hf_vars = len(X_hf)\n",
    "        \n",
    "        # Initialize aggregated variables\n",
    "        X_aggregated = np.zeros((n_samples, n_hf_vars))\n",
    "        \n",
    "        # Calculate weights\n",
    "        lags = np.arange(self.max_lags)\n",
    "        weights = self.weight_function(lags, weight_params)\n",
    "        \n",
    "        # Apply weights to each high-frequency variable\n",
    "        for i, X_var in enumerate(X_hf):\n",
    "            # Weighted sum across lags\n",
    "            X_aggregated[:, i] = np.sum(X_var * weights, axis=1)\n",
    "        \n",
    "        return X_aggregated\n",
    "    \n",
    "    def _objective_function(self, weight_params, X_hf, X_ar, y):\n",
    "        \"\"\"\n",
    "        Objective function for MIDAS parameter optimization.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        weight_params: ndarray\n",
    "            Parameters for the weighting function\n",
    "        X_hf: list of ndarrays\n",
    "            List of high-frequency variables\n",
    "        X_ar: ndarray\n",
    "            Autoregressive features\n",
    "        y: ndarray\n",
    "            Target variable\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Mean squared error with regularization\n",
    "        \"\"\"\n",
    "        # Aggregate high-frequency variables\n",
    "        X_midas = self._aggregate_high_frequency(X_hf, weight_params)\n",
    "        \n",
    "        # Combine with autoregressive features\n",
    "        if X_ar is not None:\n",
    "            X = np.column_stack([X_ar, X_midas])\n",
    "        else:\n",
    "            X = X_midas\n",
    "        \n",
    "        # Add intercept\n",
    "        if self.fit_intercept:\n",
    "            X = np.column_stack([np.ones(X.shape[0]), X])\n",
    "        \n",
    "        # Compute coefficients using Ridge regression\n",
    "        coef = np.linalg.lstsq(\n",
    "            X.T @ X + self.regularization * np.eye(X.shape[1]),\n",
    "            X.T @ y,\n",
    "            rcond=None\n",
    "        )[0]\n",
    "        \n",
    "        # Calculate predictions\n",
    "        y_pred = X @ coef\n",
    "        \n",
    "        # Calculate MSE\n",
    "        mse = np.mean((y - y_pred) ** 2)\n",
    "        \n",
    "        return mse\n",
    "    \n",
    "    def fit(self, X_hf, y, X_ar=None):\n",
    "        \"\"\"\n",
    "        Fit MIDAS regression model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_hf: list of DataFrames or ndarrays\n",
    "            List of high-frequency variables, each with shape (n_samples, max_lags)\n",
    "        y: DataFrame or ndarray\n",
    "            Target variable\n",
    "        X_ar: DataFrame or ndarray, optional\n",
    "            Autoregressive features\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self\n",
    "            Fitted model instance\n",
    "        \"\"\"\n",
    "        # Convert inputs to numpy arrays if needed\n",
    "        if isinstance(y, pd.Series) or isinstance(y, pd.DataFrame):\n",
    "            y = y.values.flatten()\n",
    "        else:\n",
    "            y = np.asarray(y).flatten()\n",
    "        \n",
    "        # Process high-frequency variables\n",
    "        X_hf_arrays = []\n",
    "        for X in X_hf:\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X_hf_arrays.append(X.values)\n",
    "            else:\n",
    "                X_hf_arrays.append(np.asarray(X))\n",
    "        \n",
    "        # Process autoregressive features\n",
    "        if X_ar is not None:\n",
    "            if isinstance(X_ar, pd.DataFrame):\n",
    "                X_ar = X_ar.values\n",
    "            else:\n",
    "                X_ar = np.asarray(X_ar)\n",
    "        \n",
    "        # Initialize weight parameters\n",
    "        init_params = self.rng.normal(0, 0.1, self.n_weight_params)\n",
    "        \n",
    "        # Use bounded optimization for better stability\n",
    "        bounds = [(-10, 10)] * self.n_weight_params\n",
    "        \n",
    "        # Optimize weight parameters\n",
    "        result = minimize(\n",
    "            self._objective_function,\n",
    "            init_params,\n",
    "            args=(X_hf_arrays, X_ar, y),\n",
    "            method='L-BFGS-B',\n",
    "            bounds=bounds,\n",
    "            options={'maxiter': self.max_iter, 'gtol': self.tol}\n",
    "        )\n",
    "        \n",
    "        self.weight_params = result.x\n",
    "        \n",
    "        # Calculate final weights\n",
    "        lags = np.arange(self.max_lags)\n",
    "        self.weights_ = self.weight_function(lags, self.weight_params)\n",
    "        \n",
    "        # Aggregate high-frequency variables with optimized weights\n",
    "        X_midas = self._aggregate_high_frequency(X_hf_arrays, self.weight_params)\n",
    "        \n",
    "        # Combine with autoregressive features\n",
    "        if X_ar is not None:\n",
    "            X = np.column_stack([X_ar, X_midas])\n",
    "        else:\n",
    "            X = X_midas\n",
    "        \n",
    "        # Fit Ridge regression\n",
    "        ridge = Ridge(alpha=self.regularization, fit_intercept=self.fit_intercept)\n",
    "        ridge.fit(X, y)\n",
    "        \n",
    "        # Store coefficients\n",
    "        if self.fit_intercept:\n",
    "            self.intercept_ = ridge.intercept_\n",
    "            self.coef_ = ridge.coef_\n",
    "        else:\n",
    "            self.intercept_ = 0.0\n",
    "            self.coef_ = ridge.coef_\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_hf, X_ar=None):\n",
    "        \"\"\"\n",
    "        Make predictions with fitted model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_hf: list of DataFrames or ndarrays\n",
    "            List of high-frequency variables\n",
    "        X_ar: DataFrame or ndarray, optional\n",
    "            Autoregressive features\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        ndarray\n",
    "            Predictions\n",
    "        \"\"\"\n",
    "        if self.weight_params is None:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        # Convert inputs to numpy arrays if needed\n",
    "        X_hf_arrays = []\n",
    "        for X in X_hf:\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X_hf_arrays.append(X.values)\n",
    "            else:\n",
    "                X_hf_arrays.append(np.asarray(X))\n",
    "        \n",
    "        if X_ar is not None:\n",
    "            if isinstance(X_ar, pd.DataFrame):\n",
    "                X_ar = X_ar.values\n",
    "            else:\n",
    "                X_ar = np.asarray(X_ar)\n",
    "        \n",
    "        # Aggregate high-frequency variables with fitted weights\n",
    "        X_midas = self._aggregate_high_frequency(X_hf_arrays, self.weight_params)\n",
    "        \n",
    "        # Combine with autoregressive features\n",
    "        if X_ar is not None:\n",
    "            X = np.column_stack([X_ar, X_midas])\n",
    "        else:\n",
    "            X = X_midas\n",
    "        \n",
    "        # Make predictions\n",
    "        if self.fit_intercept:\n",
    "            y_pred = self.intercept_ + X @ self.coef_\n",
    "        else:\n",
    "            y_pred = X @ self.coef_\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def get_midas_weights(self):\n",
    "        \"\"\"\n",
    "        Get the MIDAS weighting function parameters and weights.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary with parameters and weights\n",
    "        \"\"\"\n",
    "        if self.weight_params is None:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        lags = np.arange(self.max_lags)\n",
    "        weights = self.weight_function(lags, self.weight_params)\n",
    "        \n",
    "        return {\n",
    "            'parameters': self.weight_params,\n",
    "            'weights': weights,\n",
    "            'lags': lags\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecdcd68-4ca1-4ec4-b51a-b1ba670549d6",
   "metadata": {},
   "source": [
    "# Module 5: Hierarchical GDP Prediction System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71066cb2-a7ff-4c76-abf6-e528a816d9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalGDPPredictor:\n",
    "    \"\"\"\n",
    "    Hierarchical system for GDP prediction using multi-frequency data.\n",
    "    \n",
    "    This class orchestrates the hierarchical prediction process:\n",
    "    1. Daily factors extraction\n",
    "    2. Weekly factors extraction with daily inputs\n",
    "    3. Monthly factors extraction with weekly inputs\n",
    "    4. Quarterly GDP prediction with monthly inputs\n",
    "    \n",
    "    Attributes:\n",
    "        daily_model: DynamicFactorModel for daily data\n",
    "        weekly_model: DynamicFactorModel for weekly data\n",
    "        monthly_model: DynamicFactorModel for monthly data\n",
    "        gdp_model: MIDASRegressor or scikit-learn regressor for GDP prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 daily_factors=5, weekly_factors=3, monthly_factors=3,\n",
    "                 daily_ar_lags=2, weekly_ar_lags=2, monthly_ar_lags=2, gdp_ar_lags=4,\n",
    "                 use_midas=True, midas_max_lags=6, random_state=None):\n",
    "        \"\"\"\n",
    "        Initialize the hierarchical GDP prediction system.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        daily_factors: int\n",
    "            Number of factors to extract from daily data\n",
    "        weekly_factors: int\n",
    "            Number of factors to extract from weekly data\n",
    "        monthly_factors: int\n",
    "            Number of factors to extract from monthly data\n",
    "        daily_ar_lags: int\n",
    "            Autoregressive lags for daily factors\n",
    "        weekly_ar_lags: int\n",
    "            Autoregressive lags for weekly factors\n",
    "        monthly_ar_lags: int\n",
    "            Autoregressive lags for monthly factors\n",
    "        gdp_ar_lags: int\n",
    "            Autoregressive lags for GDP\n",
    "        use_midas: bool\n",
    "            Whether to use MIDAS for the final GDP prediction\n",
    "        midas_max_lags: int\n",
    "            Maximum number of lags for MIDAS\n",
    "        random_state: int or None\n",
    "            Random state for reproducibility\n",
    "        \"\"\"\n",
    "        self.daily_factors = daily_factors\n",
    "        self.weekly_factors = weekly_factors\n",
    "        self.monthly_factors = monthly_factors\n",
    "        \n",
    "        self.daily_ar_lags = daily_ar_lags\n",
    "        self.weekly_ar_lags = weekly_ar_lags\n",
    "        self.monthly_ar_lags = monthly_ar_lags\n",
    "        self.gdp_ar_lags = gdp_ar_lags\n",
    "        \n",
    "        self.use_midas = use_midas\n",
    "        self.midas_max_lags = midas_max_lags\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Initialize models\n",
    "        self.daily_model = DynamicFactorModel(\n",
    "            n_factors=daily_factors,\n",
    "            ar_lags=daily_ar_lags,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        self.weekly_model = DynamicFactorModel(\n",
    "            n_factors=weekly_factors,\n",
    "            ar_lags=weekly_ar_lags,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        self.monthly_model = DynamicFactorModel(\n",
    "            n_factors=monthly_factors,\n",
    "            ar_lags=monthly_ar_lags,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Initialize GDP model based on configuration\n",
    "        if use_midas:\n",
    "            self.gdp_model = MIDASRegressor(\n",
    "                weight_function='exponential_almon',\n",
    "                max_lags=midas_max_lags,\n",
    "                n_weight_params=2,\n",
    "                ar_lags=gdp_ar_lags,\n",
    "                regularization=0.01,\n",
    "                random_state=random_state\n",
    "            )\n",
    "        else:\n",
    "            # Use Ridge regression as fallback\n",
    "            from sklearn.linear_model import Ridge\n",
    "            self.gdp_model = Ridge(alpha=0.01)\n",
    "        \n",
    "        # Storage for fitted factors\n",
    "        self.daily_factors_df = None\n",
    "        self.weekly_factors_df = None\n",
    "        self.monthly_factors_df = None\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit_daily_model(self, daily_df):\n",
    "        \"\"\"\n",
    "        Fit the daily DFM model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        daily_df: pandas.DataFrame\n",
    "            Daily data with technical indicators\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            Extracted daily factors\n",
    "        \"\"\"\n",
    "        print(f\"Fitting daily model with {daily_df.shape[1]} features\")\n",
    "        self.daily_model.fit(daily_df)\n",
    "        self.daily_factors_df = self.daily_model.get_factors()\n",
    "        \n",
    "        print(f\"Extracted {self.daily_factors_df.shape[1]} daily factors\")\n",
    "        return self.daily_factors_df\n",
    "    \n",
    "    def fit_weekly_model(self, weekly_df, daily_factors=None):\n",
    "        \"\"\"\n",
    "        Fit the weekly DFM model, incorporating daily factors if provided.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        weekly_df: pandas.DataFrame\n",
    "            Weekly data with technical indicators\n",
    "        daily_factors: pandas.DataFrame, optional\n",
    "            Daily factors aligned to weekly dates\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            Extracted weekly factors\n",
    "        \"\"\"\n",
    "        if daily_factors is not None:\n",
    "            # Ensure indices align\n",
    "            common_index = weekly_df.index.intersection(daily_factors.index)\n",
    "            weekly_df = weekly_df.loc[common_index]\n",
    "            daily_factors = daily_factors.loc[common_index]\n",
    "            \n",
    "            # Combine weekly data with daily factors\n",
    "            combined_df = pd.concat([weekly_df, daily_factors], axis=1)\n",
    "            print(f\"Fitting weekly model with {weekly_df.shape[1]} weekly features and {daily_factors.shape[1]} daily factors\")\n",
    "        else:\n",
    "            combined_df = weekly_df\n",
    "            print(f\"Fitting weekly model with {weekly_df.shape[1]} features (no daily factors)\")\n",
    "        \n",
    "        self.weekly_model.fit(combined_df)\n",
    "        self.weekly_factors_df = self.weekly_model.get_factors()\n",
    "        \n",
    "        print(f\"Extracted {self.weekly_factors_df.shape[1]} weekly factors\")\n",
    "        return self.weekly_factors_df\n",
    "    \n",
    "    def fit_monthly_model(self, monthly_df, weekly_factors=None):\n",
    "        \"\"\"\n",
    "        Fit the monthly DFM model, incorporating weekly factors if provided.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        monthly_df: pandas.DataFrame\n",
    "            Monthly data with technical indicators\n",
    "        weekly_factors: pandas.DataFrame, optional\n",
    "            Weekly factors aligned to monthly dates\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            Extracted monthly factors\n",
    "        \"\"\"\n",
    "        if weekly_factors is not None:\n",
    "            # Ensure indices align\n",
    "            common_index = monthly_df.index.intersection(weekly_factors.index)\n",
    "            monthly_df = monthly_df.loc[common_index]\n",
    "            weekly_factors = weekly_factors.loc[common_index]\n",
    "            \n",
    "            # Combine monthly data with weekly factors\n",
    "            combined_df = pd.concat([monthly_df, weekly_factors], axis=1)\n",
    "            print(f\"Fitting monthly model with {monthly_df.shape[1]} monthly features and {weekly_factors.shape[1]} weekly factors\")\n",
    "        else:\n",
    "            combined_df = monthly_df\n",
    "            print(f\"Fitting monthly model with {monthly_df.shape[1]} features (no weekly factors)\")\n",
    "        \n",
    "        self.monthly_model.fit(combined_df)\n",
    "        self.monthly_factors_df = self.monthly_model.get_factors()\n",
    "        \n",
    "        print(f\"Extracted {self.monthly_factors_df.shape[1]} monthly factors\")\n",
    "        return self.monthly_factors_df\n",
    "    \n",
    "    def fit_gdp_model(self, gdp_series, monthly_factors, use_ar=True):\n",
    "        \"\"\"\n",
    "        Fit the GDP prediction model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        gdp_series: pandas.Series\n",
    "            GDP growth rates\n",
    "        monthly_factors: pandas.DataFrame\n",
    "            Monthly factors aligned to quarterly dates\n",
    "        use_ar: bool\n",
    "            Whether to use autoregressive components\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self\n",
    "            Fitted instance\n",
    "        \"\"\"\n",
    "        # Align indices\n",
    "        common_index = gdp_series.index.intersection(monthly_factors.index)\n",
    "        y = gdp_series.loc[common_index]\n",
    "        X_monthly = monthly_factors.loc[common_index]\n",
    "        \n",
    "        if self.use_midas:\n",
    "            # Prepare data for MIDAS model\n",
    "            # We need to create lag structure for monthly factors\n",
    "            X_lags = []\n",
    "            for col in X_monthly.columns:\n",
    "                # Create lag matrix for each factor\n",
    "                lag_matrix = pd.DataFrame(index=X_monthly.index)\n",
    "                for lag in range(self.midas_max_lags):\n",
    "                    lag_matrix[f\"{col}_lag{lag}\"] = X_monthly[col].shift(lag)\n",
    "                \n",
    "                # Forward fill any NaNs at the beginning\n",
    "                lag_matrix = lag_matrix.fillna(method='ffill')\n",
    "                X_lags.append(lag_matrix.values)\n",
    "            \n",
    "            # Prepare autoregressive features\n",
    "            if use_ar and self.gdp_ar_lags > 0:\n",
    "                X_ar = pd.DataFrame(index=y.index)\n",
    "                for lag in range(1, self.gdp_ar_lags + 1):\n",
    "                    X_ar[f\"GDP_lag{lag}\"] = y.shift(lag)\n",
    "                \n",
    "                # Forward fill any NaNs at the beginning\n",
    "                X_ar = X_ar.fillna(method='ffill')\n",
    "                \n",
    "                # Remove rows with NaNs (initial periods where lags aren't available)\n",
    "                mask = ~X_ar.isna().any(axis=1)\n",
    "                X_ar = X_ar[mask]\n",
    "                y = y[mask]\n",
    "                X_lags = [X[mask] for X in X_lags]\n",
    "                \n",
    "                print(f\"Fitting MIDAS model with {len(X_lags)} monthly factors, {X_ar.shape[1]} GDP lags, {len(y)} observations\")\n",
    "                self.gdp_model.fit(X_lags, y, X_ar)\n",
    "            else:\n",
    "                # No autoregressive features\n",
    "                # Remove rows with NaNs in the factors\n",
    "                mask = ~pd.DataFrame(np.column_stack(X_lags)).isna().any(axis=1)\n",
    "                y = y[mask]\n",
    "                X_lags = [X[mask] for X in X_lags]\n",
    "                \n",
    "                print(f\"Fitting MIDAS model with {len(X_lags)} monthly factors, {len(y)} observations, no autoregressive terms\")\n",
    "                self.gdp_model.fit(X_lags, y)\n",
    "        else:\n",
    "            # Using standard Ridge regression\n",
    "            # Prepare autoregressive features\n",
    "            if use_ar and self.gdp_ar_lags > 0:\n",
    "                X_ar = pd.DataFrame(index=y.index)\n",
    "                for lag in range(1, self.gdp_ar_lags + 1):\n",
    "                    X_ar[f\"GDP_lag{lag}\"] = y.shift(lag)\n",
    "                \n",
    "                # Combine with monthly factors\n",
    "                X_combined = pd.concat([X_monthly, X_ar], axis=1)\n",
    "            else:\n",
    "                X_combined = X_monthly\n",
    "            \n",
    "            # Handle NaNs\n",
    "            X_combined = X_combined.fillna(method='ffill')\n",
    "            mask = ~X_combined.isna().any(axis=1) & ~y.isna()\n",
    "            X_combined = X_combined[mask]\n",
    "            y = y[mask]\n",
    "            \n",
    "            print(f\"Fitting Ridge regression with {X_combined.shape[1]} features, {len(y)} observations\")\n",
    "            self.gdp_model.fit(X_combined, y)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def fit(self, daily_df, weekly_df, monthly_df, gdp_series, \n",
    "            align_dates=True, use_ar=True):\n",
    "        \"\"\"\n",
    "        Fit the complete hierarchical model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        daily_df: pandas.DataFrame\n",
    "            Daily data with technical indicators\n",
    "        weekly_df: pandas.DataFrame\n",
    "            Weekly data with technical indicators\n",
    "        monthly_df: pandas.DataFrame\n",
    "            Monthly data with technical indicators\n",
    "        gdp_series: pandas.Series\n",
    "            GDP growth rates\n",
    "        align_dates: bool\n",
    "            Whether to automatically align dates across frequencies\n",
    "        use_ar: bool\n",
    "            Whether to use autoregressive components in each step\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self\n",
    "            Fitted instance\n",
    "        \"\"\"\n",
    "        # 1. Fit daily model and get factors\n",
    "        daily_factors = self.fit_daily_model(daily_df)\n",
    "        \n",
    "        # 2. Align daily factors to weekly dates\n",
    "        if align_dates:\n",
    "            # Find weekly dates\n",
    "            weekly_dates = weekly_df.index\n",
    "            \n",
    "            # Align daily factors to weekly dates (use last available)\n",
    "            aligned_daily_factors = pd.DataFrame(index=weekly_dates)\n",
    "            \n",
    "            for weekly_date in weekly_dates:\n",
    "                daily_data = daily_factors[daily_factors.index <= weekly_date]\n",
    "                if not daily_data.empty:\n",
    "                    aligned_daily_factors.loc[weekly_date] = daily_data.iloc[-1]\n",
    "            \n",
    "            # Forward fill any NaNs\n",
    "            aligned_daily_factors = aligned_daily_factors.fillna(method='ffill')\n",
    "        else:\n",
    "            aligned_daily_factors = daily_factors\n",
    "        \n",
    "        # 3. Fit weekly model with daily factors\n",
    "        weekly_factors = self.fit_weekly_model(weekly_df, aligned_daily_factors)\n",
    "        \n",
    "        # 4. Align weekly factors to monthly dates\n",
    "        if align_dates:\n",
    "            # Find monthly dates\n",
    "            monthly_dates = monthly_df.index\n",
    "            \n",
    "            # Align weekly factors to monthly dates (use last available)\n",
    "            aligned_weekly_factors = pd.DataFrame(index=monthly_dates)\n",
    "            \n",
    "            for monthly_date in monthly_dates:\n",
    "                weekly_data = weekly_factors[weekly_factors.index <= monthly_date]\n",
    "                if not weekly_data.empty:\n",
    "                    aligned_weekly_factors.loc[monthly_date] = weekly_data.iloc[-1]\n",
    "            \n",
    "            # Forward fill any NaNs\n",
    "            aligned_weekly_factors = aligned_weekly_factors.fillna(method='ffill')\n",
    "        else:\n",
    "            aligned_weekly_factors = weekly_factors\n",
    "        \n",
    "        # 5. Fit monthly model with weekly factors\n",
    "        monthly_factors = self.fit_monthly_model(monthly_df, aligned_weekly_factors)\n",
    "        \n",
    "        # 6. Align monthly factors to quarterly GDP dates\n",
    "        if align_dates:\n",
    "            # Find quarterly dates\n",
    "            quarterly_dates = gdp_series.index\n",
    "            \n",
    "            # Align monthly factors to quarterly dates (use last available)\n",
    "            aligned_monthly_factors = pd.DataFrame(index=quarterly_dates)\n",
    "            \n",
    "            for quarterly_date in quarterly_dates:\n",
    "                monthly_data = monthly_factors[monthly_factors.index <= quarterly_date]\n",
    "                if not monthly_data.empty:\n",
    "                    aligned_monthly_factors.loc[quarterly_date] = monthly_data.iloc[-1]\n",
    "            \n",
    "            # Forward fill any NaNs\n",
    "            aligned_monthly_factors = aligned_monthly_factors.fillna(method='ffill')\n",
    "        else:\n",
    "            aligned_monthly_factors = monthly_factors\n",
    "        \n",
    "        # 7. Fit GDP model with monthly factors\n",
    "        self.fit_gdp_model(gdp_series, aligned_monthly_factors, use_ar)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, daily_df=None, weekly_df=None, monthly_df=None, gdp_history=None,\n",
    "                predict_date=None, steps_ahead=1):\n",
    "        \"\"\"\n",
    "        Generate GDP predictions using the hierarchical model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        daily_df: pandas.DataFrame, optional\n",
    "            Daily data for prediction period\n",
    "        weekly_df: pandas.DataFrame, optional\n",
    "            Weekly data for prediction period\n",
    "        monthly_df: pandas.DataFrame, optional\n",
    "            Monthly data for prediction period\n",
    "        gdp_history: pandas.Series, optional\n",
    "            Historical GDP data for autoregressive features\n",
    "        predict_date: datetime or str, optional\n",
    "            Date for which to generate prediction\n",
    "        steps_ahead: int\n",
    "            Number of steps ahead to forecast\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.Series\n",
    "            GDP growth predictions\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model has not been fitted yet. Call fit() first.\")\n",
    "        \n",
    "        # 1. Process daily data and extract factors\n",
    "        if daily_df is not None:\n",
    "            # Transform daily data to factors\n",
    "            daily_factors = pd.DataFrame(\n",
    "                self.daily_model.transform(daily_df),\n",
    "                index=daily_df.index,\n",
    "                columns=[f\"DailyFactor{i+1}\" for i in range(self.daily_factors)]\n",
    "            )\n",
    "        else:\n",
    "            # Use existing daily factors\n",
    "            daily_factors = self.daily_factors_df\n",
    "        \n",
    "        # 2. Process weekly data and extract factors\n",
    "        if weekly_df is not None:\n",
    "            # Align daily factors to weekly dates if needed\n",
    "            if daily_factors is not None:\n",
    "                # Find weekly dates\n",
    "                weekly_dates = weekly_df.index\n",
    "                \n",
    "                # Align daily factors to weekly dates (use last available)\n",
    "                aligned_daily_factors = pd.DataFrame(index=weekly_dates)\n",
    "                \n",
    "                for weekly_date in weekly_dates:\n",
    "                    daily_data = daily_factors[daily_factors.index <= weekly_date]\n",
    "                    if not daily_data.empty:\n",
    "                        aligned_daily_factors.loc[weekly_date] = daily_data.iloc[-1]\n",
    "                \n",
    "                # Forward fill any NaNs\n",
    "                aligned_daily_factors = aligned_daily_factors.fillna(method='ffill')\n",
    "                \n",
    "                # Combine weekly data with aligned daily factors\n",
    "                combined_weekly = pd.concat([weekly_df, aligned_daily_factors], axis=1)\n",
    "            else:\n",
    "                combined_weekly = weekly_df\n",
    "            \n",
    "            # Transform to weekly factors\n",
    "            weekly_factors = pd.DataFrame(\n",
    "                self.weekly_model.transform(combined_weekly),\n",
    "                index=weekly_df.index,\n",
    "                columns=[f\"WeeklyFactor{i+1}\" for i in range(self.weekly_factors)]\n",
    "            )\n",
    "        else:\n",
    "            # Use existing weekly factors\n",
    "            weekly_factors = self.weekly_factors_df\n",
    "        \n",
    "        # 3. Process monthly data and extract factors\n",
    "        if monthly_df is not None:\n",
    "            # Align weekly factors to monthly dates if needed\n",
    "            if weekly_factors is not None:\n",
    "                # Find monthly dates\n",
    "                monthly_dates = monthly_df.index\n",
    "                \n",
    "                # Align weekly factors to monthly dates (use last available)\n",
    "                aligned_weekly_factors = pd.DataFrame(index=monthly_dates)\n",
    "                \n",
    "                for monthly_date in monthly_dates:\n",
    "                    weekly_data = weekly_factors[weekly_factors.index <= monthly_date]\n",
    "                    if not weekly_data.empty:\n",
    "                        aligned_weekly_factors.loc[monthly_date] = weekly_data.iloc[-1]\n",
    "                \n",
    "                # Forward fill any NaNs\n",
    "                aligned_weekly_factors = aligned_weekly_factors.fillna(method='ffill')\n",
    "                \n",
    "                # Combine monthly data with aligned weekly factors\n",
    "                combined_monthly = pd.concat([monthly_df, aligned_weekly_factors], axis=1)\n",
    "            else:\n",
    "                combined_monthly = monthly_df\n",
    "            \n",
    "            # Transform to monthly factors\n",
    "            monthly_factors = pd.DataFrame(\n",
    "                self.monthly_model.transform(combined_monthly),\n",
    "                index=monthly_df.index,\n",
    "                columns=[f\"MonthlyFactor{i+1}\" for i in range(self.monthly_factors)]\n",
    "            )\n",
    "        else:\n",
    "            # Use existing monthly factors\n",
    "            monthly_factors = self.monthly_factors_df\n",
    "        \n",
    "        # 4. Use monthly factors to predict GDP\n",
    "        if predict_date is not None:\n",
    "            # Filter data up to predict_date\n",
    "            monthly_factors = monthly_factors[monthly_factors.index <= predict_date]\n",
    "        \n",
    "        # If we have a GDP model using MIDAS\n",
    "        if self.use_midas:\n",
    "            # Prepare data for MIDAS model\n",
    "            # We need to create lag structure for monthly factors\n",
    "            X_lags = []\n",
    "            for col in monthly_factors.columns:\n",
    "                # Create lag matrix for each factor\n",
    "                lag_matrix = pd.DataFrame(index=monthly_factors.index)\n",
    "                for lag in range(self.midas_max_lags):\n",
    "                    lag_matrix[f\"{col}_lag{lag}\"] = monthly_factors[col].shift(lag)\n",
    "                \n",
    "                # Forward fill any NaNs at the beginning\n",
    "                lag_matrix = lag_matrix.fillna(method='ffill')\n",
    "                X_lags.append(lag_matrix.values)\n",
    "            \n",
    "            # Prepare autoregressive features if needed\n",
    "            if gdp_history is not None and self.gdp_ar_lags > 0:\n",
    "                X_ar = pd.DataFrame(index=monthly_factors.index)\n",
    "                for lag in range(1, self.gdp_ar_lags + 1):\n",
    "                    X_ar[f\"GDP_lag{lag}\"] = gdp_history.shift(lag)\n",
    "                \n",
    "                # Forward fill any NaNs at the beginning\n",
    "                X_ar = X_ar.fillna(method='ffill')\n",
    "                \n",
    "                # Make prediction\n",
    "                gdp_pred = self.gdp_model.predict(X_lags, X_ar)\n",
    "            else:\n",
    "                # No autoregressive features\n",
    "                gdp_pred = self.gdp_model.predict(X_lags)\n",
    "        else:\n",
    "            # Using standard Ridge regression\n",
    "            # Prepare autoregressive features if needed\n",
    "            if gdp_history is not None and self.gdp_ar_lags > 0:\n",
    "                X_ar = pd.DataFrame(index=monthly_factors.index)\n",
    "                for lag in range(1, self.gdp_ar_lags + 1):\n",
    "                    X_ar[f\"GDP_lag{lag}\"] = gdp_history.shift(lag)\n",
    "                \n",
    "                # Combine with monthly factors\n",
    "                X_combined = pd.concat([monthly_factors, X_ar], axis=1)\n",
    "            else:\n",
    "                X_combined = monthly_factors\n",
    "            \n",
    "            # Handle NaNs\n",
    "            X_combined = X_combined.fillna(method='ffill')\n",
    "            \n",
    "            # Make prediction\n",
    "            gdp_pred = self.gdp_model.predict(X_combined)\n",
    "        \n",
    "        # Convert to pandas Series\n",
    "        gdp_predictions = pd.Series(gdp_pred, index=monthly_factors.index, name=\"GDP_prediction\")\n",
    "        \n",
    "        return gdp_predictions\n",
    "    \n",
    "    def get_factor_loadings(self):\n",
    "        \"\"\"\n",
    "        Get factor loadings for all levels.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of factor loadings\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        loadings = {\n",
    "            'daily': self.daily_model.get_factor_loadings(),\n",
    "            'weekly': self.weekly_model.get_factor_loadings(),\n",
    "            'monthly': self.monthly_model.get_factor_loadings()\n",
    "        }\n",
    "        \n",
    "        return loadings\n",
    "    \n",
    "    def get_factors(self):\n",
    "        \"\"\"\n",
    "        Get extracted factors for all levels.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of factors\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        factors = {\n",
    "            'daily': self.daily_factors_df,\n",
    "            'weekly': self.weekly_factors_df,\n",
    "            'monthly': self.monthly_factors_df\n",
    "        }\n",
    "        \n",
    "        return factors\n",
    "    \n",
    "    def get_midas_weights(self):\n",
    "        \"\"\"\n",
    "        Get MIDAS weights if using MIDAS model.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of MIDAS weights\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        if not self.use_midas:\n",
    "            return None\n",
    "        \n",
    "        return self.gdp_model.get_midas_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fae684-a078-44c5-b8dd-278349a88ba6",
   "metadata": {},
   "source": [
    "# Module 6: Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22acb9d5-27f0-413a-939b-16b7ff988aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDPForecastEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluation framework for GDP forecasting models.\n",
    "    \n",
    "    This class provides comprehensive evaluation metrics and visualizations\n",
    "    for GDP forecasting performance.\n",
    "    \n",
    "    References:\n",
    "    -----------\n",
    "    - Clements, M. P., & Hendry, D. F. (1993). On the limitations of comparing mean \n",
    "      square forecast errors. Journal of Forecasting, 12(8), 617-637.\n",
    "    - Diebold, F. X., & Mariano, R. S. (1995). Comparing predictive accuracy. \n",
    "      Journal of Business & Economic Statistics, 13(3), 253-263.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the evaluator.\"\"\"\n",
    "        self.results = {}\n",
    "        self.models = {}\n",
    "        self.actual = None\n",
    "    \n",
    "    def add_model(self, name, predictions, actual=None):\n",
    "        \"\"\"\n",
    "        Add a model's predictions for evaluation.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        name: str\n",
    "            Model name\n",
    "        predictions: pandas.Series\n",
    "            Predicted GDP values\n",
    "        actual: pandas.Series, optional\n",
    "            Actual GDP values (if not already set)\n",
    "        \"\"\"\n",
    "        self.models[name] = predictions\n",
    "        \n",
    "        if actual is not None and self.actual is None:\n",
    "            self.actual = actual\n",
    "    \n",
    "    def calculate_metrics(self, rolling_window=None):\n",
    "        \"\"\"\n",
    "        Calculate evaluation metrics for all models.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        rolling_window: int, optional\n",
    "            Window size for rolling metrics calculation\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        if self.actual is None:\n",
    "            raise ValueError(\"Actual values not set. Provide actual values when adding a model.\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for model_name, predictions in self.models.items():\n",
    "            # Align predictions with actual values\n",
    "            common_index = self.actual.index.intersection(predictions.index)\n",
    "            y_true = self.actual.loc[common_index]\n",
    "            y_pred = predictions.loc[common_index]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = self._calculate_model_metrics(y_true, y_pred, model_name)\n",
    "            \n",
    "            # Add rolling metrics if requested\n",
    "            if rolling_window is not None and len(y_true) > rolling_window:\n",
    "                rolling_metrics = self._calculate_rolling_metrics(y_true, y_pred, rolling_window)\n",
    "                metrics.update(rolling_metrics)\n",
    "            \n",
    "            results[model_name] = metrics\n",
    "        \n",
    "        self.results = results\n",
    "        return results\n",
    "    \n",
    "    def _calculate_model_metrics(self, y_true, y_pred, model_name):\n",
    "        \"\"\"\n",
    "        Calculate comprehensive evaluation metrics for a model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_true: pandas.Series\n",
    "            Actual values\n",
    "        y_pred: pandas.Series\n",
    "            Predicted values\n",
    "        model_name: str\n",
    "            Model name\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "        \n",
    "        # Calculate basic error metrics\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        # Calculate directional accuracy\n",
    "        direction_true = np.sign(y_true.diff().fillna(0))\n",
    "        direction_pred = np.sign(y_pred.diff().fillna(0))\n",
    "        \n",
    "        # Ignore zero changes\n",
    "        nonzero_mask = direction_true != 0\n",
    "        if nonzero_mask.any():\n",
    "            direction_accuracy = np.mean(direction_true[nonzero_mask] == direction_pred[nonzero_mask])\n",
    "        else:\n",
    "            direction_accuracy = np.nan\n",
    "        \n",
    "        # Calculate mean absolute percentage error\n",
    "        # Use a safe version to handle zeros\n",
    "        nonzero_mask = y_true != 0\n",
    "        if nonzero_mask.any():\n",
    "            mape = np.mean(np.abs((y_true[nonzero_mask] - y_pred[nonzero_mask]) / y_true[nonzero_mask])) * 100\n",
    "        else:\n",
    "            mape = np.nan\n",
    "        \n",
    "        # Calculate Theil's U statistic\n",
    "        # U = sqrt(MSE(model)) / sqrt(MSE(naive))\n",
    "        # Naive forecast is previous value (no change)\n",
    "        naive_pred = y_true.shift(1).fillna(method='bfill')\n",
    "        naive_mse = mean_squared_error(y_true[1:], naive_pred[1:])\n",
    "        \n",
    "        if naive_mse > 0:\n",
    "            theils_u = np.sqrt(mse) / np.sqrt(naive_mse)\n",
    "        else:\n",
    "            theils_u = np.nan\n",
    "        \n",
    "        # Calculate advanced forecast accuracy metrics\n",
    "        \n",
    "        # Mean Directional Accuracy (MDA)\n",
    "        actual_changes = y_true.diff().fillna(0)\n",
    "        predicted_changes = y_pred.diff().fillna(0)\n",
    "        mda = np.mean((actual_changes * predicted_changes) > 0)\n",
    "        \n",
    "        # Confusion matrix for directional forecasts\n",
    "        direction_true_binary = (actual_changes > 0).astype(int)\n",
    "        direction_pred_binary = (predicted_changes > 0).astype(int)\n",
    "        \n",
    "        true_pos = np.sum((direction_true_binary == 1) & (direction_pred_binary == 1))\n",
    "        false_pos = np.sum((direction_true_binary == 0) & (direction_pred_binary == 1))\n",
    "        true_neg = np.sum((direction_true_binary == 0) & (direction_pred_binary == 0))\n",
    "        false_neg = np.sum((direction_true_binary == 1) & (direction_pred_binary == 0))\n",
    "        \n",
    "        # Hit rate (% of positive changes correctly predicted)\n",
    "        if (true_pos + false_neg) > 0:\n",
    "            hit_rate = true_pos / (true_pos + false_neg)\n",
    "        else:\n",
    "            hit_rate = np.nan\n",
    "        \n",
    "        # False alarm rate (% of negative changes incorrectly predicted as positive)\n",
    "        if (false_pos + true_neg) > 0:\n",
    "            false_alarm_rate = false_pos / (false_pos + true_neg)\n",
    "        else:\n",
    "            false_alarm_rate = np.nan\n",
    "        \n",
    "        # Calculate over/underprediction bias\n",
    "        bias = np.mean(y_pred - y_true)\n",
    "        \n",
    "        # Create results dictionary\n",
    "        metrics = {\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'mape': mape,\n",
    "            'r2': r2,\n",
    "            'direction_accuracy': direction_accuracy,\n",
    "            'theils_u': theils_u,\n",
    "            'mean_directional_accuracy': mda,\n",
    "            'hit_rate': hit_rate,\n",
    "            'false_alarm_rate': false_alarm_rate,\n",
    "            'bias': bias,\n",
    "            'confusion_matrix': {\n",
    "                'true_pos': true_pos,\n",
    "                'false_pos': false_pos,\n",
    "                'true_neg': true_neg,\n",
    "                'false_neg': false_neg\n",
    "            },\n",
    "            'forecast_errors': y_pred - y_true\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _calculate_rolling_metrics(self, y_true, y_pred, window):\n",
    "        \"\"\"\n",
    "        Calculate rolling evaluation metrics.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_true: pandas.Series\n",
    "            Actual values\n",
    "        y_pred: pandas.Series\n",
    "            Predicted values\n",
    "        window: int\n",
    "            Rolling window size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of rolling metrics\n",
    "        \"\"\"\n",
    "        # Initialize rolling metrics\n",
    "        rolling_rmse = []\n",
    "        rolling_mae = []\n",
    "        rolling_direction_accuracy = []\n",
    "        \n",
    "        # Loop through rolling windows\n",
    "        for i in range(len(y_true) - window + 1):\n",
    "            window_true = y_true.iloc[i:i+window]\n",
    "            window_pred = y_pred.iloc[i:i+window]\n",
    "            \n",
    "            # Calculate metrics for this window\n",
    "            mse = np.mean((window_true - window_pred) ** 2)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = np.mean(np.abs(window_true - window_pred))\n",
    "            \n",
    "            # Calculate directional accuracy\n",
    "            direction_true = np.sign(window_true.diff().fillna(0))\n",
    "            direction_pred = np.sign(window_pred.diff().fillna(0))\n",
    "            \n",
    "            # Ignore zero changes\n",
    "            nonzero_mask = direction_true != 0\n",
    "            if nonzero_mask.any():\n",
    "                direction_accuracy = np.mean(direction_true[nonzero_mask] == direction_pred[nonzero_mask])\n",
    "            else:\n",
    "                direction_accuracy = np.nan\n",
    "            \n",
    "            # Add to lists\n",
    "            rolling_rmse.append(rmse)\n",
    "            rolling_mae.append(mae)\n",
    "            rolling_direction_accuracy.append(direction_accuracy)\n",
    "        \n",
    "        # Convert to pandas Series with appropriate index\n",
    "        index = y_true.index[window-1:]\n",
    "        rolling_metrics = {\n",
    "            'rolling_rmse': pd.Series(rolling_rmse, index=index[:len(rolling_rmse)]),\n",
    "            'rolling_mae': pd.Series(rolling_mae, index=index[:len(rolling_mae)]),\n",
    "            'rolling_direction_accuracy': pd.Series(rolling_direction_accuracy, index=index[:len(rolling_direction_accuracy)])\n",
    "        }\n",
    "        \n",
    "        return rolling_metrics\n",
    "    \n",
    "    def diebold_mariano_test(self, model1, model2, alternative='two-sided'):\n",
    "        \"\"\"\n",
    "        Perform Diebold-Mariano test to compare forecast accuracy.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model1: str\n",
    "            First model name\n",
    "        model2: str\n",
    "            Second model name\n",
    "        alternative: str\n",
    "            Alternative hypothesis ('two-sided', 'less', 'greater')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            DM statistic and p-value\n",
    "            \n",
    "        References:\n",
    "        -----------\n",
    "        - Diebold, F. X., & Mariano, R. S. (1995). Comparing predictive accuracy. \n",
    "          Journal of Business & Economic Statistics, 13(3), 253-263.\n",
    "        \"\"\"\n",
    "        import statsmodels.api as sm\n",
    "        \n",
    "        if model1 not in self.models or model2 not in self.models:\n",
    "            raise ValueError(f\"Models {model1} and/or {model2} not found\")\n",
    "        \n",
    "        # Get predictions\n",
    "        pred1 = self.models[model1]\n",
    "        pred2 = self.models[model2]\n",
    "        \n",
    "        # Align predictions with actual values\n",
    "        common_index = self.actual.index.intersection(pred1.index).intersection(pred2.index)\n",
    "        y_true = self.actual.loc[common_index]\n",
    "        y_pred1 = pred1.loc[common_index]\n",
    "        y_pred2 = pred2.loc[common_index]\n",
    "        \n",
    "        # Calculate squared errors\n",
    "        error1 = (y_true - y_pred1) ** 2\n",
    "        error2 = (y_true - y_pred2) ** 2\n",
    "        \n",
    "        # Calculate loss differential\n",
    "        d = error1 - error2\n",
    "        \n",
    "        # Calculate DM statistic\n",
    "        n = len(d)\n",
    "        if n <= 1:\n",
    "            return np.nan, np.nan\n",
    "        \n",
    "        # Estimate lag-1 autocorrelation of loss differential\n",
    "        acf_result = sm.tsa.acf(d, nlags=1, fft=False)\n",
    "        gamma_0 = acf_result[0]  # This is the variance of d\n",
    "        gamma_1 = acf_result[1] * gamma_0  # Autocovariance at lag 1\n",
    "        \n",
    "        # Calculate long-run variance with Newey-West correction for autocorrelation\n",
    "        lrvar = gamma_0 + 2 * gamma_1\n",
    "        \n",
    "        # Calculate DM statistic\n",
    "        dm_stat = d.mean() / np.sqrt(lrvar / n)\n",
    "        \n",
    "        # Calculate p-value based on alternative hypothesis\n",
    "        if alternative == 'two-sided':\n",
    "            p_value = 2 * (1 - norm.cdf(np.abs(dm_stat)))\n",
    "        elif alternative == 'less':\n",
    "            p_value = norm.cdf(dm_stat)\n",
    "        elif alternative == 'greater':\n",
    "            p_value = 1 - norm.cdf(dm_stat)\n",
    "        else:\n",
    "            raise ValueError(\"alternative must be 'two-sided', 'less', or 'greater'\")\n",
    "        \n",
    "        return dm_stat, p_value\n",
    "    \n",
    "    def plot_forecasts(self, start_date=None, end_date=None, figsize=(12, 6)):\n",
    "        \"\"\"\n",
    "        Plot actual vs predicted GDP.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        start_date: str or datetime, optional\n",
    "            Start date for plot\n",
    "        end_date: str or datetime, optional\n",
    "            End date for plot\n",
    "        figsize: tuple\n",
    "            Figure size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            Figure object\n",
    "        \"\"\"\n",
    "        if self.actual is None:\n",
    "            raise ValueError(\"Actual values not set\")\n",
    "        \n",
    "        # Filter by date range if provided\n",
    "        actual = self.actual\n",
    "        if start_date is not None:\n",
    "            actual = actual[actual.index >= pd.to_datetime(start_date)]\n",
    "        if end_date is not None:\n",
    "            actual = actual[actual.index <= pd.to_datetime(end_date)]\n",
    "        \n",
    "        # Create plot\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        # Plot actual values\n",
    "        ax.plot(actual.index, actual, 'k-', linewidth=2, label='Actual GDP')\n",
    "        \n",
    "        # Plot predictions for each model\n",
    "        colors = plt.cm.tab10.colors\n",
    "        for i, (model_name, predictions) in enumerate(self.models.items()):\n",
    "            # Filter predictions by date range\n",
    "            pred = predictions\n",
    "            if start_date is not None:\n",
    "                pred = pred[pred.index >= pd.to_datetime(start_date)]\n",
    "            if end_date is not None:\n",
    "                pred = pred[pred.index <= pd.to_datetime(end_date)]\n",
    "            \n",
    "            # Only use shared dates\n",
    "            common_index = actual.index.intersection(pred.index)\n",
    "            pred = pred.loc[common_index]\n",
    "            \n",
    "            color = colors[i % len(colors)]\n",
    "            ax.plot(pred.index, pred, 'o-', color=color, linewidth=1.5, label=f'{model_name}')\n",
    "        \n",
    "        # Add recession shading if available\n",
    "        try:\n",
    "            from pandas_datareader.data import DataReader\n",
    "            from pandas_datareader._utils import RemoteDataError\n",
    "            \n",
    "            try:\n",
    "                # Get US recession data from FRED\n",
    "                recession = DataReader('USREC', 'fred', start=actual.index[0], end=actual.index[-1])\n",
    "                \n",
    "                # Create shaded regions for recessions\n",
    "                last_date = None\n",
    "                for date, value in recession.itertuples():\n",
    "                    if value == 1.0:  # Recession period\n",
    "                        if last_date is None:\n",
    "                            last_date = date\n",
    "                    elif last_date is not None:\n",
    "                        # End of recession period\n",
    "                        ax.axvspan(last_date, date, alpha=0.2, color='gray')\n",
    "                        last_date = None\n",
    "                \n",
    "                # Handle case where we're still in a recession at the end of the data\n",
    "                if last_date is not None:\n",
    "                    ax.axvspan(last_date, actual.index[-1], alpha=0.2, color='gray')\n",
    "            except RemoteDataError:\n",
    "                print(\"Could not retrieve recession data from FRED\")\n",
    "        except ImportError:\n",
    "            print(\"pandas_datareader not available for recession shading\")\n",
    "        \n",
    "        # Add legend, grid, labels, etc.\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('GDP Growth (%)')\n",
    "        ax.set_title('GDP Growth: Actual vs Predicted')\n",
    "        ax.legend(loc='best')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Format y-axis to show percentage\n",
    "        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.1f}%'))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def plot_error_distribution(self, figsize=(12, 8)):\n",
    "        \"\"\"\n",
    "        Plot error distributions for all models.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        figsize: tuple\n",
    "            Figure size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            Figure object\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            self.calculate_metrics()\n",
    "        \n",
    "        n_models = len(self.models)\n",
    "        fig, axes = plt.subplots(n_models, 2, figsize=figsize)\n",
    "        \n",
    "        # Handle case with single model\n",
    "        if n_models == 1:\n",
    "            axes = axes.reshape(1, 2)\n",
    "        \n",
    "        # Iterate through models\n",
    "        for i, (model_name, metrics) in enumerate(self.results.items()):\n",
    "            errors = metrics['forecast_errors']\n",
    "            \n",
    "            # Histogram of errors\n",
    "            bins = min(20, max(5, int(np.sqrt(len(errors)))))\n",
    "            axes[i, 0].hist(errors, bins=bins, alpha=0.7, edgecolor='black')\n",
    "            axes[i, 0].axvline(x=0, color='r', linestyle='--')\n",
    "            axes[i, 0].set_title(f'{model_name}: Error Distribution')\n",
    "            axes[i, 0].set_xlabel('Forecast Error (Predicted - Actual)')\n",
    "            axes[i, 0].set_ylabel('Frequency')\n",
    "            \n",
    "            # Add metrics to plot\n",
    "            metrics_text = (\n",
    "                f\"RMSE: {metrics['rmse']:.4f}\\n\"\n",
    "                f\"MAE: {metrics['mae']:.4f}\\n\"\n",
    "                f\"Bias: {metrics['bias']:.4f}\\n\"\n",
    "                f\"Dir. Acc: {metrics['direction_accuracy']:.2f}\"\n",
    "            )\n",
    "            axes[i, 0].annotate(\n",
    "                metrics_text, xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                va='top', ha='left', bbox=dict(boxstyle='round', fc='white', alpha=0.7)\n",
    "            )\n",
    "            \n",
    "            # Q-Q plot\n",
    "            from scipy import stats\n",
    "            \n",
    "            # Get z-scores for normal distribution\n",
    "            z = (errors - errors.mean()) / errors.std()\n",
    "            \n",
    "            # Create Q-Q plot\n",
    "            stats.probplot(z, dist=\"norm\", plot=axes[i, 1])\n",
    "            axes[i, 1].set_title(f'{model_name}: Q-Q Plot')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def plot_rolling_metrics(self, window=8, figsize=(12, 15)):\n",
    "        \"\"\"\n",
    "        Plot rolling metrics for all models.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        window: int\n",
    "            Rolling window size\n",
    "        figsize: tuple\n",
    "            Figure size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            Figure object\n",
    "        \"\"\"\n",
    "        # Ensure we have rolling metrics\n",
    "        if not self.results or 'rolling_rmse' not in next(iter(self.results.values())):\n",
    "            self.calculate_metrics(rolling_window=window)\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 1, figsize=figsize)\n",
    "        \n",
    "        # Plot rolling RMSE\n",
    "        for model_name, metrics in self.results.items():\n",
    "            axes[0].plot(\n",
    "                metrics['rolling_rmse'].index,\n",
    "                metrics['rolling_rmse'],\n",
    "                'o-',\n",
    "                label=model_name\n",
    "            )\n",
    "        \n",
    "        axes[0].set_title(f'Rolling RMSE ({window}-quarter window)')\n",
    "        axes[0].set_ylabel('RMSE')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].legend(loc='best')\n",
    "        \n",
    "        # Plot rolling MAE\n",
    "        for model_name, metrics in self.results.items():\n",
    "            axes[1].plot(\n",
    "                metrics['rolling_mae'].index,\n",
    "                metrics['rolling_mae'],\n",
    "                'o-',\n",
    "                label=model_name\n",
    "            )\n",
    "        \n",
    "        axes[1].set_title(f'Rolling MAE ({window}-quarter window)')\n",
    "        axes[1].set_ylabel('MAE')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].legend(loc='best')\n",
    "        \n",
    "        # Plot rolling direction accuracy\n",
    "        for model_name, metrics in self.results.items():\n",
    "            axes[2].plot(\n",
    "                metrics['rolling_direction_accuracy'].index,\n",
    "                metrics['rolling_direction_accuracy'],\n",
    "                'o-',\n",
    "                label=model_name\n",
    "            )\n",
    "        \n",
    "        axes[2].set_title(f'Rolling Direction Accuracy ({window}-quarter window)')\n",
    "        axes[2].set_ylabel('Direction Accuracy')\n",
    "        axes[2].set_ylim(0, 1)\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        axes[2].legend(loc='best')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def generate_report(self, output_file=None, include_plots=True):\n",
    "        \"\"\"\n",
    "        Generate comprehensive evaluation report.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_file: str, optional\n",
    "            Path to save report (HTML or markdown)\n",
    "        include_plots: bool\n",
    "            Whether to include plots in the report\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            Report content\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            self.calculate_metrics()\n",
    "        \n",
    "        # Start building report\n",
    "        report = \"# GDP Forecasting Model Evaluation Report\\n\\n\"\n",
    "        report += f\"Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\\n\\n\"\n",
    "        \n",
    "        # Add model summary\n",
    "        report += \"## Models Evaluated\\n\\n\"\n",
    "        report += f\"Number of models: {len(self.models)}\\n\"\n",
    "        report += f\"Evaluation period: {self.actual.index[0]} to {self.actual.index[-1]}\\n\"\n",
    "        report += f\"Number of observations: {len(self.actual)}\\n\\n\"\n",
    "        \n",
    "        # Add performance metrics table\n",
    "        report += \"## Performance Metrics\\n\\n\"\n",
    "        report += \"| Model | RMSE | MAE | MAPE | R² | Direction Accuracy | Theil's U | Bias |\\n\"\n",
    "        report += \"|-------|------|-----|------|----|--------------------|-----------|------|\\n\"\n",
    "        \n",
    "        for model_name, metrics in self.results.items():\n",
    "            report += (\n",
    "                f\"| {model_name} | \"\n",
    "                f\"{metrics['rmse']:.4f} | \"\n",
    "                f\"{metrics['mae']:.4f} | \"\n",
    "                f\"{metrics['mape']:.2f}% | \"\n",
    "                f\"{metrics['r2']:.4f} | \"\n",
    "                f\"{metrics['direction_accuracy']:.2f} | \"\n",
    "                f\"{metrics['theils_u']:.4f} | \"\n",
    "                f\"{metrics['bias']:.4f} |\\n\"\n",
    "            )\n",
    "        \n",
    "        report += \"\\n\"\n",
    "        \n",
    "        # Add detailed analysis for each model\n",
    "        report += \"## Detailed Model Analysis\\n\\n\"\n",
    "        \n",
    "        for model_name, metrics in self.results.items():\n",
    "            report += f\"### {model_name}\\n\\n\"\n",
    "            \n",
    "            # Confusion matrix\n",
    "            cm = metrics['confusion_matrix']\n",
    "            report += \"#### Directional Forecast Confusion Matrix\\n\\n\"\n",
    "            report += \"| | Predicted Up | Predicted Down |\\n\"\n",
    "            report += \"|------------|--------------|----------------|\\n\"\n",
    "            report += f\"| **Actual Up** | {cm['true_pos']} | {cm['false_neg']} |\\n\"\n",
    "            report += f\"| **Actual Down** | {cm['false_pos']} | {cm['true_neg']} |\\n\\n\"\n",
    "            \n",
    "            # Additional metrics\n",
    "            report += \"#### Additional Metrics\\n\\n\"\n",
    "            report += f\"* Mean Directional Accuracy: {metrics['mean_directional_accuracy']:.4f}\\n\"\n",
    "            report += f\"* Hit Rate (% of Up movements correctly predicted): {metrics['hit_rate']:.4f}\\n\"\n",
    "            report += f\"* False Alarm Rate: {metrics['false_alarm_rate']:.4f}\\n\"\n",
    "            report += f\"* Bias (Average overestimation): {metrics['bias']:.4f}\\n\\n\"\n",
    "        \n",
    "        # Add model comparison using Diebold-Mariano test\n",
    "        if len(self.models) > 1:\n",
    "            report += \"## Model Comparison: Diebold-Mariano Test\\n\\n\"\n",
    "            report += \"| Model 1 | Model 2 | DM Statistic | p-value | Conclusion |\\n\"\n",
    "            report += \"|---------|---------|--------------|---------|------------|\\n\"\n",
    "            \n",
    "            models = list(self.models.keys())\n",
    "            for i in range(len(models)):\n",
    "                for j in range(i+1, len(models)):\n",
    "                    dm_stat, p_value = self.diebold_mariano_test(models[i], models[j])\n",
    "                    \n",
    "                    # Determine conclusion\n",
    "                    if p_value < 0.01:\n",
    "                        significance = \"***\"\n",
    "                    elif p_value < 0.05:\n",
    "                        significance = \"**\"\n",
    "                    elif p_value < 0.1:\n",
    "                        significance = \"*\"\n",
    "                    else:\n",
    "                        significance = \"\"\n",
    "                    \n",
    "                    if np.isnan(dm_stat) or np.isnan(p_value):\n",
    "                        conclusion = \"Insufficient data\"\n",
    "                    elif p_value < 0.05:\n",
    "                        if dm_stat > 0:\n",
    "                            conclusion = f\"Model 2 is more accurate {significance}\"\n",
    "                        else:\n",
    "                            conclusion = f\"Model 1 is more accurate {significance}\"\n",
    "                    else:\n",
    "                        conclusion = \"No significant difference\"\n",
    "                    \n",
    "                    report += (\n",
    "                        f\"| {models[i]} | {models[j]} | \"\n",
    "                        f\"{dm_stat:.4f} | {p_value:.4f} | {conclusion} |\\n\"\n",
    "                    )\n",
    "            \n",
    "            report += \"\\n*Significance levels: *** = 1%, ** = 5%, * = 10%\\n\\n\"\n",
    "        \n",
    "        # Add conclusion\n",
    "        report += \"## Conclusion\\n\\n\"\n",
    "        \n",
    "        # Determine best model based on metrics\n",
    "        rmse_ranking = {model: metrics['rmse'] for model, metrics in self.results.items()}\n",
    "        best_rmse = min(rmse_ranking.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        dir_acc_ranking = {model: metrics['direction_accuracy'] for model, metrics in self.results.items()}\n",
    "        best_dir_acc = max(dir_acc_ranking.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        report += f\"Based on RMSE, the best performing model is **{best_rmse}**.\\n\\n\"\n",
    "        report += f\"Based on directional accuracy, the best performing model is **{best_dir_acc}**.\\n\\n\"\n",
    "        \n",
    "        # If plots are included and an output file is specified\n",
    "        if include_plots and output_file:\n",
    "            # Save plots to files\n",
    "            import os\n",
    "            output_dir = os.path.dirname(output_file)\n",
    "            if output_dir and not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            \n",
    "            # Base file name without extension\n",
    "            base_name = os.path.splitext(output_file)[0]\n",
    "            \n",
    "            # Forecast plot\n",
    "            forecast_plot_path = f\"{base_name}_forecasts.png\"\n",
    "            fig = self.plot_forecasts()\n",
    "            fig.savefig(forecast_plot_path)\n",
    "            plt.close(fig)\n",
    "            \n",
    "            # Error distribution plot\n",
    "            error_plot_path = f\"{base_name}_errors.png\"\n",
    "            fig = self.plot_error_distribution()\n",
    "            fig.savefig(error_plot_path)\n",
    "            plt.close(fig)\n",
    "            \n",
    "            # Rolling metrics plot\n",
    "            rolling_plot_path = f\"{base_name}_rolling.png\"\n",
    "            fig = self.plot_rolling_metrics()\n",
    "            fig.savefig(rolling_plot_path)\n",
    "            plt.close(fig)\n",
    "            \n",
    "            # Add images to report\n",
    "            report += \"## Visualizations\\n\\n\"\n",
    "            report += \"### Forecast Comparison\\n\\n\"\n",
    "            report += f\"![Forecast Comparison]({os.path.basename(forecast_plot_path)})\\n\\n\"\n",
    "            \n",
    "            report += \"### Error Distribution\\n\\n\"\n",
    "            report += f\"![Error Distribution]({os.path.basename(error_plot_path)})\\n\\n\"\n",
    "            \n",
    "            report += \"### Rolling Metrics\\n\\n\"\n",
    "            report += f\"![Rolling Metrics]({os.path.basename(rolling_plot_path)})\\n\\n\"\n",
    "        \n",
    "        # Save report to file if specified\n",
    "        if output_file:\n",
    "            with open(output_file, 'w') as f:\n",
    "                f.write(report)\n",
    "        \n",
    "        return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b7e5a4-ad8e-4049-baa3-e55a0ac6e875",
   "metadata": {},
   "source": [
    "# Module 7: Main Workflow Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b32e0be9-22b3-4df2-9900-2eca25c3f249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting GDP Forecasting Workflow at 2025-06-04 17:51:27.404505\n",
      "================================================================================\n",
      "\n",
      "1. Setting up configuration...\n",
      "Configuration set up with 7 daily files, 2 weekly files, 7 monthly files, 1 quarterly files\n",
      "\n",
      "2. Data Preprocessing...\n",
      "Found 18 files in ./Project_Data\n",
      "Processing daily data...\n",
      "Processed Oil_Investing_Fixed.csv: 10950 observations, 2 features\n",
      "Processed Gold_Investing_fixed.csv: 13099 observations, 2 features\n",
      "Processed SPX.csv: 25362 observations, 2 features\n",
      "Processed 10Y-03M_YieldCurve_daily.csv: 11271 observations, 2 features\n",
      "Processed 10Y-02Y_YieldCurve_daily.csv: 12730 observations, 2 features\n",
      "Processed COPPER_Macrotrends_1959.csv: 17144 observations, 2 features\n",
      "Processed Lumber_daily_macrotrends.csv: 13672 observations, 2 features\n",
      "Created ratio: Copper_Gold_Ratio with 17145 observations\n",
      "Created ratio: Lumber_Gold_Ratio with 13672 observations\n",
      "Final daily dataset: 25381 observations, 18 features\n",
      "Processing weekly data...\n",
      "Processed InitialUnemploymentClaims_weekly.csv: 3035 observations, 2 features\n",
      "Processed Fiancial_conditions.csv: 2830 observations, 2 features\n",
      "Final weekly dataset: 3038 observations, 4 features\n",
      "Processing monthly data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/msspbvw50gdgd13489tvcll00000gn/T/ipykernel_54192/2458607752.py:139: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  df = df.asfreq('M', method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed CPI_mon_monthly.csv: 829 observations, 2 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/msspbvw50gdgd13489tvcll00000gn/T/ipykernel_54192/2458607752.py:139: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  df = df.asfreq('M', method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Unemployment_monthly.csv: 925 observations, 2 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/msspbvw50gdgd13489tvcll00000gn/T/ipykernel_54192/2458607752.py:139: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  df = df.asfreq('M', method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed InterestRate_monthly.csv: 847 observations, 2 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/msspbvw50gdgd13489tvcll00000gn/T/ipykernel_54192/2458607752.py:139: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  df = df.asfreq('M', method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed HousingStarts_monthly.csv: 792 observations, 2 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/msspbvw50gdgd13489tvcll00000gn/T/ipykernel_54192/2458607752.py:139: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  df = df.asfreq('M', method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Heavy_Truck_Sales.csv: 698 observations, 2 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/msspbvw50gdgd13489tvcll00000gn/T/ipykernel_54192/2458607752.py:139: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  df = df.asfreq('M', method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Manufacturing_Production_Motor_and_Vehicle_Parts.csv: 637 observations, 2 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/msspbvw50gdgd13489tvcll00000gn/T/ipykernel_54192/2458607752.py:139: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  df = df.asfreq('M', method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Consumer_Confidence.csv: 768 observations, 2 features\n",
      "Final monthly dataset: 926 observations, 14 features\n",
      "Processing quarterly data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/msspbvw50gdgd13489tvcll00000gn/T/ipykernel_54192/2458607752.py:142: FutureWarning: 'Q' is deprecated and will be removed in a future version, please use 'QE' instead.\n",
      "  df = df.asfreq('Q', method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed GDP_quaterly.csv: 311 observations, 2 features\n",
      "Final quarterly dataset: 311 observations, 2 features\n",
      "Processed data: daily=(25381, 18), weekly=(3038, 4), monthly=(926, 14), quarterly=(311, 2)\n",
      "Data overview saved to ./output/data_overview.png\n",
      "\n",
      "3. Calculating Technical Indicators...\n",
      "Applied indicators to Oil_Investing_Fixed_Close_raw: 45 new features\n",
      "Applied indicators to Oil_Investing_Fixed_Close_pct_change: 45 new features\n",
      "Applied indicators to Gold_Investing_fixed_Close_raw: 45 new features\n",
      "Applied indicators to Gold_Investing_fixed_Close_pct_change: 45 new features\n",
      "Applied indicators to SPX_Close_raw: 45 new features\n",
      "Applied indicators to SPX_Close_pct_change: 45 new features\n",
      "Applied indicators to 10Y-03M_YieldCurve_daily_Close_raw: 45 new features\n",
      "Applied indicators to 10Y-03M_YieldCurve_daily_Close_diff: 45 new features\n",
      "Applied indicators to 10Y-02Y_YieldCurve_daily_Close_raw: 45 new features\n",
      "Applied indicators to 10Y-02Y_YieldCurve_daily_Close_diff: 45 new features\n",
      "Applied indicators to COPPER_Macrotrends_1959_Close_raw: 45 new features\n",
      "Applied indicators to COPPER_Macrotrends_1959_Close_pct_change: 45 new features\n",
      "Applied indicators to Lumber_daily_macrotrends_Close_raw: 45 new features\n",
      "Applied indicators to Lumber_daily_macrotrends_Close_pct_change: 45 new features\n",
      "Applied indicators to Copper_Gold_Ratio_raw: 45 new features\n",
      "Applied indicators to Copper_Gold_Ratio_pct_change: 45 new features\n",
      "Applied indicators to Lumber_Gold_Ratio_raw: 45 new features\n",
      "Applied indicators to Lumber_Gold_Ratio_pct_change: 45 new features\n",
      "Applied indicators to InitialUnemploymentClaims_weekly_Value_raw: 45 new features\n",
      "Applied indicators to InitialUnemploymentClaims_weekly_Value_pct_change: 45 new features\n",
      "Applied indicators to Fiancial_conditions_Value_raw: 45 new features\n",
      "Applied indicators to Fiancial_conditions_Value_diff: 45 new features\n",
      "Applied indicators to CPI_mon_monthly_Value_raw: 45 new features\n",
      "Applied indicators to CPI_mon_monthly_Value_pct_change: 45 new features\n",
      "Applied indicators to Unemployment_monthly_Value_raw: 45 new features\n",
      "Applied indicators to Unemployment_monthly_Value_diff: 45 new features\n",
      "Applied indicators to InterestRate_monthly_Value_raw: 45 new features\n",
      "Applied indicators to InterestRate_monthly_Value_diff: 45 new features\n",
      "Applied indicators to HousingStarts_monthly_Value_raw: 45 new features\n",
      "Applied indicators to HousingStarts_monthly_Value_pct_change: 45 new features\n",
      "Applied indicators to Heavy_Truck_Sales_Value_raw: 45 new features\n",
      "Applied indicators to Heavy_Truck_Sales_Value_pct_change: 45 new features\n",
      "Applied indicators to Manufacturing_Production_Motor_and_Vehicle_Parts_Value_raw: 45 new features\n",
      "Applied indicators to Manufacturing_Production_Motor_and_Vehicle_Parts_Value_pct_change: 45 new features\n",
      "Applied indicators to Consumer_Confidence_Value_raw: 45 new features\n",
      "Applied indicators to Consumer_Confidence_Value_diff: 45 new features\n",
      "Applied indicators to GDP_quaterly_Value_raw: 45 new features\n",
      "Applied indicators to GDP_quaterly_Value_pct_change: 45 new features\n",
      "Calculated technical indicators: daily=(25381, 810), weekly=(3038, 180), monthly=(926, 630), quarterly=(311, 90)\n",
      "\n",
      "4. Aligning Data for Hierarchical Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/msspbvw50gdgd13489tvcll00000gn/T/ipykernel_54192/2458607752.py:508: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  aligned_df = aligned_df.ffill().bfill()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned daily to weekly: (3038, 810)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/msspbvw50gdgd13489tvcll00000gn/T/ipykernel_54192/2458607752.py:508: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  aligned_df = aligned_df.ffill().bfill()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned weekly to monthly: (926, 180)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/msspbvw50gdgd13489tvcll00000gn/T/ipykernel_54192/2458607752.py:508: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  aligned_df = aligned_df.ffill().bfill()\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned monthly to quarterly: (311, 630)\n",
      "\n",
      "5. Creating Train-Test Split...\n",
      "Train-test split at 2009-03-31 00:00:00: train=248, test=63\n",
      "\n",
      "6. Building Models...\n",
      "Building Pure Hierarchical Model...\n",
      "Fitting daily model with 810 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/msspbvw50gdgd13489tvcll00000gn/T/ipykernel_54192/2224935741.py:95: RuntimeWarning: invalid value encountered in divide\n",
      "  X_std = (X - np.nanmean(X, axis=0)) / np.nanstd(X, axis=0)\n",
      "/var/folders/bl/msspbvw50gdgd13489tvcll00000gn/T/ipykernel_54192/2224935741.py:274: RuntimeWarning: invalid value encountered in matmul\n",
      "  alpha_curr = alpha_pred + K_t @ v_t\n",
      "/var/folders/bl/msspbvw50gdgd13489tvcll00000gn/T/ipykernel_54192/2224935741.py:237: RuntimeWarning: invalid value encountered in matmul\n",
      "  alpha_pred = transition @ alpha_curr\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import norm\n",
    "\n",
    "def run_gdp_forecast_workflow(\n",
    "    data_folder,\n",
    "    output_folder='./output',\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    train_test_split=0.8,\n",
    "    use_midas=True,\n",
    "    use_pure_hierarchical=True,\n",
    "    daily_factors=5,\n",
    "    weekly_factors=3,\n",
    "    monthly_factors=3,\n",
    "    gdp_ar_lags=4,\n",
    "    random_state=42,\n",
    "    save_models=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the complete GDP forecasting workflow.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_folder: str\n",
    "        Path to the data folder\n",
    "    output_folder: str\n",
    "        Path to the output folder\n",
    "    start_date: str or None\n",
    "        Start date for analysis\n",
    "    end_date: str or None\n",
    "        End date for analysis\n",
    "    train_test_split: float\n",
    "        Proportion of data to use for training\n",
    "    use_midas: bool\n",
    "        Whether to use MIDAS for GDP prediction\n",
    "    use_pure_hierarchical: bool\n",
    "        Whether to create a pure hierarchical model version\n",
    "    daily_factors: int\n",
    "        Number of factors to extract from daily data\n",
    "    weekly_factors: int\n",
    "        Number of factors to extract from weekly data\n",
    "    monthly_factors: int\n",
    "        Number of factors to extract from monthly data\n",
    "    gdp_ar_lags: int\n",
    "        Number of autoregressive lags for GDP\n",
    "    random_state: int\n",
    "        Random seed for reproducibility\n",
    "    save_models: bool\n",
    "        Whether to save the models\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (evaluator, models, preprocessor)\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Set up logging\n",
    "    log_file = os.path.join(output_folder, 'workflow_log.txt')\n",
    "    \n",
    "    def log(message):\n",
    "        \"\"\"Log message to file and print to console.\"\"\"\n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(f\"{pd.Timestamp.now()}: {message}\\n\")\n",
    "        print(message)\n",
    "    \n",
    "    log(\"=\" * 80)\n",
    "    log(f\"Starting GDP Forecasting Workflow at {pd.Timestamp.now()}\")\n",
    "    log(\"=\" * 80)\n",
    "    \n",
    "    # 1. Configuration\n",
    "    log(\"\\n1. Setting up configuration...\")\n",
    "    \n",
    "    # Daily data configuration\n",
    "    daily_config = {\n",
    "        'daily': {\n",
    "            'files': {\n",
    "                'Oil_Investing_Fixed.csv': {\n",
    "                    'columns': ['Close'],\n",
    "                    'transformations': {'Close': ['raw', 'pct_change']},\n",
    "                    'start_date': '1983-01-01'\n",
    "                },\n",
    "                'Gold_Investing_fixed.csv': {\n",
    "                    'columns': ['Close'],\n",
    "                    'transformations': {'Close': ['raw', 'pct_change']},\n",
    "                    'start_date': '1975-01-01'\n",
    "                },\n",
    "                'SPX.csv': {\n",
    "                    'columns': ['Close'],\n",
    "                    'transformations': {'Close': ['raw', 'pct_change']},\n",
    "                    'start_date': '1927-01-01'\n",
    "                },\n",
    "                '10Y-03M_YieldCurve_daily.csv': {\n",
    "                    'columns': ['Close'],\n",
    "                    'transformations': {'Close': ['raw', 'diff']},\n",
    "                    'start_date': '1982-01-01'\n",
    "                },\n",
    "                '10Y-02Y_YieldCurve_daily.csv': {\n",
    "                    'columns': ['Close'],\n",
    "                    'transformations': {'Close': ['raw', 'diff']},\n",
    "                    'start_date': '1976-01-01'\n",
    "                },\n",
    "                'COPPER_Macrotrends_1959.csv': {\n",
    "                    'columns': ['Close'],\n",
    "                    'transformations': {'Close': ['raw', 'pct_change']},\n",
    "                    'start_date': '1959-01-01'\n",
    "                },\n",
    "                'Lumber_daily_macrotrends.csv': {\n",
    "                    'columns': ['Close'],\n",
    "                    'transformations': {'Close': ['raw', 'pct_change']},\n",
    "                    'start_date': '1972-01-01'\n",
    "                }\n",
    "            },\n",
    "            'ratios': {\n",
    "                'Copper_Gold_Ratio': {\n",
    "                    'numerator': 'COPPER_Macrotrends_1959',\n",
    "                    'denominator': 'Gold_Investing_fixed',\n",
    "                    'transformations': ['raw', 'pct_change']\n",
    "                },\n",
    "                'Lumber_Gold_Ratio': {\n",
    "                    'numerator': 'Lumber_daily_macrotrends',\n",
    "                    'denominator': 'Gold_Investing_fixed',\n",
    "                    'transformations': ['raw', 'pct_change']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Weekly data configuration\n",
    "    weekly_config = {\n",
    "        'weekly': {\n",
    "            'files': {\n",
    "                'InitialUnemploymentClaims_weekly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1967-01-01'\n",
    "                },\n",
    "                'Fiancial_conditions.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'diff']},\n",
    "                    'start_date': '1971-01-01'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Monthly data configuration\n",
    "    monthly_config = {\n",
    "        'monthly': {\n",
    "            'files': {\n",
    "                'CPI_mon_monthly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1955-01-01'\n",
    "                },\n",
    "                'Unemployment_monthly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'diff']},\n",
    "                    'start_date': '1948-01-01'\n",
    "                },\n",
    "                'InterestRate_monthly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'diff']},\n",
    "                    'start_date': '1954-01-01'\n",
    "                },\n",
    "                'HousingStarts_monthly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1959-01-01'\n",
    "                },\n",
    "                'Heavy_Truck_Sales.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1967-01-01'\n",
    "                },\n",
    "                'Manufacturing_Production_Motor_and_Vehicle_Parts.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1972-01-01'\n",
    "                },\n",
    "                'Consumer_Confidence.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'diff']},\n",
    "                    'start_date': '1960-01-01'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Quarterly data configuration\n",
    "    quarterly_config = {\n",
    "        'quarterly': {\n",
    "            'files': {\n",
    "                'GDP_quaterly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1947-01-01'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Combine all configurations\n",
    "    data_config = {}\n",
    "    data_config.update(daily_config)\n",
    "    data_config.update(weekly_config)\n",
    "    data_config.update(monthly_config)\n",
    "    data_config.update(quarterly_config)\n",
    "    \n",
    "    log(f\"Configuration set up with {len(daily_config['daily']['files'])} daily files, \" +\n",
    "        f\"{len(weekly_config['weekly']['files'])} weekly files, \" +\n",
    "        f\"{len(monthly_config['monthly']['files'])} monthly files, \" +\n",
    "        f\"{len(quarterly_config['quarterly']['files'])} quarterly files\")\n",
    "    \n",
    "    # 2. Data Preprocessing\n",
    "    log(\"\\n2. Data Preprocessing...\")\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = MultiFrequencyPreprocessor(data_folder)\n",
    "    preprocessor.set_config(data_config)\n",
    "    \n",
    "    # Set date range if provided\n",
    "    if start_date is not None:\n",
    "        preprocessor.set_date_range(start_date=start_date)\n",
    "    if end_date is not None:\n",
    "        preprocessor.set_date_range(end_date=end_date)\n",
    "    \n",
    "    # Process data for each frequency\n",
    "    daily_df = preprocessor.process_frequency_data('daily')\n",
    "    weekly_df = preprocessor.process_frequency_data('weekly')\n",
    "    monthly_df = preprocessor.process_frequency_data('monthly')\n",
    "    quarterly_df = preprocessor.process_frequency_data('quarterly')\n",
    "    \n",
    "    log(f\"Processed data: daily={daily_df.shape}, weekly={weekly_df.shape}, \" +\n",
    "        f\"monthly={monthly_df.shape}, quarterly={quarterly_df.shape}\")\n",
    "    \n",
    "    # Plot data overview\n",
    "    try:\n",
    "        fig = preprocessor.plot_data_overview()\n",
    "        fig.savefig(os.path.join(output_folder, 'data_overview.png'))\n",
    "        plt.close(fig)\n",
    "        log(f\"Data overview saved to {os.path.join(output_folder, 'data_overview.png')}\")\n",
    "    except Exception as e:\n",
    "        log(f\"Warning: Could not create data overview plot: {e}\")\n",
    "    \n",
    "    # 3. Technical Indicators\n",
    "    log(\"\\n3. Calculating Technical Indicators...\")\n",
    "    \n",
    "    # Initialize technical indicators calculator\n",
    "    tech_indicators = MultiFrequencyTechnicalIndicators()\n",
    "    \n",
    "    # Calculate technical indicators for each frequency\n",
    "    daily_indicators = tech_indicators.apply_indicators(daily_df, frequency='daily')\n",
    "    weekly_indicators = tech_indicators.apply_indicators(weekly_df, frequency='weekly')\n",
    "    monthly_indicators = tech_indicators.apply_indicators(monthly_df, frequency='monthly')\n",
    "    quarterly_indicators = tech_indicators.apply_indicators(quarterly_df, frequency='quarterly')\n",
    "    \n",
    "    log(f\"Calculated technical indicators: daily={daily_indicators.shape}, \" +\n",
    "        f\"weekly={weekly_indicators.shape}, monthly={monthly_indicators.shape}, \" +\n",
    "        f\"quarterly={quarterly_indicators.shape}\")\n",
    "    \n",
    "    # 4. Data Alignment for Hierarchical Model\n",
    "    log(\"\\n4. Aligning Data for Hierarchical Model...\")\n",
    "    \n",
    "    # Get GDP target series\n",
    "    gdp_target = quarterly_df['GDP_quaterly_Value_pct_change']\n",
    "    \n",
    "    # Align daily data to weekly dates\n",
    "    daily_to_weekly = preprocessor.align_to_dates(daily_indicators, weekly_indicators.index, method='last')\n",
    "    log(f\"Aligned daily to weekly: {daily_to_weekly.shape}\")\n",
    "    \n",
    "    # Align combined weekly data to monthly dates\n",
    "    weekly_to_monthly = preprocessor.align_to_dates(weekly_indicators, monthly_indicators.index, method='last')\n",
    "    log(f\"Aligned weekly to monthly: {weekly_to_monthly.shape}\")\n",
    "    \n",
    "    # Align combined monthly data to quarterly dates\n",
    "    monthly_to_quarterly = preprocessor.align_to_dates(monthly_indicators, gdp_target.index, method='last')\n",
    "    log(f\"Aligned monthly to quarterly: {monthly_to_quarterly.shape}\")\n",
    "    \n",
    "    # 5. Train-Test Split\n",
    "    log(\"\\n5. Creating Train-Test Split...\")\n",
    "    \n",
    "    # Determine split point\n",
    "    n_quarters = len(gdp_target)\n",
    "    n_train = int(n_quarters * train_test_split)\n",
    "    split_date = gdp_target.index[n_train]\n",
    "    \n",
    "    # Split GDP data\n",
    "    train_gdp = gdp_target.iloc[:n_train]\n",
    "    test_gdp = gdp_target.iloc[n_train:]\n",
    "    \n",
    "    # Split aligned data\n",
    "    train_monthly_aligned = monthly_to_quarterly.loc[train_gdp.index]\n",
    "    test_monthly_aligned = monthly_to_quarterly.loc[test_gdp.index]\n",
    "    \n",
    "    log(f\"Train-test split at {split_date}: train={len(train_gdp)}, test={len(test_gdp)}\")\n",
    "    \n",
    "    # 6. Model Building\n",
    "    log(\"\\n6. Building Models...\")\n",
    "    \n",
    "    # Initialize models dictionary\n",
    "    models = {}\n",
    "    \n",
    "    # 6.1. Pure Hierarchical Model\n",
    "    if use_pure_hierarchical:\n",
    "        log(\"Building Pure Hierarchical Model...\")\n",
    "        try:\n",
    "            # Initialize hierarchical predictor with standard settings\n",
    "            pure_model = HierarchicalGDPPredictor(\n",
    "                daily_factors=daily_factors,\n",
    "                weekly_factors=weekly_factors,\n",
    "                monthly_factors=monthly_factors,\n",
    "                gdp_ar_lags=gdp_ar_lags,\n",
    "                use_midas=False,  # Pure hierarchical uses direct factor weighting\n",
    "                random_state=random_state\n",
    "            )\n",
    "            \n",
    "            # Fit the model with all frequency data\n",
    "            pure_model.fit(\n",
    "                daily_df=daily_indicators,\n",
    "                weekly_df=weekly_indicators,\n",
    "                monthly_df=monthly_indicators,\n",
    "                gdp_series=train_gdp,\n",
    "                align_dates=True,\n",
    "                use_ar=True\n",
    "            )\n",
    "            \n",
    "            # Store in models dictionary\n",
    "            models['Pure_Hierarchical'] = pure_model\n",
    "            log(\"Pure Hierarchical Model successfully built\")\n",
    "            \n",
    "            # Save model if requested\n",
    "            if save_models:\n",
    "                model_path = os.path.join(output_folder, 'pure_hierarchical_model.pkl')\n",
    "                with open(model_path, 'wb') as f:\n",
    "                    pickle.dump(pure_model, f)\n",
    "                log(f\"Pure Hierarchical Model saved to {model_path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            log(f\"Error building Pure Hierarchical Model: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # 6.2. Hierarchical + MIDAS Model\n",
    "    if use_midas:\n",
    "        log(\"Building Hierarchical + MIDAS Model...\")\n",
    "        try:\n",
    "            # Initialize hierarchical predictor with MIDAS\n",
    "            hybrid_model = HierarchicalGDPPredictor(\n",
    "                daily_factors=daily_factors,\n",
    "                weekly_factors=weekly_factors,\n",
    "                monthly_factors=monthly_factors,\n",
    "                gdp_ar_lags=gdp_ar_lags,\n",
    "                use_midas=True,  # Use MIDAS for final GDP prediction\n",
    "                midas_max_lags=6,  # Use up to 6 quarters of monthly factors\n",
    "                random_state=random_state\n",
    "            )\n",
    "            \n",
    "            # Fit the model with all frequency data\n",
    "            hybrid_model.fit(\n",
    "                daily_df=daily_indicators,\n",
    "                weekly_df=weekly_indicators,\n",
    "                monthly_df=monthly_indicators,\n",
    "                gdp_series=train_gdp,\n",
    "                align_dates=True,\n",
    "                use_ar=True\n",
    "            )\n",
    "            \n",
    "            # Store in models dictionary\n",
    "            models['Hierarchical_MIDAS'] = hybrid_model\n",
    "            log(\"Hierarchical + MIDAS Model successfully built\")\n",
    "            \n",
    "            # Save model if requested\n",
    "            if save_models:\n",
    "                model_path = os.path.join(output_folder, 'hybrid_midas_model.pkl')\n",
    "                with open(model_path, 'wb') as f:\n",
    "                    pickle.dump(hybrid_model, f)\n",
    "                log(f\"Hierarchical + MIDAS Model saved to {model_path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            log(f\"Error building Hierarchical + MIDAS Model: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # 6.3. Baseline Models\n",
    "    log(\"Building Baseline Models...\")\n",
    "    \n",
    "    # 6.3.1. AR Model (autoregressive)\n",
    "    try:\n",
    "        # Create lag features\n",
    "        X_ar = pd.DataFrame(index=train_gdp.index)\n",
    "        for lag in range(1, gdp_ar_lags + 1):\n",
    "            X_ar[f'lag_{lag}'] = train_gdp.shift(lag)\n",
    "        \n",
    "        # Drop rows with NaN values\n",
    "        valid_rows = ~X_ar.isna().any(axis=1)\n",
    "        X_ar_valid = X_ar[valid_rows]\n",
    "        y_ar_valid = train_gdp[valid_rows]\n",
    "        \n",
    "        # Fit AR model\n",
    "        ar_model = Ridge(alpha=0.1, random_state=random_state)\n",
    "        ar_model.fit(X_ar_valid, y_ar_valid)\n",
    "        \n",
    "        # Store model\n",
    "        models['AR_Baseline'] = ar_model\n",
    "        log(\"AR Baseline Model successfully built\")\n",
    "        \n",
    "        # Save lag columns for prediction\n",
    "        models['AR_lag_columns'] = X_ar.columns.tolist()\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"Error building AR Baseline Model: {e}\")\n",
    "    \n",
    "    # 6.3.2. MA Model (moving average of previous quarters)\n",
    "    try:\n",
    "        # Create different MA versions\n",
    "        ma_windows = [4]  # 1-year moving average\n",
    "        \n",
    "        for window in ma_windows:\n",
    "            ma_model = {'window': window}\n",
    "            models[f'MA_{window}_Baseline'] = ma_model\n",
    "            log(f\"MA-{window} Baseline Model defined\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log(f\"Error defining MA Baseline Models: {e}\")\n",
    "    \n",
    "    # 7. Model Evaluation\n",
    "    log(\"\\n7. Evaluating Models...\")\n",
    "    \n",
    "    # Create evaluator\n",
    "    evaluator = GDPForecastEvaluator()\n",
    "    \n",
    "    # Set actual values\n",
    "    evaluator.add_model('Actual', test_gdp, test_gdp)\n",
    "    \n",
    "    # Generate predictions for each model\n",
    "    for model_name, model in models.items():\n",
    "        try:\n",
    "            if model_name == 'Pure_Hierarchical':\n",
    "                # Generate predictions using the hierarchical model\n",
    "                predictions = model.predict(\n",
    "                    monthly_df=monthly_indicators,\n",
    "                    gdp_history=gdp_target,\n",
    "                    predict_date=None  # Use all data\n",
    "                )\n",
    "                \n",
    "                # Filter to test period\n",
    "                test_predictions = predictions.loc[test_gdp.index]\n",
    "                evaluator.add_model(model_name, test_predictions)\n",
    "                log(f\"Generated predictions for {model_name}: {len(test_predictions)} quarters\")\n",
    "                \n",
    "            elif model_name == 'Hierarchical_MIDAS':\n",
    "                # Generate predictions using the hybrid model\n",
    "                predictions = model.predict(\n",
    "                    monthly_df=monthly_indicators,\n",
    "                    gdp_history=gdp_target,\n",
    "                    predict_date=None  # Use all data\n",
    "                )\n",
    "                \n",
    "                # Filter to test period\n",
    "                test_predictions = predictions.loc[test_gdp.index]\n",
    "                evaluator.add_model(model_name, test_predictions)\n",
    "                log(f\"Generated predictions for {model_name}: {len(test_predictions)} quarters\")\n",
    "                \n",
    "            elif model_name == 'AR_Baseline':\n",
    "                # Create features for test period\n",
    "                X_ar_test = pd.DataFrame(index=test_gdp.index)\n",
    "                for lag, col in enumerate(models['AR_lag_columns'], 1):\n",
    "                    X_ar_test[col] = gdp_target.shift(lag).loc[test_gdp.index]\n",
    "                \n",
    "                # Make predictions\n",
    "                ar_predictions = pd.Series(\n",
    "                    model.predict(X_ar_test),\n",
    "                    index=test_gdp.index,\n",
    "                    name=model_name\n",
    "                )\n",
    "                \n",
    "                evaluator.add_model(model_name, ar_predictions)\n",
    "                log(f\"Generated predictions for {model_name}: {len(ar_predictions)} quarters\")\n",
    "                \n",
    "            elif 'MA_' in model_name:\n",
    "                # Get window size from model\n",
    "                window = model['window']\n",
    "                \n",
    "                # Calculate moving average for each test point\n",
    "                ma_predictions = pd.Series(index=test_gdp.index)\n",
    "                \n",
    "                for i, date in enumerate(test_gdp.index):\n",
    "                    # Get previous window periods\n",
    "                    hist_data = gdp_target[gdp_target.index < date]\n",
    "                    if len(hist_data) >= window:\n",
    "                        ma_predictions[date] = hist_data[-window:].mean()\n",
    "                    else:\n",
    "                        # Use all available data if less than window\n",
    "                        ma_predictions[date] = hist_data.mean() if len(hist_data) > 0 else np.nan\n",
    "                \n",
    "                # Fill any missing values\n",
    "                ma_predictions = ma_predictions.fillna(method='ffill').fillna(0)\n",
    "                \n",
    "                evaluator.add_model(model_name, ma_predictions)\n",
    "                log(f\"Generated predictions for {model_name}: {len(ma_predictions)} quarters\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            log(f\"Error generating predictions for {model_name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    log(\"Calculating evaluation metrics...\")\n",
    "    metrics = evaluator.calculate_metrics(rolling_window=8)\n",
    "    \n",
    "    # Output key metrics\n",
    "    log(\"\\nKey Performance Metrics:\")\n",
    "    log(\"-\" * 80)\n",
    "    log(f\"{'Model':<25} {'RMSE':>10} {'MAE':>10} {'Dir Acc':>10}\")\n",
    "    log(\"-\" * 80)\n",
    "    \n",
    "    for model_name, model_metrics in metrics.items():\n",
    "        if model_name != 'Actual':\n",
    "            log(f\"{model_name:<25} {model_metrics['rmse']:>10.4f} {model_metrics['mae']:>10.4f} {model_metrics['direction_accuracy']:>10.4f}\")\n",
    "    \n",
    "    # Create plots\n",
    "    log(\"\\nGenerating evaluation plots...\")\n",
    "    \n",
    "    try:\n",
    "        # Forecasts plot\n",
    "        fig = evaluator.plot_forecasts()\n",
    "        fig.savefig(os.path.join(output_folder, 'gdp_forecasts.png'))\n",
    "        plt.close(fig)\n",
    "        log(f\"Forecasts plot saved to {os.path.join(output_folder, 'gdp_forecasts.png')}\")\n",
    "        \n",
    "        # Error distribution plot\n",
    "        fig = evaluator.plot_error_distribution()\n",
    "        fig.savefig(os.path.join(output_folder, 'error_distribution.png'))\n",
    "        plt.close(fig)\n",
    "        log(f\"Error distribution plot saved to {os.path.join(output_folder, 'error_distribution.png')}\")\n",
    "        \n",
    "        # Rolling metrics plot\n",
    "        fig = evaluator.plot_rolling_metrics()\n",
    "        fig.savefig(os.path.join(output_folder, 'rolling_metrics.png'))\n",
    "        plt.close(fig)\n",
    "        log(f\"Rolling metrics plot saved to {os.path.join(output_folder, 'rolling_metrics.png')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"Error generating evaluation plots: {e}\")\n",
    "    \n",
    "    # 8. Generate comprehensive report\n",
    "    log(\"\\n8. Generating Final Report...\")\n",
    "    \n",
    "    try:\n",
    "        report_path = os.path.join(output_folder, 'gdp_forecast_evaluation.md')\n",
    "        report_content = evaluator.generate_report(report_path, include_plots=True)\n",
    "        log(f\"Comprehensive evaluation report saved to {report_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"Error generating evaluation report: {e}\")\n",
    "    \n",
    "    # 9. Conclusion\n",
    "    log(\"\\n9. Workflow Completed\")\n",
    "    log(\"=\" * 80)\n",
    "    log(f\"GDP Forecasting Workflow completed at {pd.Timestamp.now()}\")\n",
    "    log(\"=\" * 80)\n",
    "    \n",
    "    return evaluator, models, preprocessor\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set parameters\n",
    "    DATA_FOLDER = \"./Project_Data\"\n",
    "    OUTPUT_FOLDER = \"./output\"\n",
    "    \n",
    "    # Run the workflow\n",
    "    evaluator, models, preprocessor = run_gdp_forecast_workflow(\n",
    "        data_folder=DATA_FOLDER,\n",
    "        output_folder=OUTPUT_FOLDER,\n",
    "        start_date='1980-01-01',  # Start date for analysis\n",
    "        end_date=None,           # End date (use None for all available data)\n",
    "        train_test_split=0.8,    # Use 80% of data for training\n",
    "        use_midas=True,          # Use MIDAS for final GDP prediction\n",
    "        use_pure_hierarchical=True,  # Also build pure hierarchical model\n",
    "        daily_factors=5,         # Number of daily factors\n",
    "        weekly_factors=3,        # Number of weekly factors\n",
    "        monthly_factors=3,       # Number of monthly factors\n",
    "        gdp_ar_lags=4,           # Number of AR lags for GDP\n",
    "        random_state=42,         # For reproducibility\n",
    "        save_models=True         # Save models to files\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55364f18-141e-4e89-bb0c-b319cb9c9179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
