{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf6b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Advanced GDP Forecasting System\n",
    "\n",
    "This module implements state-of-the-art methods for GDP forecasting:\n",
    "1. Neural MIDAS with GRU for mixed-frequency modeling\n",
    "2. Intelligent feature selection for high-dimensional economic data\n",
    "3. Quantile Regression Forests for uncertainty quantification\n",
    "4. Enhanced forecast evaluation with economic significance metrics\n",
    "\n",
    "References to scientific literature are included throughout the implementation.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import warnings\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Silence warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Utility Functions\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "def trace_nans(name, df, threshold=0):\n",
    "    \"\"\"\n",
    "    Comprehensive NaN tracing function for pandas DataFrames.\n",
    "    \"\"\"\n",
    "    if isinstance(df, pd.Series):\n",
    "        nan_count = df.isna().sum()\n",
    "        total = len(df)\n",
    "        if nan_count > 0:\n",
    "            print(f\"WARNING: {name} Series contains {nan_count}/{total} NaNs ({nan_count/total:.2%})\")\n",
    "        return\n",
    "        \n",
    "    nan_count = df.isna().sum().sum()\n",
    "    if nan_count > 0:\n",
    "        rows, cols = df.shape\n",
    "        total_cells = rows * cols\n",
    "        \n",
    "        print(f\"WARNING: {name} contains {nan_count}/{total_cells} NaNs ({nan_count/total_cells:.2%})\")\n",
    "        \n",
    "        cols_with_nans = df.columns[df.isna().sum() > threshold]\n",
    "        if len(cols_with_nans) > 0:\n",
    "            print(f\"  Columns with > {threshold} NaNs:\")\n",
    "            for col in cols_with_nans:\n",
    "                col_nans = df[col].isna().sum()\n",
    "                print(f\"    {col}: {col_nans}/{rows} NaNs ({col_nans/rows:.2%})\")\n",
    "        \n",
    "        row_nan_counts = df.isna().sum(axis=1)\n",
    "        rows_with_many_nans = row_nan_counts[row_nan_counts > cols//4].sort_values(ascending=False)\n",
    "        if len(rows_with_many_nans) > 0:\n",
    "            print(f\"  Rows with significant NaNs:\")\n",
    "            for idx, count in rows_with_many_nans.head(5).items():\n",
    "                print(f\"    Row at {idx}: {count}/{cols} NaNs ({count/cols:.2%})\")\n",
    "        \n",
    "        first_rows_nan_pct = df.head(rows//10).isna().sum().sum() / (rows//10 * cols)\n",
    "        last_rows_nan_pct = df.tail(rows//10).isna().sum().sum() / (rows//10 * cols)\n",
    "        if first_rows_nan_pct > 0.1:\n",
    "            print(f\"  First 10% of rows have {first_rows_nan_pct:.2%} NaNs - possible lag/window effect\")\n",
    "        if last_rows_nan_pct > 0.1:\n",
    "            print(f\"  Last 10% of rows have {last_rows_nan_pct:.2%} NaNs - possible trailing window effect\")\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Feature Selection Module\n",
    "#-----------------------------------------------------------------------------\n",
    "class AdvancedFeatureSelector:\n",
    "    \"\"\"\n",
    "    Intelligent feature selection for economic time series data.\n",
    "    \n",
    "    Based on research by Bai & Ng (2020), who demonstrated that targeted feature \n",
    "    selection can dramatically improve macroeconomic forecasting with large datasets.\n",
    "    \n",
    "    References:\n",
    "    -----------\n",
    "    Bai, J., & Ng, S. (2020). Acute vs. Chronic Impulses in High-Dimensional Dynamic \n",
    "    Factor Models. Journal of Econometrics, 214(1), 101-120.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method='boruta', max_features=50, n_estimators=100, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the feature selector.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        method : str\n",
    "            Feature selection method ('boruta', 'rf_importance', 'mutual_info')\n",
    "        max_features : int\n",
    "            Maximum number of features to select\n",
    "        n_estimators : int\n",
    "            Number of estimators for ensemble methods\n",
    "        random_state : int\n",
    "            Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.max_features = max_features\n",
    "        self.n_estimators = n_estimators\n",
    "        self.random_state = random_state\n",
    "        self.selected_features = None\n",
    "        self.feature_importance = None\n",
    "    \n",
    "    def selective_feature_engineering(self, X_monthly, X_quarterly, target_column=None):\n",
    "        \"\"\"\n",
    "        Intelligent feature selection across mixed-frequency data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_monthly : DataFrame\n",
    "            Monthly features\n",
    "        X_quarterly : DataFrame\n",
    "            Quarterly features (including target if target_column is None)\n",
    "        target_column : str, optional\n",
    "            Name of target column in X_quarterly\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Selected features for monthly and quarterly data\n",
    "        \"\"\"\n",
    "        from scipy import stats\n",
    "        \n",
    "        # Extract target variable\n",
    "        if target_column is None:\n",
    "            # Assume first column is target\n",
    "            y = X_quarterly.iloc[:, 0]\n",
    "            X_q = X_quarterly.iloc[:, 1:]\n",
    "        else:\n",
    "            y = X_quarterly[target_column]\n",
    "            X_q = X_quarterly.drop(columns=[target_column])\n",
    "        \n",
    "        print(f\"Processing {len(X_monthly.columns)} monthly features and {len(X_q.columns)} quarterly features\")\n",
    "        \n",
    "        # Extract quarterly features from monthly data\n",
    "        quarterly_features = pd.DataFrame(index=y.index)\n",
    "        \n",
    "        for col in X_monthly.columns:\n",
    "            # For each monthly feature, create 5 quarterly aggregations\n",
    "            for q_date in y.index:\n",
    "                # Get monthly data for the quarter (last 3 months)\n",
    "                quarter_start = pd.Timestamp(q_date) - pd.DateOffset(months=3)\n",
    "                month_data = X_monthly[col][(X_monthly.index > quarter_start) & \n",
    "                                         (X_monthly.index <= q_date)]\n",
    "                \n",
    "                if len(month_data) > 0:\n",
    "                    # Last value\n",
    "                    quarterly_features.loc[q_date, f\"{col}_last\"] = month_data.iloc[-1]\n",
    "                    \n",
    "                    # Mean value\n",
    "                    quarterly_features.loc[q_date, f\"{col}_mean\"] = month_data.mean()\n",
    "                    \n",
    "                    # Standard deviation (volatility)\n",
    "                    quarterly_features.loc[q_date, f\"{col}_std\"] = month_data.std() if len(month_data) > 1 else 0\n",
    "                    \n",
    "                    # Trend (slope of linear regression)\n",
    "                    if len(month_data) > 1:\n",
    "                        try:\n",
    "                            x = np.arange(len(month_data))\n",
    "                            slope = stats.linregress(x, month_data.values).slope\n",
    "                            quarterly_features.loc[q_date, f\"{col}_slope\"] = slope\n",
    "                        except:\n",
    "                            # Handle case where linear regression fails\n",
    "                            quarterly_features.loc[q_date, f\"{col}_slope\"] = 0\n",
    "                    else:\n",
    "                        quarterly_features.loc[q_date, f\"{col}_slope\"] = 0\n",
    "                    \n",
    "                    # Acceleration (second difference)\n",
    "                    if len(month_data) > 2:\n",
    "                        try:\n",
    "                            diff2 = np.diff(month_data.values, 2)\n",
    "                            if len(diff2) > 0 and np.isfinite(diff2[-1]):\n",
    "                                quarterly_features.loc[q_date, f\"{col}_accel\"] = diff2[-1]\n",
    "                            else:\n",
    "                                quarterly_features.loc[q_date, f\"{col}_accel\"] = 0\n",
    "                        except:\n",
    "                            quarterly_features.loc[q_date, f\"{col}_accel\"] = 0\n",
    "                    else:\n",
    "                        quarterly_features.loc[q_date, f\"{col}_accel\"] = 0\n",
    "        \n",
    "        # Combine all features\n",
    "        combined_features = pd.concat([quarterly_features, X_q], axis=1)\n",
    "        \n",
    "        # Handle missing values\n",
    "        combined_features = combined_features.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "        \n",
    "        # Handle infinite values and extreme outliers\n",
    "        print(\"Cleaning data by handling infinities and outliers...\")\n",
    "        for col in combined_features.columns:\n",
    "            # Replace inf values with NaN and then fill\n",
    "            inf_mask = ~np.isfinite(combined_features[col])\n",
    "            if inf_mask.any():\n",
    "                print(f\"  Column {col} contains {inf_mask.sum()} infinity values - replacing\")\n",
    "                combined_features.loc[inf_mask, col] = np.nan\n",
    "                \n",
    "            # Handle extreme values using winsorization (capping)\n",
    "            if combined_features[col].count() > 0:  # Only process if we have values\n",
    "                q1 = combined_features[col].quantile(0.01)\n",
    "                q99 = combined_features[col].quantile(0.99)\n",
    "                iqr = q99 - q1\n",
    "                \n",
    "                # Set very extreme values to boundaries (prevent numeric overflow)\n",
    "                lower_bound = q1 - 3 * iqr\n",
    "                upper_bound = q99 + 3 * iqr\n",
    "                \n",
    "                # Count extreme values\n",
    "                extreme_mask = (combined_features[col] < lower_bound) | (combined_features[col] > upper_bound)\n",
    "                if extreme_mask.any():\n",
    "                    print(f\"  Column {col} contains {extreme_mask.sum()} extreme values - winsorizing\")\n",
    "                    combined_features.loc[combined_features[col] < lower_bound, col] = lower_bound\n",
    "                    combined_features.loc[combined_features[col] > upper_bound, col] = upper_bound\n",
    "        \n",
    "        # Fill any remaining NaN values\n",
    "        combined_features = combined_features.fillna(0)\n",
    "        \n",
    "        # Double-check for any remaining infinities or NaNs\n",
    "        if not np.isfinite(combined_features.values).all():\n",
    "            print(\"Warning: Some infinite values remain after cleaning\")\n",
    "            # Force replace any remaining problematic values\n",
    "            combined_features = combined_features.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "        \n",
    "        # Standardize features with robust scaler to handle outliers better\n",
    "        try:\n",
    "            # Try using RobustScaler which is less affected by outliers\n",
    "            from sklearn.preprocessing import RobustScaler\n",
    "            scaler = RobustScaler()\n",
    "            X_scaled = scaler.fit_transform(combined_features)\n",
    "        except Exception as e:\n",
    "            print(f\"RobustScaler failed: {e}\")\n",
    "            # Fallback to manual standardization\n",
    "            print(\"Falling back to manual standardization...\")\n",
    "            X_scaled = np.zeros_like(combined_features.values)\n",
    "            for i, col in enumerate(combined_features.columns):\n",
    "                col_data = combined_features[col].values\n",
    "                col_median = np.median(col_data)\n",
    "                col_mad = np.median(np.abs(col_data - col_median)) + 1e-10  # avoid division by zero\n",
    "                X_scaled[:, i] = (col_data - col_median) / col_mad\n",
    "        \n",
    "        X_scaled_df = pd.DataFrame(X_scaled, index=combined_features.index, columns=combined_features.columns)\n",
    "        \n",
    "        # Apply feature selection\n",
    "        print(f\"Applying {self.method} feature selection method...\")\n",
    "        \n",
    "        if self.method == 'boruta':\n",
    "            try:\n",
    "                # Boruta feature selection (wrapper method)\n",
    "                from boruta import BorutaPy\n",
    "                \n",
    "                # Base estimator\n",
    "                rf = RandomForestRegressor(\n",
    "                    n_estimators=self.n_estimators,\n",
    "                    max_depth=7,\n",
    "                    random_state=self.random_state,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                \n",
    "                # Initialize Boruta\n",
    "                boruta_selector = BorutaPy(\n",
    "                    rf, \n",
    "                    n_estimators='auto', \n",
    "                    verbose=2, \n",
    "                    random_state=self.random_state,\n",
    "                    max_iter=100\n",
    "                )\n",
    "                \n",
    "                # Fit\n",
    "                boruta_selector.fit(X_scaled, y.values)\n",
    "                \n",
    "                # Get results\n",
    "                self.feature_importance = pd.Series(\n",
    "                    boruta_selector.ranking_,\n",
    "                    index=combined_features.columns\n",
    "                ).sort_values()\n",
    "                \n",
    "                # Get selected features\n",
    "                selected_mask = boruta_selector.support_\n",
    "                \n",
    "                if sum(selected_mask) > self.max_features:\n",
    "                    # Too many features selected, use ranking to narrow down\n",
    "                    top_indices = np.argsort(boruta_selector.ranking_)[:self.max_features]\n",
    "                    selected_mask = np.zeros_like(selected_mask, dtype=bool)\n",
    "                    selected_mask[top_indices] = True\n",
    "                \n",
    "                self.selected_features = combined_features.columns[selected_mask].tolist()\n",
    "                \n",
    "            except ImportError:\n",
    "                print(\"Boruta not available, falling back to random forest importance\")\n",
    "                self.method = 'rf_importance'\n",
    "        \n",
    "        if self.method == 'rf_importance':\n",
    "            # Random forest feature importance\n",
    "            rf = RandomForestRegressor(\n",
    "                n_estimators=self.n_estimators,\n",
    "                max_depth=7,\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            # Fit\n",
    "            rf.fit(X_scaled, y)\n",
    "            \n",
    "            # Get feature importance\n",
    "            self.feature_importance = pd.Series(\n",
    "                rf.feature_importances_,\n",
    "                index=combined_features.columns\n",
    "            ).sort_values(ascending=False)\n",
    "            \n",
    "            # Select top features\n",
    "            self.selected_features = self.feature_importance.index[:self.max_features].tolist()\n",
    "        \n",
    "        elif self.method == 'mutual_info':\n",
    "            # Mutual information regression\n",
    "            mi_scores = mutual_info_regression(X_scaled, y, random_state=self.random_state)\n",
    "            \n",
    "            # Create feature importance\n",
    "            self.feature_importance = pd.Series(\n",
    "                mi_scores,\n",
    "                index=combined_features.columns\n",
    "            ).sort_values(ascending=False)\n",
    "            \n",
    "            # Select top features\n",
    "            self.selected_features = self.feature_importance.index[:self.max_features].tolist()\n",
    "        \n",
    "        # Split selected features into monthly and quarterly groups\n",
    "        monthly_features_prefix = [col.split('_')[0] for col in quarterly_features.columns]\n",
    "        \n",
    "        monthly_selected = [col for col in self.selected_features \n",
    "                           if any(col.startswith(prefix) for prefix in monthly_features_prefix)]\n",
    "        \n",
    "        quarterly_selected = [col for col in self.selected_features \n",
    "                             if col in X_q.columns]\n",
    "        \n",
    "        print(f\"Selected {len(monthly_selected)} monthly-derived features and {len(quarterly_selected)} quarterly features\")\n",
    "        \n",
    "        return {\n",
    "            'monthly': monthly_selected,\n",
    "            'quarterly': quarterly_selected,\n",
    "            'all': self.selected_features,\n",
    "            'importance': self.feature_importance\n",
    "        }\n",
    "    \n",
    "    def transform(self, X_monthly, X_quarterly, quarterly_dates=None, align_dates=True):\n",
    "        \"\"\"\n",
    "        Transform data using selected features.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_monthly : DataFrame\n",
    "            Monthly features\n",
    "        X_quarterly : DataFrame\n",
    "            Quarterly features\n",
    "        quarterly_dates : DatetimeIndex, optional\n",
    "            Quarterly dates to use for alignment\n",
    "        align_dates : bool\n",
    "            Whether to align monthly data to quarterly dates\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame\n",
    "            Transformed data with selected features\n",
    "        \"\"\"\n",
    "        from scipy import stats\n",
    "        \n",
    "        if self.selected_features is None:\n",
    "            raise ValueError(\"No features selected. Call selective_feature_engineering first.\")\n",
    "        \n",
    "        # Extract quarterly features from monthly data (if applicable)\n",
    "        if align_dates:\n",
    "            if quarterly_dates is None:\n",
    "                quarterly_dates = X_quarterly.index\n",
    "            \n",
    "            quarterly_features = pd.DataFrame(index=quarterly_dates)\n",
    "            \n",
    "            # Get list of base monthly columns that were included\n",
    "            monthly_cols = set()\n",
    "            for feature in self.selected_features:\n",
    "                parts = feature.split('_')\n",
    "                if len(parts) > 1 and f\"{parts[0]}_last\" in self.selected_features:\n",
    "                    monthly_cols.add(parts[0])\n",
    "            \n",
    "            # Process only needed monthly columns\n",
    "            for col in monthly_cols:\n",
    "                # For each needed monthly feature, create quarterly aggregations\n",
    "                for q_date in quarterly_dates:\n",
    "                    # Get monthly data for the quarter (last 3 months)\n",
    "                    quarter_start = pd.Timestamp(q_date) - pd.DateOffset(months=3)\n",
    "                    month_data = X_monthly[col][(X_monthly.index > quarter_start) & \n",
    "                                             (X_monthly.index <= q_date)]\n",
    "                    \n",
    "                    if len(month_data) > 0:\n",
    "                        # Create only needed aggregations\n",
    "                        if f\"{col}_last\" in self.selected_features:\n",
    "                            quarterly_features.loc[q_date, f\"{col}_last\"] = month_data.iloc[-1]\n",
    "                        \n",
    "                        if f\"{col}_mean\" in self.selected_features:\n",
    "                            quarterly_features.loc[q_date, f\"{col}_mean\"] = month_data.mean()\n",
    "                        \n",
    "                        if f\"{col}_std\" in self.selected_features:\n",
    "                            quarterly_features.loc[q_date, f\"{col}_std\"] = month_data.std() if len(month_data) > 1 else 0\n",
    "                        \n",
    "                        if f\"{col}_slope\" in self.selected_features:\n",
    "                            if len(month_data) > 1:\n",
    "                                x = np.arange(len(month_data))\n",
    "                                slope = stats.linregress(x, month_data.values).slope\n",
    "                                quarterly_features.loc[q_date, f\"{col}_slope\"] = slope\n",
    "                            else:\n",
    "                                quarterly_features.loc[q_date, f\"{col}_slope\"] = 0\n",
    "                        \n",
    "                        if f\"{col}_accel\" in self.selected_features:\n",
    "                            if len(month_data) > 2:\n",
    "                                diff2 = np.diff(month_data.values, 2)\n",
    "                                quarterly_features.loc[q_date, f\"{col}_accel\"] = diff2[-1]\n",
    "                            else:\n",
    "                                quarterly_features.loc[q_date, f\"{col}_accel\"] = 0\n",
    "            \n",
    "            # Combine with quarterly features\n",
    "            X_q_selected = X_quarterly[\n",
    "                [col for col in self.selected_features if col in X_quarterly.columns]\n",
    "            ]\n",
    "            \n",
    "            transformed_data = pd.concat([quarterly_features, X_q_selected], axis=1)\n",
    "            \n",
    "            # Handle missing values\n",
    "            transformed_data = transformed_data.fillna(method='ffill').fillna(method='bfill')\n",
    "            \n",
    "        else:\n",
    "            # Just select columns from the provided data\n",
    "            transformed_data = X_quarterly[\n",
    "                [col for col in self.selected_features if col in X_quarterly.columns]\n",
    "            ]\n",
    "        \n",
    "        return transformed_data\n",
    "    \n",
    "    def plot_feature_importance(self, top_n=20, figsize=(10, 12)):\n",
    "        \"\"\"\n",
    "        Plot feature importance.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        top_n : int\n",
    "            Number of top features to show\n",
    "        figsize : tuple\n",
    "            Figure size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            Feature importance plot\n",
    "        \"\"\"\n",
    "        if self.feature_importance is None:\n",
    "            raise ValueError(\"No feature importance available. Call selective_feature_engineering first.\")\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Plot top N features\n",
    "        top_features = self.feature_importance.sort_values(ascending=True).tail(top_n)\n",
    "        ax = top_features.plot.barh()\n",
    "        \n",
    "        ax.set_title(f'Top {top_n} Features by Importance')\n",
    "        ax.set_xlabel('Importance')\n",
    "        ax.set_ylabel('Feature')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return plt.gcf()\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Neural MIDAS Implementation\n",
    "#-----------------------------------------------------------------------------\n",
    "class MIDASGRU:\n",
    "    \"\"\"\n",
    "    Neural MIDAS with GRU for mixed-frequency time series forecasting.\n",
    "    \n",
    "    Based on research by Babii et al. (2022) and Goulet Coulombe (2020), who demonstrated\n",
    "    that neural networks with recurrent architectures can capture complex nonlinear\n",
    "    patterns in mixed-frequency macroeconomic data.\n",
    "    \n",
    "    References:\n",
    "    -----------\n",
    "    Babii, A., Ghysels, E., & Striaukas, J. (2022). Machine Learning Time Series \n",
    "    Regressions with an Application to Nowcasting. Journal of Business & Economic \n",
    "    Statistics, 40(3), 1094-1106.\n",
    "    \n",
    "    Goulet Coulombe, P. (2020). The Macroeconomy as a Random Forest. \n",
    "    Working Paper, arXiv:2006.12724.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, high_freq_dim=None, low_freq_dim=None, hidden_size=32, max_lags=12, \n",
    "                 dropout_rate=0.2, learning_rate=0.001, batch_size=32, epochs=200,\n",
    "                 random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the Neural MIDAS model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        high_freq_dim : int\n",
    "            Dimension of high-frequency data\n",
    "        low_freq_dim : int\n",
    "            Dimension of low-frequency/autoregressive data\n",
    "        hidden_size : int\n",
    "            Size of hidden layers\n",
    "        max_lags : int\n",
    "            Maximum number of lags for high-frequency data\n",
    "        dropout_rate : float\n",
    "            Dropout rate for regularization\n",
    "        learning_rate : float\n",
    "            Learning rate for optimization\n",
    "        batch_size : int\n",
    "            Batch size for training\n",
    "        epochs : int\n",
    "            Maximum number of training epochs\n",
    "        random_state : int\n",
    "            Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.high_freq_dim = high_freq_dim\n",
    "        self.low_freq_dim = low_freq_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_lags = max_lags\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.random_state = random_state\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        \n",
    "        # Set random seed\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        # Try importing TensorFlow\n",
    "        try:\n",
    "            import tensorflow as tf\n",
    "            tf.random.set_seed(random_state)\n",
    "            self.tf = tf\n",
    "        except ImportError:\n",
    "            print(\"TensorFlow not available. Neural MIDAS model cannot be used.\")\n",
    "            self.tf = None\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"Build the Neural MIDAS model architecture.\"\"\"\n",
    "        if self.tf is None:\n",
    "            raise ImportError(\"TensorFlow is required for Neural MIDAS.\")\n",
    "        \n",
    "        # High-frequency input (time steps × features)\n",
    "        high_freq_input = self.tf.keras.Input(shape=(self.max_lags, self.high_freq_dim))\n",
    "        \n",
    "        # Process high-frequency data with GRU\n",
    "        gru_output = self.tf.keras.layers.GRU(\n",
    "            self.hidden_size, \n",
    "            return_sequences=False,\n",
    "            dropout=self.dropout_rate,\n",
    "            recurrent_dropout=0.0\n",
    "        )(high_freq_input)\n",
    "        \n",
    "        # Low-frequency/autoregressive input (if provided)\n",
    "        if self.low_freq_dim > 0:\n",
    "            low_freq_input = self.tf.keras.Input(shape=(self.low_freq_dim,))\n",
    "            \n",
    "            # Process low-frequency data with a dense layer\n",
    "            low_freq_processed = self.tf.keras.layers.Dense(self.hidden_size//2)(low_freq_input)\n",
    "            low_freq_processed = self.tf.keras.layers.BatchNormalization()(low_freq_processed)\n",
    "            low_freq_processed = self.tf.keras.layers.Activation('relu')(low_freq_processed)\n",
    "            \n",
    "            # Combine high and low frequency information\n",
    "            combined = self.tf.keras.layers.Concatenate()([gru_output, low_freq_processed])\n",
    "        else:\n",
    "            combined = gru_output\n",
    "            low_freq_input = None\n",
    "        \n",
    "        # Final prediction layers\n",
    "        x = self.tf.keras.layers.Dense(self.hidden_size, activation='relu')(combined)\n",
    "        x = self.tf.keras.layers.BatchNormalization()(x)\n",
    "        x = self.tf.keras.layers.Dropout(self.dropout_rate)(x)\n",
    "        \n",
    "        # Output layer for GDP prediction\n",
    "        output = self.tf.keras.layers.Dense(1)(x)\n",
    "        \n",
    "        # Create model with appropriate inputs\n",
    "        if self.low_freq_dim > 0:\n",
    "            model = self.tf.keras.Model(inputs=[high_freq_input, low_freq_input], outputs=output)\n",
    "        else:\n",
    "            model = self.tf.keras.Model(inputs=high_freq_input, outputs=output)\n",
    "        \n",
    "        # Compile model\n",
    "        optimizer = self.tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def prepare_midas_data(self, X_monthly, y, X_quarterly=None, align_dates=True):\n",
    "        \"\"\"\n",
    "        Prepare data for Neural MIDAS model, with proper alignment of frequencies.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_monthly : DataFrame\n",
    "            Monthly features\n",
    "        y : Series\n",
    "            Target variable (quarterly)\n",
    "        X_quarterly : DataFrame, optional\n",
    "            Quarterly features\n",
    "        align_dates : bool\n",
    "            Whether to align data based on dates\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            Prepared data for MIDAS model\n",
    "        \"\"\"\n",
    "        # Create lag structure for monthly data\n",
    "        quarterly_dates = y.index\n",
    "        monthly_features = []\n",
    "        \n",
    "        for date in quarterly_dates:\n",
    "            # Get data for each quarterly date\n",
    "            # We need the preceding months for each quarter\n",
    "            month_end = pd.Timestamp(date)\n",
    "            month_start = month_end - pd.DateOffset(months=self.max_lags)\n",
    "            \n",
    "            # Get monthly data in this window\n",
    "            window_data = X_monthly[(X_monthly.index > month_start) & \n",
    "                                   (X_monthly.index <= month_end)]\n",
    "            \n",
    "            # Ensure we have the right number of months\n",
    "            if len(window_data) < self.max_lags:\n",
    "                # Pad with zeros if needed\n",
    "                pad_size = self.max_lags - len(window_data)\n",
    "                pad_df = pd.DataFrame(0, \n",
    "                                     index=range(pad_size), \n",
    "                                     columns=window_data.columns)\n",
    "                window_data = pd.concat([pad_df, window_data.reset_index(drop=True)])\n",
    "            \n",
    "            # If we have too many months, take the most recent ones\n",
    "            elif len(window_data) > self.max_lags:\n",
    "                window_data = window_data.iloc[-self.max_lags:]\n",
    "            \n",
    "            # Add to the list\n",
    "            monthly_features.append(window_data.values)\n",
    "        \n",
    "        # Convert to numpy array [n_samples, n_lags, n_features]\n",
    "        X_hf = np.array(monthly_features)\n",
    "        \n",
    "        # Handle quarterly features if provided\n",
    "        if X_quarterly is not None:\n",
    "            # Align quarterly data with target\n",
    "            common_idx = y.index.intersection(X_quarterly.index)\n",
    "            X_lf = X_quarterly.loc[common_idx].values\n",
    "            y_aligned = y.loc[common_idx].values\n",
    "            \n",
    "            # Keep only matching samples for high-freq data\n",
    "            date_indices = [i for i, date in enumerate(quarterly_dates) if date in common_idx]\n",
    "            X_hf = X_hf[date_indices]\n",
    "        else:\n",
    "            X_lf = None\n",
    "            y_aligned = y.values\n",
    "        \n",
    "        # Update dimensions\n",
    "        self.high_freq_dim = X_hf.shape[2]\n",
    "        \n",
    "        if X_lf is not None:\n",
    "            self.low_freq_dim = X_lf.shape[1]\n",
    "        else:\n",
    "            self.low_freq_dim = 0\n",
    "        \n",
    "        # Return prepared data\n",
    "        if X_lf is not None:\n",
    "            return [X_hf, X_lf], y_aligned\n",
    "        else:\n",
    "            return X_hf, y_aligned\n",
    "    \n",
    "    def fit(self, X_monthly, y, X_quarterly=None, validation_split=0.2, verbose=1):\n",
    "        \"\"\"\n",
    "        Fit the Neural MIDAS model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_monthly : DataFrame\n",
    "            Monthly data\n",
    "        y : Series\n",
    "            Quarterly target variable\n",
    "        X_quarterly : DataFrame, optional\n",
    "            Quarterly data for AR component\n",
    "        validation_split : float\n",
    "            Proportion of data to use for validation\n",
    "        verbose : int\n",
    "            Verbosity level\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self\n",
    "            Fitted model instance\n",
    "        \"\"\"\n",
    "        if self.tf is None:\n",
    "            raise ImportError(\"TensorFlow is required for Neural MIDAS.\")\n",
    "        \n",
    "        # Prepare data\n",
    "        inputs, targets = self.prepare_midas_data(X_monthly, y, X_quarterly)\n",
    "        \n",
    "        # Build model\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "        # Add early stopping and learning rate reduction\n",
    "        callbacks = [\n",
    "            self.tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=20,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            self.tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=10,\n",
    "                min_lr=1e-6\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Fit model\n",
    "        history = self.model.fit(\n",
    "            inputs, targets,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=callbacks,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        self.history = history\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_monthly, X_quarterly=None, quarterly_dates=None):\n",
    "        \"\"\"\n",
    "        Generate predictions with the fitted model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_monthly : DataFrame\n",
    "            Monthly data\n",
    "        X_quarterly : DataFrame, optional\n",
    "            Quarterly data for AR component\n",
    "        quarterly_dates : DatetimeIndex, optional\n",
    "            Quarterly dates for prediction\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        Series\n",
    "            Predicted values\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        # Default to all dates if not specified\n",
    "        if quarterly_dates is None and X_quarterly is not None:\n",
    "            quarterly_dates = X_quarterly.index\n",
    "        elif quarterly_dates is None:\n",
    "            # Try to infer from monthly data\n",
    "            # We'll use the end of each quarter in the monthly data\n",
    "            all_months = pd.DatetimeIndex(X_monthly.index)\n",
    "            quarterly_dates = pd.DatetimeIndex([date for date in all_months \n",
    "                                              if date.month in [3, 6, 9, 12] and \n",
    "                                              date.day >= 28])\n",
    "        \n",
    "        # Create lag structure for monthly data\n",
    "        monthly_features = []\n",
    "        \n",
    "        for date in quarterly_dates:\n",
    "            # Get data for each quarterly date\n",
    "            month_end = pd.Timestamp(date)\n",
    "            month_start = month_end - pd.DateOffset(months=self.max_lags)\n",
    "            \n",
    "            # Get monthly data in this window\n",
    "            window_data = X_monthly[(X_monthly.index > month_start) & \n",
    "                                   (X_monthly.index <= month_end)]\n",
    "            \n",
    "            # Ensure we have the right number of months\n",
    "            if len(window_data) < self.max_lags:\n",
    "                # Pad with zeros if needed\n",
    "                pad_size = self.max_lags - len(window_data)\n",
    "                pad_df = pd.DataFrame(0, \n",
    "                                     index=range(pad_size), \n",
    "                                     columns=window_data.columns)\n",
    "                window_data = pd.concat([pad_df, window_data.reset_index(drop=True)])\n",
    "            \n",
    "            # If we have too many months, take the most recent ones\n",
    "            elif len(window_data) > self.max_lags:\n",
    "                window_data = window_data.iloc[-self.max_lags:]\n",
    "            \n",
    "            # Add to the list\n",
    "            monthly_features.append(window_data.values)\n",
    "        \n",
    "        # Convert to numpy array [n_samples, n_lags, n_features]\n",
    "        X_hf = np.array(monthly_features)\n",
    "        \n",
    "        # Handle quarterly features if provided\n",
    "        if X_quarterly is not None:\n",
    "            # Get matching quarterly data\n",
    "            common_idx = quarterly_dates.intersection(X_quarterly.index)\n",
    "            X_lf = X_quarterly.loc[common_idx].values\n",
    "            \n",
    "            # Keep only matching samples for high-freq data\n",
    "            date_indices = [i for i, date in enumerate(quarterly_dates) if date in common_idx]\n",
    "            X_hf = X_hf[date_indices]\n",
    "            \n",
    "            # Update quarterly dates\n",
    "            quarterly_dates = common_idx\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = self.model.predict([X_hf, X_lf])\n",
    "        else:\n",
    "            # Make predictions with only high-frequency data\n",
    "            y_pred = self.model.predict(X_hf)\n",
    "        \n",
    "        # Convert to Series\n",
    "        predictions = pd.Series(y_pred.flatten(), index=quarterly_dates, name='MIDAS_GRU')\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def plot_training_history(self, figsize=(10, 6)):\n",
    "        \"\"\"\n",
    "        Plot training history.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        figsize : tuple\n",
    "            Figure size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            Training history plot\n",
    "        \"\"\"\n",
    "        if self.history is None:\n",
    "            raise ValueError(\"Model has not been trained yet\")\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.history.history['loss'], label='Training Loss')\n",
    "        plt.plot(self.history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss (MSE)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot MAE\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.history.history['mae'], label='Training MAE')\n",
    "        plt.plot(self.history.history['val_mae'], label='Validation MAE')\n",
    "        plt.title('Model MAE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return plt.gcf()\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Quantile Regression Forests\n",
    "#-----------------------------------------------------------------------------\n",
    "class QuantileGDPForecaster:\n",
    "    \"\"\"\n",
    "    Quantile Regression Forests for GDP forecasting with uncertainty quantification.\n",
    "    \n",
    "    Based on research by Adrian et al. (2022) and Meinshausen (2006), who demonstrated\n",
    "    that quantile-based approaches can effectively capture the entire distribution of \n",
    "    potential economic outcomes, particularly during downturns.\n",
    "    \n",
    "    References:\n",
    "    -----------\n",
    "    Adrian, T., Boyarchenko, N., & Giannone, D. (2022). Multimodal Density Forecasts for \n",
    "    the U.S. Economy. Review of Economics and Statistics, 104(5), 926-942.\n",
    "    \n",
    "    Meinshausen, N. (2006). Quantile Regression Forests. Journal of Machine Learning \n",
    "    Research, 7, 983-999.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=500, max_features='sqrt', min_samples_leaf=5,\n",
    "                 quantiles=[0.1, 0.25, 0.5, 0.75, 0.9], random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the Quantile Regression Forest model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_estimators : int\n",
    "            Number of trees in the forest\n",
    "        max_features : str or int\n",
    "            Maximum number of features for splits\n",
    "        min_samples_leaf : int\n",
    "            Minimum samples in each leaf node\n",
    "        quantiles : list\n",
    "            Quantiles to compute\n",
    "        random_state : int\n",
    "            Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.quantiles = quantiles\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Initialize model\n",
    "        self.rf_model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_features=max_features,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            bootstrap=True,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Storage for training data\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Quantile Regression Forest model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Training features\n",
    "        y : array-like\n",
    "            Target values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self\n",
    "            Fitted model instance\n",
    "        \"\"\"\n",
    "        # Store training data\n",
    "        self.X_train = X.copy() if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
    "        self.y_train = y.copy() if isinstance(y, pd.Series) else pd.Series(y)\n",
    "        \n",
    "        # Fit random forest\n",
    "        self.rf_model.fit(X, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _get_leaves_for_sample(self, X_sample):\n",
    "        \"\"\"Helper method to get leaf indices for a sample.\"\"\"\n",
    "        leaves = []\n",
    "        \n",
    "        # Convert to numpy array with explicit dtype\n",
    "        if isinstance(X_sample, pd.Series):\n",
    "            X_sample = X_sample.values\n",
    "        \n",
    "        # Ensure correct dtype - sklearn's trees can be picky about dtype\n",
    "        X_sample = np.asarray(X_sample, dtype=np.float32)\n",
    "        \n",
    "        for tree in self.rf_model.estimators_:\n",
    "            try:\n",
    "                # Get the leaf node index for each tree\n",
    "                leaf_id = tree.tree_.apply(X_sample.reshape(1, -1))[0]\n",
    "                leaves.append(leaf_id)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error getting leaf node - {e}\")\n",
    "                # Use a fallback - just use a random leaf\n",
    "                # This isn't ideal but prevents the process from breaking\n",
    "                leaf_id = np.random.randint(0, tree.tree_.node_count)\n",
    "                leaves.append(leaf_id)\n",
    "            \n",
    "        return leaves\n",
    "    \n",
    "    def _get_weights(self, X_sample):\n",
    "        \"\"\"\n",
    "        Get weights for each training sample based on leaf co-occurrence.\n",
    "        \n",
    "        This is the core of the quantile regression forest algorithm from\n",
    "        Meinshausen (2006).\n",
    "        \"\"\"\n",
    "        # Get leaf indices for the test sample\n",
    "        try:\n",
    "            sample_leaves = self._get_leaves_for_sample(X_sample)\n",
    "            \n",
    "            # Initialize weights\n",
    "            n_trees = len(self.rf_model.estimators_)\n",
    "            n_train_samples = len(self.y_train)\n",
    "            weights = np.zeros(n_train_samples)\n",
    "            \n",
    "            # For each tree, find training samples in the same leaf\n",
    "            for t, tree in enumerate(self.rf_model.estimators_):\n",
    "                # Get all leaf assignments for training data\n",
    "                # Convert training data to float32 for consistency\n",
    "                X_train_float32 = self.X_train.values.astype(np.float32)\n",
    "                train_leaves = tree.tree_.apply(X_train_float32)\n",
    "                \n",
    "                # Find samples in the same leaf\n",
    "                same_leaf = (train_leaves == sample_leaves[t])\n",
    "                \n",
    "                # Increment weights\n",
    "                weights[same_leaf] += 1.0 / n_trees\n",
    "            \n",
    "            return weights\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error calculating weights - {e}\")\n",
    "            # Return uniform weights as fallback\n",
    "            return np.ones(len(self.y_train)) / len(self.y_train)\n",
    "    \n",
    "    def predict_quantiles(self, X, return_all=False, compute_intervals=True):\n",
    "        \"\"\"\n",
    "        Generate quantile predictions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Features\n",
    "        return_all : bool\n",
    "            Whether to return all sample predictions\n",
    "        compute_intervals : bool\n",
    "            Whether to compute prediction intervals\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame\n",
    "            Predicted quantiles for each sample\n",
    "        \"\"\"\n",
    "        # Ensure X is DataFrame\n",
    "        X = X.copy() if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
    "        \n",
    "        # For storing results\n",
    "        results = {\n",
    "            'mean': np.zeros(len(X)),\n",
    "            'median': np.zeros(len(X))\n",
    "        }\n",
    "        \n",
    "        # Add quantile columns\n",
    "        for q in self.quantiles:\n",
    "            q_name = f\"q{int(q*100)}\"\n",
    "            results[q_name] = np.zeros(len(X))\n",
    "        \n",
    "        # If requested, store all predictions\n",
    "        if return_all:\n",
    "            all_predictions = []\n",
    "        \n",
    "        # Get predictions for each sample\n",
    "        for i in range(len(X)):\n",
    "            X_sample = X.iloc[i].values\n",
    "            \n",
    "            # Option 1: Use random forest predictions directly (faster but less accurate)\n",
    "            if len(X) > 100:  # Use faster method for larger datasets\n",
    "                # Get predictions from all trees\n",
    "                tree_preds = np.array([tree.predict(X_sample.reshape(1, -1))[0] \n",
    "                                     for tree in self.rf_model.estimators_])\n",
    "                \n",
    "                # Calculate quantiles and mean\n",
    "                results['mean'][i] = np.mean(tree_preds)\n",
    "                results['median'][i] = np.median(tree_preds)\n",
    "                \n",
    "                for q in self.quantiles:\n",
    "                    q_name = f\"q{int(q*100)}\"\n",
    "                    results[q_name][i] = np.quantile(tree_preds, q)\n",
    "                \n",
    "                if return_all:\n",
    "                    all_predictions.append(tree_preds)\n",
    "            \n",
    "            # Option 2: Use the proper quantile regression forest weighting (more accurate)\n",
    "            else:\n",
    "                # Get weights for training samples\n",
    "                weights = self._get_weights(X_sample)\n",
    "                \n",
    "                # Calculate weighted quantiles\n",
    "                results['mean'][i] = np.average(self.y_train, weights=weights)\n",
    "                results['median'][i] = weighted_quantile(self.y_train.values, 0.5, weights)\n",
    "                \n",
    "                for q in self.quantiles:\n",
    "                    q_name = f\"q{int(q*100)}\"\n",
    "                    results[q_name][i] = weighted_quantile(self.y_train.values, q, weights)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        result_df = pd.DataFrame(results, index=X.index if hasattr(X, 'index') else None)\n",
    "        \n",
    "        # Add prediction intervals if requested\n",
    "        if compute_intervals:\n",
    "            lower_q = min(self.quantiles)\n",
    "            upper_q = max(self.quantiles)\n",
    "            \n",
    "            result_df['prediction_interval'] = result_df[f\"q{int(upper_q*100)}\"] - result_df[f\"q{int(lower_q*100)}\"]\n",
    "            result_df['uncertainty_ratio'] = result_df['prediction_interval'] / result_df['median'].abs()\n",
    "        \n",
    "        # Add additional information if requested\n",
    "        if return_all:\n",
    "            return result_df, all_predictions\n",
    "        else:\n",
    "            return result_df\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Generate point predictions (median).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Features\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        Series\n",
    "            Predicted median values\n",
    "        \"\"\"\n",
    "        # Get quantile predictions\n",
    "        quantile_preds = self.predict_quantiles(X)\n",
    "        \n",
    "        # Return median predictions\n",
    "        return quantile_preds['median']\n",
    "    \n",
    "    def plot_forecast_distribution(self, X, y_true=None, figsize=(12, 6)):\n",
    "        \"\"\"\n",
    "        Plot the forecast distribution.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Features\n",
    "        y_true : array-like, optional\n",
    "            Actual values\n",
    "        figsize : tuple\n",
    "            Figure size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            Forecast distribution plot\n",
    "        \"\"\"\n",
    "        # Get quantile predictions\n",
    "        quantile_preds = self.predict_quantiles(X)\n",
    "        \n",
    "        # Create figure\n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Create x-axis (time or sample index)\n",
    "        x = np.arange(len(X)) if not hasattr(X, 'index') else X.index\n",
    "        \n",
    "        # Shade prediction intervals\n",
    "        plt.fill_between(x, \n",
    "                       quantile_preds[f\"q{int(min(self.quantiles)*100)}\"],\n",
    "                       quantile_preds[f\"q{int(max(self.quantiles)*100)}\"],\n",
    "                       alpha=0.3, label=f\"{int(min(self.quantiles)*100)}-{int(max(self.quantiles)*100)} Percentile\")\n",
    "        \n",
    "        # Shade narrower prediction intervals if we have more quantiles\n",
    "        if len(self.quantiles) > 2:\n",
    "            # Find the 25th and 75th percentile columns if they exist\n",
    "            q25_col = next((col for col in quantile_preds.columns if col == 'q25'), None)\n",
    "            q75_col = next((col for col in quantile_preds.columns if col == 'q75'), None)\n",
    "            \n",
    "            if q25_col and q75_col:\n",
    "                plt.fill_between(x, \n",
    "                               quantile_preds[q25_col],\n",
    "                               quantile_preds[q75_col],\n",
    "                               alpha=0.5, label=\"25-75 Percentile\")\n",
    "        \n",
    "        # Plot median\n",
    "        plt.plot(x, quantile_preds['median'], 'b-', linewidth=2, label='Median Forecast')\n",
    "        \n",
    "        # Plot actuals if provided\n",
    "        if y_true is not None:\n",
    "            plt.plot(x, y_true, 'k-', linewidth=2, label='Actual Values')\n",
    "        \n",
    "        # Add highlights for recessions if available\n",
    "        try:\n",
    "            from pandas_datareader.data import DataReader\n",
    "            \n",
    "            # Get recession data if X has date index\n",
    "            if hasattr(X, 'index') and isinstance(X.index, pd.DatetimeIndex):\n",
    "                start_date = X.index[0]\n",
    "                end_date = X.index[-1]\n",
    "                \n",
    "                try:\n",
    "                    # Get US recession data from FRED\n",
    "                    recession = DataReader('USREC', 'fred', start=start_date, end=end_date)\n",
    "                    \n",
    "                    # Create shaded regions for recessions\n",
    "                    last_date = None\n",
    "                    for date, value in recession.itertuples():\n",
    "                        if value == 1.0:  # Recession period\n",
    "                            if last_date is None:\n",
    "                                last_date = date\n",
    "                        elif last_date is not None:\n",
    "                            # End of recession period\n",
    "                            plt.axvspan(last_date, date, alpha=0.2, color='gray')\n",
    "                            last_date = None\n",
    "                    \n",
    "                    # Handle case where we're still in a recession at the end\n",
    "                    if last_date is not None:\n",
    "                        plt.axvspan(last_date, end_date, alpha=0.2, color='gray')\n",
    "                except:\n",
    "                    pass  # Silently ignore if recession data not available\n",
    "        except ImportError:\n",
    "            pass  # pandas_datareader not available\n",
    "        \n",
    "        # Add grid and legend\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(loc='best')\n",
    "        plt.title('GDP Forecast with Uncertainty Bands')\n",
    "        plt.xlabel('Date' if hasattr(X, 'index') else 'Sample')\n",
    "        plt.ylabel('GDP Growth (%)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return plt.gcf()\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Utility Functions for Quantile Regression\n",
    "#-----------------------------------------------------------------------------\n",
    "def weighted_quantile(values, quantile, weights=None):\n",
    "    \"\"\"\n",
    "    Compute the weighted quantile of a 1D array.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    values : array-like\n",
    "        Input array\n",
    "    quantile : float\n",
    "        Quantile to compute (0.0 to 1.0)\n",
    "    weights : array-like, optional\n",
    "        Weights for each value\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Weighted quantile\n",
    "    \"\"\"\n",
    "    values = np.array(values)\n",
    "    \n",
    "    if weights is None:\n",
    "        # Use standard numpy quantile\n",
    "        return np.quantile(values, quantile)\n",
    "    \n",
    "    # Sort values and weights\n",
    "    sorted_idx = np.argsort(values)\n",
    "    sorted_values = values[sorted_idx]\n",
    "    sorted_weights = weights[sorted_idx]\n",
    "    \n",
    "    # Calculate cumulative sum of weights\n",
    "    cumsum_weights = np.cumsum(sorted_weights)\n",
    "    \n",
    "    # Normalize weights\n",
    "    total_weight = cumsum_weights[-1]\n",
    "    normalized_cumsum = cumsum_weights / total_weight\n",
    "    \n",
    "    # Find index where normalized cumsum exceeds quantile\n",
    "    idx = np.searchsorted(normalized_cumsum, quantile)\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if idx == 0:\n",
    "        return sorted_values[0]\n",
    "    elif idx == len(values):\n",
    "        return sorted_values[-1]\n",
    "    else:\n",
    "        # Interpolate between values if necessary\n",
    "        prev_idx = idx - 1\n",
    "        prev_val = sorted_values[prev_idx]\n",
    "        prev_cumsum = normalized_cumsum[prev_idx]\n",
    "        \n",
    "        val = sorted_values[idx]\n",
    "        cumsum = normalized_cumsum[idx]\n",
    "        \n",
    "        # Linear interpolation\n",
    "        fraction = (quantile - prev_cumsum) / (cumsum - prev_cumsum) if cumsum > prev_cumsum else 0\n",
    "        return prev_val + fraction * (val - prev_val)\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Advanced GDP Forecast System - Main Class\n",
    "#-----------------------------------------------------------------------------\n",
    "class AdvancedGDPForecastSystem:\n",
    "    \"\"\"\n",
    "    Advanced GDP Forecasting System combining state-of-the-art methods.\n",
    "    \n",
    "    This system integrates:\n",
    "    1. Intelligent feature selection for economic time series\n",
    "    2. Neural MIDAS with GRU for mixed-frequency modeling\n",
    "    3. Quantile Regression Forests for uncertainty quantification\n",
    "    \n",
    "    Based on research by Adrian et al. (2022), Bai & Ng (2020), and Babii et al. (2022),\n",
    "    who demonstrated significant improvements in GDP forecasting accuracy and uncertainty\n",
    "    quantification using these approaches.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_features=50, midas_lags=12, n_trees=500, \n",
    "                 selection_method='boruta', quantiles=[0.1, 0.25, 0.5, 0.75, 0.9],\n",
    "                 random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the GDP forecasting system.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        max_features : int\n",
    "            Maximum number of features to select\n",
    "        midas_lags : int\n",
    "            Maximum lag periods for MIDAS\n",
    "        n_trees : int\n",
    "            Number of trees for Quantile Regression Forest\n",
    "        selection_method : str\n",
    "            Feature selection method ('boruta', 'rf_importance', 'mutual_info')\n",
    "        quantiles : list\n",
    "            Quantiles to estimate\n",
    "        random_state : int\n",
    "            Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.max_features = max_features\n",
    "        self.midas_lags = midas_lags\n",
    "        self.n_trees = n_trees\n",
    "        self.selection_method = selection_method\n",
    "        self.quantiles = quantiles\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Initialize components\n",
    "        self.feature_selector = AdvancedFeatureSelector(\n",
    "            method=selection_method,\n",
    "            max_features=max_features,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Try to import TensorFlow for MIDAS-GRU\n",
    "        try:\n",
    "            import tensorflow as tf\n",
    "            self.midas_model = MIDASGRU(\n",
    "                max_lags=midas_lags,\n",
    "                hidden_size=32,\n",
    "                epochs=200,\n",
    "                random_state=random_state\n",
    "            )\n",
    "            self.use_neural_midas = True\n",
    "        except ImportError:\n",
    "            print(\"TensorFlow not available. Will use RandomForest only.\")\n",
    "            self.midas_model = None\n",
    "            self.use_neural_midas = False\n",
    "        \n",
    "        # Initialize Quantile Regression Forest\n",
    "        self.qrf_model = QuantileGDPForecaster(\n",
    "            n_estimators=n_trees,\n",
    "            quantiles=quantiles,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Storage for preprocessed data\n",
    "        self.train_data = {}\n",
    "        self.test_data = {}\n",
    "        self.features_info = None\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, monthly_df, quarterly_df, target_column, \n",
    "            test_size=0.2, use_neural_midas=True, validation_split=0.2):\n",
    "        \"\"\"\n",
    "        Fit the complete GDP forecasting system.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        monthly_df : DataFrame\n",
    "            Monthly economic data\n",
    "        quarterly_df : DataFrame\n",
    "            Quarterly data including GDP\n",
    "        target_column : str\n",
    "            Name of the GDP target column\n",
    "        test_size : float\n",
    "            Proportion of data to hold out for testing\n",
    "        use_neural_midas : bool\n",
    "            Whether to use Neural MIDAS model\n",
    "        validation_split : float\n",
    "            Proportion of training data to use for validation\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self\n",
    "            Fitted model instance\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Advanced GDP Forecasting System - Training Phase\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # 1. Extract target variable\n",
    "        y_full = quarterly_df[target_column]\n",
    "        X_quarterly = quarterly_df.drop(columns=[target_column])\n",
    "        \n",
    "        print(f\"\\n1. Data Overview:\")\n",
    "        print(f\"   - Monthly features: {monthly_df.shape[1]} variables, {monthly_df.shape[0]} time periods\")\n",
    "        print(f\"   - Quarterly features: {X_quarterly.shape[1]} variables, {X_quarterly.shape[0]} time periods\")\n",
    "        print(f\"   - Target variable: {target_column} with {len(y_full)} observations\")\n",
    "        \n",
    "        # 2. Train-test split (time series)\n",
    "        n_test = int(len(y_full) * test_size)\n",
    "        n_train = len(y_full) - n_test\n",
    "        \n",
    "        train_idx = y_full.index[:n_train]\n",
    "        test_idx = y_full.index[n_train:]\n",
    "        \n",
    "        # Split data\n",
    "        y_train = y_full.loc[train_idx]\n",
    "        y_test = y_full.loc[test_idx]\n",
    "        \n",
    "        X_quarterly_train = X_quarterly.loc[train_idx]\n",
    "        X_quarterly_test = X_quarterly.loc[test_idx]\n",
    "        \n",
    "        # Split monthly data based on dates\n",
    "        last_train_date = train_idx[-1]\n",
    "        X_monthly_train = monthly_df[monthly_df.index <= last_train_date]\n",
    "        X_monthly_test = monthly_df[monthly_df.index > last_train_date]\n",
    "        \n",
    "        print(f\"\\n2. Train-Test Split:\")\n",
    "        print(f\"   - Training period: {train_idx[0]} to {train_idx[-1]} ({len(train_idx)} quarters)\")\n",
    "        print(f\"   - Testing period: {test_idx[0]} to {test_idx[-1]} ({len(test_idx)} quarters)\")\n",
    "        \n",
    "        # Store data\n",
    "        self.train_data = {\n",
    "            'monthly': X_monthly_train,\n",
    "            'quarterly': X_quarterly_train,\n",
    "            'target': y_train\n",
    "        }\n",
    "        \n",
    "        self.test_data = {\n",
    "            'monthly': X_monthly_test,\n",
    "            'quarterly': X_quarterly_test,\n",
    "            'target': y_test\n",
    "        }\n",
    "        \n",
    "        # 3. Feature selection\n",
    "        print(\"\\n3. Feature Selection:\")\n",
    "        self.features_info = self.feature_selector.selective_feature_engineering(\n",
    "            X_monthly_train, \n",
    "            pd.concat([y_train, X_quarterly_train], axis=1)\n",
    "        )\n",
    "        \n",
    "        # Apply transformation to get selected features\n",
    "        X_train_selected = self.feature_selector.transform(\n",
    "            X_monthly_train, \n",
    "            X_quarterly_train,\n",
    "            quarterly_dates=y_train.index\n",
    "        )\n",
    "        \n",
    "        print(f\"   - Selected {len(self.features_info['all'])} features in total\")\n",
    "        \n",
    "        # Store the selected training data\n",
    "        self.train_data['selected_features'] = X_train_selected\n",
    "        \n",
    "        # 4. Train models\n",
    "        print(\"\\n4. Model Training:\")\n",
    "        \n",
    "        # Train Quantile Regression Forest\n",
    "        print(\"   - Training Quantile Regression Forest...\")\n",
    "        self.qrf_model.fit(X_train_selected, y_train)\n",
    "        \n",
    "        # Train Neural MIDAS if available and requested\n",
    "        if self.midas_model is not None and use_neural_midas and self.use_neural_midas:\n",
    "            print(\"   - Training Neural MIDAS-GRU model...\")\n",
    "            self.midas_model.fit(\n",
    "                X_monthly_train, \n",
    "                y_train, \n",
    "                X_quarterly=X_train_selected,\n",
    "                validation_split=validation_split\n",
    "            )\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        print(\"\\nTraining completed successfully.\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, monthly_df=None, quarterly_df=None, return_quantiles=True):\n",
    "        \"\"\"\n",
    "        Generate GDP forecasts with the fitted system.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        monthly_df : DataFrame, optional\n",
    "            Monthly data for prediction (uses test data if None)\n",
    "        quarterly_df : DataFrame, optional\n",
    "            Quarterly data for prediction (uses test data if None)\n",
    "        return_quantiles : bool\n",
    "            Whether to return quantile predictions\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Forecast results\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        # Use test data if not provided\n",
    "        if monthly_df is None:\n",
    "            monthly_df = self.test_data['monthly']\n",
    "        \n",
    "        if quarterly_df is None:\n",
    "            quarterly_df = self.test_data['quarterly']\n",
    "        \n",
    "        # Get target dates\n",
    "        if hasattr(quarterly_df, 'index'):\n",
    "            target_dates = quarterly_df.index\n",
    "        else:\n",
    "            # Try to infer from monthly data\n",
    "            all_months = pd.DatetimeIndex(monthly_df.index)\n",
    "            target_dates = pd.DatetimeIndex([date for date in all_months \n",
    "                                          if date.month in [3, 6, 9, 12] and \n",
    "                                          date.day >= 28])\n",
    "        \n",
    "        # Transform data using selected features\n",
    "        X_selected = self.feature_selector.transform(\n",
    "            monthly_df,\n",
    "            quarterly_df,\n",
    "            quarterly_dates=target_dates\n",
    "        )\n",
    "        \n",
    "        # Generate forecasts\n",
    "        results = {}\n",
    "        \n",
    "        try:\n",
    "            # QRF prediction\n",
    "            if return_quantiles:\n",
    "                try:\n",
    "                    qrf_pred = self.qrf_model.predict_quantiles(X_selected)\n",
    "                    results['qrf'] = qrf_pred\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: QRF quantile prediction failed - {e}\")\n",
    "                    # Fall back to point prediction\n",
    "                    try:\n",
    "                        pred = self.qrf_model.predict(X_selected)\n",
    "                        results['qrf'] = pd.DataFrame({'median': pred}, index=X_selected.index)\n",
    "                    except Exception as e2:\n",
    "                        print(f\"Warning: QRF point prediction also failed - {e2}\")\n",
    "            else:\n",
    "                pred = self.qrf_model.predict(X_selected)\n",
    "                results['qrf'] = pd.DataFrame({'median': pred}, index=X_selected.index)\n",
    "            \n",
    "            # Neural MIDAS prediction if available\n",
    "            if self.midas_model is not None and self.use_neural_midas:\n",
    "                try:\n",
    "                    midas_pred = self.midas_model.predict(\n",
    "                        monthly_df, \n",
    "                        X_quarterly=X_selected,\n",
    "                        quarterly_dates=target_dates\n",
    "                    )\n",
    "                    \n",
    "                    # Ensure midas_pred is a DataFrame for consistency\n",
    "                    if isinstance(midas_pred, pd.Series):\n",
    "                        midas_pred = pd.DataFrame({'median': midas_pred})\n",
    "                    \n",
    "                    results['midas_gru'] = midas_pred\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: MIDAS-GRU prediction failed - {e}\")\n",
    "            \n",
    "            # Create ensemble prediction if both models are available\n",
    "            if 'midas_gru' in results and 'qrf' in results:\n",
    "                try:\n",
    "                    # Get qrf median\n",
    "                    if isinstance(results['qrf'], pd.DataFrame) and 'median' in results['qrf'].columns:\n",
    "                        qrf_median = results['qrf']['median']\n",
    "                    elif isinstance(results['qrf'], pd.Series):\n",
    "                        qrf_median = results['qrf']\n",
    "                    else:\n",
    "                        qrf_median = results['qrf'].iloc[:, 0]\n",
    "                    \n",
    "                    # Get midas median\n",
    "                    if isinstance(results['midas_gru'], pd.DataFrame) and 'median' in results['midas_gru'].columns:\n",
    "                        midas_median = results['midas_gru']['median']\n",
    "                    elif isinstance(results['midas_gru'], pd.Series):\n",
    "                        midas_median = results['midas_gru']\n",
    "                    else:\n",
    "                        midas_median = results['midas_gru'].iloc[:, 0]\n",
    "                    \n",
    "                    # Align indices\n",
    "                    common_idx = qrf_median.index.intersection(midas_median.index)\n",
    "                    if len(common_idx) > 0:\n",
    "                        # Simple average ensemble\n",
    "                        ensemble_pred = pd.DataFrame({\n",
    "                            'median': (qrf_median.loc[common_idx] + midas_median.loc[common_idx]) / 2\n",
    "                        }, index=common_idx)\n",
    "                        \n",
    "                        # If quantiles are available, add uncertainty from QRF\n",
    "                        if return_quantiles and isinstance(results['qrf'], pd.DataFrame):\n",
    "                            for col in results['qrf'].columns:\n",
    "                                if col not in ['mean', 'median'] and col.startswith('q'):\n",
    "                                    ensemble_pred[col] = results['qrf'][col].loc[common_idx]\n",
    "                        \n",
    "                        results['ensemble'] = ensemble_pred\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Ensemble prediction failed - {e}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in prediction process: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate(self, predictions=None, actuals=None, rolling_window=8):\n",
    "        \"\"\"\n",
    "        Evaluate forecast performance.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        predictions : dict, optional\n",
    "            Dictionary of predictions\n",
    "        actuals : Series, optional\n",
    "            Actual values\n",
    "        rolling_window : int\n",
    "            Window size for rolling metrics\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Evaluation metrics\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        # Get predictions if not provided\n",
    "        if predictions is None:\n",
    "            predictions = self.predict(return_quantiles=True)\n",
    "        \n",
    "        # Get actuals if not provided\n",
    "        if actuals is None:\n",
    "            actuals = self.test_data['target']\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        metrics = {}\n",
    "        \n",
    "        for model_name, preds in predictions.items():\n",
    "            # Get point prediction based on type of prediction object\n",
    "            if isinstance(preds, pd.DataFrame) and 'median' in preds.columns:\n",
    "                point_pred = preds['median']\n",
    "            elif isinstance(preds, pd.Series):\n",
    "                point_pred = preds\n",
    "            elif isinstance(preds, pd.DataFrame):\n",
    "                # Use the first column as fallback\n",
    "                point_pred = preds.iloc[:, 0]\n",
    "            else:\n",
    "                print(f\"Warning: Unsupported prediction type for {model_name}: {type(preds)}\")\n",
    "                continue\n",
    "            \n",
    "            # Align predictions with actuals\n",
    "            common_idx = actuals.index.intersection(point_pred.index)\n",
    "            if len(common_idx) == 0:\n",
    "                print(f\"Warning: No common dates between predictions and actuals for {model_name}\")\n",
    "                continue\n",
    "                \n",
    "            y_true = actuals.loc[common_idx]\n",
    "            y_pred = point_pred.loc[common_idx]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            model_metrics = {\n",
    "                'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "                'mae': np.mean(np.abs(y_true - y_pred)),\n",
    "                'r2': r2_score(y_true, y_pred),\n",
    "            }\n",
    "            \n",
    "            # Calculate directional accuracy - FIXED IMPLEMENTATION\n",
    "            # Drop the first observation since diff() creates NaN\n",
    "            y_true_diff = y_true.diff().dropna()\n",
    "            \n",
    "            # Get corresponding predictions for the same periods\n",
    "            matching_idx = y_true_diff.index.intersection(y_pred.index)\n",
    "            if len(matching_idx) > 0:\n",
    "                y_pred_for_diff = y_pred.loc[matching_idx]\n",
    "                \n",
    "                # Calculate directional prediction accuracy\n",
    "                actual_direction = (y_true_diff > 0).astype(int)\n",
    "                pred_direction = (y_pred_for_diff > y_true.shift(1).loc[matching_idx]).astype(int)\n",
    "                \n",
    "                dir_acc = np.mean(actual_direction == pred_direction)\n",
    "                model_metrics['direction_accuracy'] = dir_acc\n",
    "                \n",
    "                # Calculate separate accuracy for up and down movements\n",
    "                up_idx = actual_direction == 1\n",
    "                down_idx = actual_direction == 0\n",
    "                \n",
    "                if up_idx.any():\n",
    "                    up_acc = np.mean(pred_direction[up_idx] == actual_direction[up_idx])\n",
    "                    model_metrics['up_direction_accuracy'] = up_acc\n",
    "                \n",
    "                if down_idx.any():\n",
    "                    down_acc = np.mean(pred_direction[down_idx] == actual_direction[down_idx])\n",
    "                    model_metrics['down_direction_accuracy'] = down_acc\n",
    "                \n",
    "                # Count of correct predictions by direction\n",
    "                model_metrics['correct_up_predictions'] = np.sum((actual_direction == 1) & (pred_direction == 1))\n",
    "                model_metrics['correct_down_predictions'] = np.sum((actual_direction == 0) & (pred_direction == 0))\n",
    "                model_metrics['total_up_movements'] = np.sum(actual_direction == 1)\n",
    "                model_metrics['total_down_movements'] = np.sum(actual_direction == 0)\n",
    "            else:\n",
    "                model_metrics['direction_accuracy'] = np.nan\n",
    "            \n",
    "            # Calculate rolling metrics if enough data\n",
    "            if len(y_true) > rolling_window:\n",
    "                rolling_rmse = []\n",
    "                rolling_mae = []\n",
    "                rolling_dir_acc = []\n",
    "                \n",
    "                for i in range(len(y_true) - rolling_window + 1):\n",
    "                    window_true = y_true.iloc[i:i+rolling_window]\n",
    "                    window_pred = y_pred.iloc[i:i+rolling_window]\n",
    "                    \n",
    "                    # Calculate metrics for this window\n",
    "                    rmse = np.sqrt(mean_squared_error(window_true, window_pred))\n",
    "                    mae = np.mean(np.abs(window_true - window_pred))\n",
    "                    \n",
    "                    # Direction accuracy\n",
    "                    window_dir_true = np.sign(window_true.diff().fillna(0))\n",
    "                    window_dir_pred = np.sign(window_pred.diff().fillna(0))\n",
    "                    \n",
    "                    nonzero_mask = window_dir_true != 0\n",
    "                    if nonzero_mask.any():\n",
    "                        window_dir_acc = np.mean(window_dir_true[nonzero_mask] == window_dir_pred[nonzero_mask])\n",
    "                    else:\n",
    "                        window_dir_acc = np.nan\n",
    "                    \n",
    "                    rolling_rmse.append(rmse)\n",
    "                    rolling_mae.append(mae)\n",
    "                    rolling_dir_acc.append(window_dir_acc)\n",
    "                \n",
    "                # Add to metrics\n",
    "                model_metrics['rolling_rmse'] = pd.Series(\n",
    "                    rolling_rmse, \n",
    "                    index=y_true.index[rolling_window-1:len(rolling_rmse)+rolling_window-1]\n",
    "                )\n",
    "                \n",
    "                model_metrics['rolling_mae'] = pd.Series(\n",
    "                    rolling_mae, \n",
    "                    index=y_true.index[rolling_window-1:len(rolling_mae)+rolling_window-1]\n",
    "                )\n",
    "                \n",
    "                model_metrics['rolling_dir_acc'] = pd.Series(\n",
    "                    rolling_dir_acc,\n",
    "                    index=y_true.index[rolling_window-1:len(rolling_dir_acc)+rolling_window-1]\n",
    "                )\n",
    "            \n",
    "            # Calculate coverage metrics if quantiles are available\n",
    "            if isinstance(preds, pd.DataFrame) and any(col.startswith('q') for col in preds.columns):\n",
    "                # Get lower and upper quantiles\n",
    "                lower_q = min(self.quantiles)\n",
    "                upper_q = max(self.quantiles)\n",
    "                \n",
    "                lower_col = f\"q{int(lower_q*100)}\"\n",
    "                upper_col = f\"q{int(upper_q*100)}\"\n",
    "                \n",
    "                if lower_col in preds.columns and upper_col in preds.columns:\n",
    "                    lower_pred = preds[lower_col].loc[common_idx]\n",
    "                    upper_pred = preds[upper_col].loc[common_idx]\n",
    "                    \n",
    "                    # Calculate coverage\n",
    "                    coverage = np.mean((y_true >= lower_pred) & (y_true <= upper_pred))\n",
    "                    model_metrics['prediction_interval_coverage'] = coverage\n",
    "                    \n",
    "                    # Calculate interval width\n",
    "                    interval_width = np.mean(upper_pred - lower_pred)\n",
    "                    model_metrics['prediction_interval_width'] = interval_width\n",
    "                    \n",
    "                    # Calculate interval efficiency (coverage / width)\n",
    "                    if interval_width > 0:\n",
    "                        model_metrics['interval_efficiency'] = coverage / interval_width\n",
    "                    else:\n",
    "                        model_metrics['interval_efficiency'] = np.nan\n",
    "            \n",
    "            # Add to metrics dictionary\n",
    "            metrics[model_name] = model_metrics\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def plot_forecasts(self, predictions=None, actuals=None, figsize=(12, 8), include_quantiles=True):\n",
    "        \"\"\"\n",
    "        Plot forecast results.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        predictions : dict, optional\n",
    "            Dictionary of predictions\n",
    "        actuals : Series, optional\n",
    "            Actual values\n",
    "        figsize : tuple\n",
    "            Figure size\n",
    "        include_quantiles : bool\n",
    "            Whether to include quantile bands\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            Forecast plot\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        # Get predictions if not provided\n",
    "        if predictions is None:\n",
    "            predictions = self.predict(return_quantiles=include_quantiles)\n",
    "        \n",
    "        # Get actuals if not provided\n",
    "        if actuals is None:\n",
    "            actuals = self.test_data['target']\n",
    "        \n",
    "        # Create figure\n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Plot actual values\n",
    "        plt.plot(actuals.index, actuals, 'k-', linewidth=2, label='Actual GDP')\n",
    "        \n",
    "        # Plot predictions for each model\n",
    "        colors = plt.cm.tab10.colors\n",
    "        \n",
    "        for i, (model_name, preds) in enumerate(predictions.items()):\n",
    "            color = colors[i % len(colors)]\n",
    "            \n",
    "            # Get point prediction\n",
    "            if 'median' in preds.columns:\n",
    "                point_pred = preds['median']\n",
    "            elif isinstance(preds, pd.Series):\n",
    "                point_pred = preds\n",
    "            else:\n",
    "                # Use the first column as fallback\n",
    "                point_pred = preds.iloc[:, 0]\n",
    "            \n",
    "            # Plot point prediction\n",
    "            plt.plot(point_pred.index, point_pred, 'o-', color=color, linewidth=1.5, label=f'{model_name}')\n",
    "            \n",
    "            # Add quantile bands if available and requested\n",
    "            if include_quantiles and isinstance(preds, pd.DataFrame) and any(col.startswith('q') for col in preds.columns):\n",
    "                # Get quantiles\n",
    "                lower_q = min(self.quantiles)\n",
    "                upper_q = max(self.quantiles)\n",
    "                \n",
    "                lower_col = f\"q{int(lower_q*100)}\"\n",
    "                upper_col = f\"q{int(upper_q*100)}\"\n",
    "                \n",
    "                if lower_col in preds.columns and upper_col in preds.columns:\n",
    "                    plt.fill_between(\n",
    "                        point_pred.index,\n",
    "                        preds[lower_col],\n",
    "                        preds[upper_col],\n",
    "                        color=color,\n",
    "                        alpha=0.2,\n",
    "                        label=f'{model_name} {int(lower_q*100)}-{int(upper_q*100)} Percentile'\n",
    "                    )\n",
    "        \n",
    "        # Add recession shading if available\n",
    "        try:\n",
    "            from pandas_datareader.data import DataReader\n",
    "            from pandas_datareader._utils import RemoteDataError\n",
    "            \n",
    "            try:\n",
    "                # Get US recession data from FRED\n",
    "                all_dates = pd.DatetimeIndex(sorted(list(set(actuals.index) | \n",
    "                                               set(predictions[list(predictions.keys())[0]].index))))\n",
    "                                               \n",
    "                start_date = all_dates[0]\n",
    "                end_date = all_dates[-1]\n",
    "                \n",
    "                recession = DataReader('USREC', 'fred', start=start_date, end=end_date)\n",
    "                \n",
    "                # Create shaded regions for recessions\n",
    "                last_date = None\n",
    "                for date, value in recession.itertuples():\n",
    "                    if value == 1.0:  # Recession period\n",
    "                        if last_date is None:\n",
    "                            last_date = date\n",
    "                    elif last_date is not None:\n",
    "                        # End of recession period\n",
    "                        plt.axvspan(last_date, date, alpha=0.2, color='gray')\n",
    "                        last_date = None\n",
    "                \n",
    "                # Handle case where we're still in a recession at the end\n",
    "                if last_date is not None:\n",
    "                    plt.axvspan(last_date, all_dates[-1], alpha=0.2, color='gray')\n",
    "            \n",
    "            except RemoteDataError:\n",
    "                print(\"Could not retrieve recession data from FRED\")\n",
    "        \n",
    "        except ImportError:\n",
    "            print(\"pandas_datareader not available for recession shading\")\n",
    "        \n",
    "        # Add legend, grid, labels, etc.\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('GDP Growth (%)')\n",
    "        plt.title('GDP Growth: Actual vs Predicted')\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Format y-axis to show percentage\n",
    "        plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.1f}%'))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return plt.gcf()\n",
    "    \n",
    "    def plot_feature_importance(self, figsize=(10, 12)):\n",
    "        \"\"\"\n",
    "        Plot feature importance.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        figsize : tuple\n",
    "            Figure size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            Feature importance plot\n",
    "        \"\"\"\n",
    "        return self.feature_selector.plot_feature_importance(figsize=figsize)\n",
    "    \n",
    "    def economic_value_of_forecasts(self, predictions=None, actuals=None, risk_aversion=5):\n",
    "        \"\"\"\n",
    "        Calculate the economic value of GDP forecasts.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        predictions : dict, optional\n",
    "            Dictionary of predictions\n",
    "        actuals : Series, optional\n",
    "            Actual values\n",
    "        risk_aversion : float\n",
    "            Coefficient of risk aversion\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Economic performance metrics\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        # Get predictions if not provided\n",
    "        if predictions is None:\n",
    "            predictions = self.predict(return_quantiles=True)\n",
    "        \n",
    "        # Get actuals if not provided\n",
    "        if actuals is None:\n",
    "            actuals = self.test_data['target']\n",
    "        \n",
    "        # Initialize results\n",
    "        econ_value = {}\n",
    "        \n",
    "        # Calculate metrics for each model\n",
    "        for model_name, preds in predictions.items():\n",
    "            # Get point prediction\n",
    "            if 'median' in preds.columns:\n",
    "                point_pred = preds['median']\n",
    "            elif isinstance(preds, pd.Series):\n",
    "                point_pred = preds\n",
    "            else:\n",
    "                # Use the first column as fallback\n",
    "                point_pred = preds.iloc[:, 0]\n",
    "            \n",
    "            # Align predictions with actuals\n",
    "            common_idx = actuals.index.intersection(point_pred.index)\n",
    "            y_true = actuals.loc[common_idx]\n",
    "            y_pred = point_pred.loc[common_idx]\n",
    "            \n",
    "            # Calculate directional accuracy\n",
    "            actual_dir = np.sign(y_true.diff().fillna(0))\n",
    "            pred_dir = np.sign(y_pred.diff().fillna(0))\n",
    "            \n",
    "            # Ignore zero changes\n",
    "            nonzero_mask = actual_dir != 0\n",
    "            if nonzero_mask.any():\n",
    "                dir_acc = np.mean(actual_dir[nonzero_mask] == pred_dir[nonzero_mask])\n",
    "            else:\n",
    "                dir_acc = np.nan\n",
    "            \n",
    "            # Simulate investment strategy based on forecasts\n",
    "            returns = []\n",
    "            \n",
    "            for t in range(len(y_true) - 1):  # -1 because we need next period's actual\n",
    "                # Use forecast to decide allocation\n",
    "                forecast = y_pred.iloc[t]\n",
    "                \n",
    "                # Simple rule: if forecast > 0, invest proportionally to forecast\n",
    "                if forecast > 0:\n",
    "                    allocation = min(1.0, forecast / 2.0)  # Cap allocation at 100%\n",
    "                else:\n",
    "                    allocation = 0.0\n",
    "                    \n",
    "                # Calculate realized return\n",
    "                realized_growth = y_true.iloc[t+1]\n",
    "                period_return = allocation * realized_growth\n",
    "                returns.append(period_return)\n",
    "            \n",
    "            # Calculate performance metrics\n",
    "            returns = np.array(returns)\n",
    "            mean_return = np.mean(returns)\n",
    "            vol_return = np.std(returns)\n",
    "            \n",
    "            # Calculate Sharpe ratio if variance is positive\n",
    "            sharpe_ratio = mean_return / vol_return if vol_return > 0 else 0\n",
    "            \n",
    "            # Calculate utility\n",
    "            utility = mean_return - 0.5 * risk_aversion * vol_return**2\n",
    "            \n",
    "            # Store results\n",
    "            econ_value[model_name] = {\n",
    "                'mean_return': mean_return,\n",
    "                'volatility': vol_return,\n",
    "                'sharpe_ratio': sharpe_ratio,\n",
    "                'utility': utility,\n",
    "                'directional_accuracy': dir_acc\n",
    "            }\n",
    "        \n",
    "        return econ_value\n",
    "    \n",
    "    def generate_report(self, output_file=None, predictions=None, actuals=None):\n",
    "        \"\"\"\n",
    "        Generate comprehensive evaluation report.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_file : str, optional\n",
    "            Path to save report\n",
    "        predictions : dict, optional\n",
    "            Dictionary of predictions\n",
    "        actuals : Series, optional\n",
    "            Actual values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            Report content\n",
    "        \"\"\"\n",
    "        # Get predictions if not provided\n",
    "        if predictions is None:\n",
    "            predictions = self.predict(return_quantiles=True)\n",
    "        \n",
    "        # Get actuals if not provided\n",
    "        if actuals is None:\n",
    "            actuals = self.test_data['target']\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = self.evaluate(predictions, actuals)\n",
    "        econ_value = self.economic_value_of_forecasts(predictions, actuals)\n",
    "        \n",
    "        # Start building report\n",
    "        report = \"# Advanced GDP Forecasting System Evaluation Report\\n\\n\"\n",
    "        report += f\"Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\\n\\n\"\n",
    "        \n",
    "        # Add model summary\n",
    "        report += \"## Models Evaluated\\n\\n\"\n",
    "        report += f\"Number of models: {len(predictions)}\\n\"\n",
    "        report += f\"Evaluation period: {actuals.index[0]} to {actuals.index[-1]}\\n\"\n",
    "        report += f\"Number of observations: {len(actuals)}\\n\\n\"\n",
    "        \n",
    "        # Add performance metrics table\n",
    "        report += \"## Statistical Performance Metrics\\n\\n\"\n",
    "        report += \"| Model | RMSE | MAE | R² | Direction Accuracy |\\n\"\n",
    "        report += \"|-------|------|-----|----|-----------------|\\n\"\n",
    "        \n",
    "        for model_name, model_metrics in metrics.items():\n",
    "            report += (\n",
    "                f\"| {model_name} | \"\n",
    "                f\"{model_metrics['rmse']:.4f} | \"\n",
    "                f\"{model_metrics['mae']:.4f} | \"\n",
    "                f\"{model_metrics['r2']:.4f} | \"\n",
    "                f\"{model_metrics['direction_accuracy']:.4f} |\\n\"\n",
    "            )\n",
    "        \n",
    "        report += \"\\n\"\n",
    "        \n",
    "        # Add economic value table\n",
    "        report += \"## Economic Value Metrics\\n\\n\"\n",
    "        report += \"| Model | Mean Return | Volatility | Sharpe Ratio | Utility |\\n\"\n",
    "        report += \"|-------|-------------|------------|--------------|--------|\\n\"\n",
    "        \n",
    "        for model_name, model_metrics in econ_value.items():\n",
    "            report += (\n",
    "                f\"| {model_name} | \"\n",
    "                f\"{model_metrics['mean_return']:.4f} | \"\n",
    "                f\"{model_metrics['volatility']:.4f} | \"\n",
    "                f\"{model_metrics['sharpe_ratio']:.4f} | \"\n",
    "                f\"{model_metrics['utility']:.4f} |\\n\"\n",
    "            )\n",
    "        \n",
    "        report += \"\\n\"\n",
    "        \n",
    "        # Add prediction interval coverage if available\n",
    "        has_intervals = False\n",
    "        for model_metrics in metrics.values():\n",
    "            if 'prediction_interval_coverage' in model_metrics:\n",
    "                has_intervals = True\n",
    "                break\n",
    "        \n",
    "        if has_intervals:\n",
    "            report += \"## Uncertainty Quantification Metrics\\n\\n\"\n",
    "            report += \"| Model | PI Coverage | PI Width | Interval Efficiency |\\n\"\n",
    "            report += \"|-------|-------------|----------|---------------------|\\n\"\n",
    "            \n",
    "            for model_name, model_metrics in metrics.items():\n",
    "                if 'prediction_interval_coverage' in model_metrics:\n",
    "                    report += (\n",
    "                        f\"| {model_name} | \"\n",
    "                        f\"{model_metrics['prediction_interval_coverage']:.4f} | \"\n",
    "                        f\"{model_metrics['prediction_interval_width']:.4f} | \"\n",
    "                        f\"{model_metrics['interval_efficiency']:.4f} |\\n\"\n",
    "                    )\n",
    "            \n",
    "            report += \"\\n\"\n",
    "        \n",
    "        # Add feature importance section\n",
    "        if self.features_info is not None:\n",
    "            report += \"## Feature Importance\\n\\n\"\n",
    "            report += \"### Top 10 Features\\n\\n\"\n",
    "            \n",
    "            top_features = self.feature_selector.feature_importance.sort_values(ascending=False).head(10)\n",
    "            \n",
    "            for feature, importance in top_features.items():\n",
    "                report += f\"- **{feature}**: {importance:.4f}\\n\"\n",
    "            \n",
    "            report += \"\\n\"\n",
    "        \n",
    "        # Add conclusion based on metrics\n",
    "        report += \"## Conclusion\\n\\n\"\n",
    "        \n",
    "        # Find best model by RMSE\n",
    "        best_rmse_model = min(metrics.items(), key=lambda x: x[1]['rmse'])[0]\n",
    "        \n",
    "        # Find best model by direction accuracy\n",
    "        best_dir_model = max(metrics.items(), key=lambda x: x[1]['direction_accuracy'])[0]\n",
    "        \n",
    "        # Find best model by economic utility\n",
    "        best_econ_model = max(econ_value.items(), key=lambda x: x[1]['utility'])[0]\n",
    "        \n",
    "        report += f\"- Based on **RMSE**, the best performing model is **{best_rmse_model}** with RMSE of {metrics[best_rmse_model]['rmse']:.4f}.\\n\"\n",
    "        report += f\"- Based on **directional accuracy**, the best performing model is **{best_dir_model}** with accuracy of {metrics[best_dir_model]['direction_accuracy']:.2%}.\\n\"\n",
    "        report += f\"- Based on **economic utility**, the best performing model is **{best_econ_model}** with utility of {econ_value[best_econ_model]['utility']:.4f}.\\n\\n\"\n",
    "        \n",
    "        # Add summary recommendation\n",
    "        if best_rmse_model == best_dir_model and best_rmse_model == best_econ_model:\n",
    "            report += f\"The **{best_rmse_model}** model outperforms across all metrics and is recommended for GDP forecasting.\"\n",
    "        else:\n",
    "            report += \"Different models excel at different metrics.\\n\\n\"\n",
    "            \n",
    "            if has_intervals:\n",
    "                # Find model with best coverage\n",
    "                best_coverage_model = max(\n",
    "                    [(model, metrics[model]['prediction_interval_coverage']) \n",
    "                     for model in metrics \n",
    "                     if 'prediction_interval_coverage' in metrics[model]],\n",
    "                    key=lambda x: x[1]\n",
    "                )[0]\n",
    "                \n",
    "                report += f\"For point forecasts, **{best_rmse_model}** is recommended, while **{best_coverage_model}** provides the most reliable uncertainty estimates.\"\n",
    "            else:\n",
    "                report += f\"For overall performance, **{best_econ_model}** is recommended based on economic utility.\"\n",
    "        \n",
    "        # Save report to file if specified\n",
    "        if output_file:\n",
    "            os.makedirs(os.path.dirname(os.path.abspath(output_file)), exist_ok=True)\n",
    "            with open(output_file, 'w') as f:\n",
    "                f.write(report)\n",
    "            print(f\"Report saved to {output_file}\")\n",
    "        \n",
    "        return report\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Main Workflow Function\n",
    "#-----------------------------------------------------------------------------\n",
    "def run_advanced_gdp_forecast_workflow(\n",
    "    data_folder,\n",
    "    output_folder='./output',\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    test_size=0.2,\n",
    "    max_features=50,\n",
    "    midas_lags=12,\n",
    "    n_trees=500,\n",
    "    selection_method='rf_importance',\n",
    "    random_state=42,\n",
    "    save_models=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a complete GDP forecasting workflow with advanced methods.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_folder : str\n",
    "        Path to the data folder\n",
    "    output_folder : str\n",
    "        Path to the output folder\n",
    "    start_date : str, optional\n",
    "        Start date for analysis\n",
    "    end_date : str, optional\n",
    "        End date for analysis\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    max_features : int\n",
    "        Maximum number of features to select\n",
    "    midas_lags : int\n",
    "        Maximum lag periods for MIDAS\n",
    "    n_trees : int\n",
    "        Number of trees for Quantile Regression Forest\n",
    "    selection_method : str\n",
    "        Feature selection method ('boruta', 'rf_importance', 'mutual_info')\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    save_models : bool\n",
    "        Whether to save the models\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (forecast_system, predictions, metrics)\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Set up logging\n",
    "    log_file = os.path.join(output_folder, 'advanced_workflow_log.txt')\n",
    "    def log(message):\n",
    "        \"\"\"Log message to file and print to console.\"\"\"\n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(f\"{pd.Timestamp.now()}: {message}\\n\")\n",
    "        print(message)\n",
    "    \n",
    "    log(\"=\" * 80)\n",
    "    log(f\"Starting Advanced GDP Forecasting Workflow at {pd.Timestamp.now()}\")\n",
    "    log(\"=\" * 80)\n",
    "    \n",
    "    # 1. Configuration\n",
    "    log(\"\\n1. Setting up configuration...\")\n",
    "    \n",
    "    # Monthly data configuration\n",
    "    monthly_config = {\n",
    "        'monthly': {\n",
    "            'files': {\n",
    "                'CPI_mon_monthly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1955-01-01'\n",
    "                },\n",
    "                'Unemployment_monthly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'diff']},\n",
    "                    'start_date': '1948-01-01'\n",
    "                },\n",
    "                'InterestRate_monthly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'diff']},\n",
    "                    'start_date': '1954-01-01'\n",
    "                },\n",
    "                'HousingStarts_monthly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1959-01-01'\n",
    "                },\n",
    "                'Heavy_Truck_Sales.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1967-01-01'\n",
    "                },\n",
    "                'Manufacturing_Production_Motor_and_Vehicle_Parts.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1972-01-01'\n",
    "                },\n",
    "                'Consumer_Confidence.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'diff']},\n",
    "                    'start_date': '1960-01-01'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Quarterly data configuration\n",
    "    quarterly_config = {\n",
    "        'quarterly': {\n",
    "            'files': {\n",
    "                'GDP_quaterly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1947-01-01'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Combine configurations\n",
    "    data_config = {}\n",
    "    data_config.update(monthly_config)\n",
    "    data_config.update(quarterly_config)\n",
    "    \n",
    "    log(f\"Configuration set up with {len(monthly_config['monthly']['files'])} monthly files, \" +\n",
    "        f\"{len(quarterly_config['quarterly']['files'])} quarterly files\")\n",
    "    \n",
    "    # 2. Data Preprocessing\n",
    "    log(\"\\n2. Data Preprocessing...\")\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    #from MultiFrequencyPreprocessor import MultiFrequencyPreprocessor  # Import from original codebase\n",
    "    \n",
    "    preprocessor = MultiFrequencyPreprocessor(data_folder)\n",
    "    preprocessor.set_config(data_config)\n",
    "    \n",
    "    # Set date range if provided\n",
    "    if start_date is not None:\n",
    "        preprocessor.set_date_range(start_date=start_date)\n",
    "    if end_date is not None:\n",
    "        preprocessor.set_date_range(end_date=end_date)\n",
    "    \n",
    "    # Process data\n",
    "    monthly_df = preprocessor.process_frequency_data('monthly')\n",
    "    trace_nans(\"Raw monthly data\", monthly_df)\n",
    "    quarterly_df = preprocessor.process_frequency_data('quarterly')\n",
    "    trace_nans(\"Raw quarterly data\", quarterly_df)\n",
    "    \n",
    "    log(f\"Processed data: monthly={monthly_df.shape}, quarterly={quarterly_df.shape}\")\n",
    "    \n",
    "    # 3. Advanced GDP Forecasting System\n",
    "    log(\"\\n3. Initializing Advanced GDP Forecasting System...\")\n",
    "    \n",
    "    # Initialize the forecasting system\n",
    "    forecast_system = AdvancedGDPForecastSystem(\n",
    "        max_features=max_features,\n",
    "        midas_lags=midas_lags,\n",
    "        n_trees=n_trees,\n",
    "        selection_method=selection_method,\n",
    "        quantiles=[0.1, 0.25, 0.5, 0.75, 0.9],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Set GDP target column\n",
    "    gdp_target_column = 'GDP_quaterly_Value_pct_change'\n",
    "    \n",
    "    # Fit the system\n",
    "    forecast_system.fit(\n",
    "        monthly_df=monthly_df,\n",
    "        quarterly_df=quarterly_df,\n",
    "        target_column=gdp_target_column,\n",
    "        test_size=test_size\n",
    "    )\n",
    "    \n",
    "    # 4. Generate forecasts\n",
    "    log(\"\\n4. Generating GDP forecasts...\")\n",
    "    \n",
    "    predictions = forecast_system.predict(return_quantiles=True)\n",
    "    \n",
    "    # 5. Evaluate forecasts\n",
    "    log(\"\\n5. Evaluating forecast performance...\")\n",
    "    \n",
    "    metrics = forecast_system.evaluate(predictions)\n",
    "    econ_value = forecast_system.economic_value_of_forecasts(predictions)\n",
    "    \n",
    "    # Print key metrics\n",
    "    log(\"\\nKey Statistical Metrics:\")\n",
    "    log(\"-\" * 80)\n",
    "    log(f\"{'Model':<15} {'RMSE':>10} {'MAE':>10} {'Dir Acc':>10}\")\n",
    "    log(\"-\" * 80)\n",
    "    \n",
    "    for model_name, model_metrics in metrics.items():\n",
    "        log(f\"{model_name:<15} {model_metrics['rmse']:>10.4f} {model_metrics['mae']:>10.4f} {model_metrics['direction_accuracy']:>10.4f}\")\n",
    "    \n",
    "    log(\"\\nEconomic Value Metrics:\")\n",
    "    log(\"-\" * 80)\n",
    "    log(f\"{'Model':<15} {'Return':>10} {'Sharpe':>10} {'Utility':>10}\")\n",
    "    log(\"-\" * 80)\n",
    "    \n",
    "    for model_name, model_metrics in econ_value.items():\n",
    "        log(f\"{model_name:<15} {model_metrics['mean_return']:>10.4f} {model_metrics['sharpe_ratio']:>10.4f} {model_metrics['utility']:>10.4f}\")\n",
    "    \n",
    "    # 6. Generate plots\n",
    "    log(\"\\n6. Creating visualization plots...\")\n",
    "    \n",
    "    # Forecast plot\n",
    "    forecast_plot = forecast_system.plot_forecasts(predictions)\n",
    "    forecast_plot.savefig(os.path.join(output_folder, 'advanced_gdp_forecasts.png'))\n",
    "    plt.close(forecast_plot)\n",
    "    log(f\"Forecast plot saved to {os.path.join(output_folder, 'advanced_gdp_forecasts.png')}\")\n",
    "    \n",
    "    # Feature importance plot\n",
    "    importance_plot = forecast_system.plot_feature_importance()\n",
    "    importance_plot.savefig(os.path.join(output_folder, 'feature_importance.png'))\n",
    "    plt.close(importance_plot)\n",
    "    log(f\"Feature importance plot saved to {os.path.join(output_folder, 'feature_importance.png')}\")\n",
    "    \n",
    "    # 7. Generate report\n",
    "    log(\"\\n7. Generating comprehensive report...\")\n",
    "    \n",
    "    report_path = os.path.join(output_folder, 'advanced_gdp_forecast_report.md')\n",
    "    report = forecast_system.generate_report(report_path, predictions)\n",
    "    log(f\"Comprehensive report saved to {report_path}\")\n",
    "    \n",
    "    # 8. Save models if requested\n",
    "    #if save_models:\n",
    "    #    log(\"\\n8. Saving models...\")\n",
    "    #    \n",
    "    #    model_path = os.path.join(output_folder, 'advanced_forecast_system.pkl')\n",
    "    #    with open(model_path, 'wb') as f:\n",
    "    #        pickle.dump(forecast_system, f)\n",
    "    #    log(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    # 9. Conclusion\n",
    "    log(\"\\n9. Workflow completed\")\n",
    "    log(\"=\" * 80)\n",
    "    log(f\"Advanced GDP Forecasting Workflow completed at {pd.Timestamp.now()}\")\n",
    "    log(\"=\" * 80)\n",
    "    \n",
    "    return forecast_system, predictions, metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set parameters\n",
    "    DATA_FOLDER = \"./Project_Data\"\n",
    "    OUTPUT_FOLDER = \"./output/advanced\"\n",
    "    \n",
    "    # Run the workflow\n",
    "    forecast_system, predictions, metrics = run_advanced_gdp_forecast_workflow(\n",
    "        data_folder=DATA_FOLDER,\n",
    "        output_folder=OUTPUT_FOLDER,\n",
    "        start_date='1980-01-01',\n",
    "        end_date=None,\n",
    "        test_size=0.2,\n",
    "        max_features=50,\n",
    "        midas_lags=12,\n",
    "        n_trees=500,\n",
    "        selection_method='rf_importance',\n",
    "        random_state=42,\n",
    "        save_models=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
