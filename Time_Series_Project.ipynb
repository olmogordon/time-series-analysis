{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc68ece-d46f-45be-82d0-9f38bee308f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting Monthly-Only GDP Forecasting Workflow at 2025-05-09 14:09:33.062144\n",
      "================================================================================\n",
      "\n",
      "1. Setting up configuration...\n",
      "Configuration set up with 7 monthly files, 1 quarterly files\n",
      "\n",
      "2. Data Preprocessing...\n",
      "Found 18 files in ./Project_Data\n",
      "Processing monthly data...\n",
      "Processed CPI_mon_monthly.csv: 829 observations, 2 features\n",
      "Processed Unemployment_monthly.csv: 925 observations, 2 features\n",
      "Processed InterestRate_monthly.csv: 847 observations, 2 features\n",
      "Processed HousingStarts_monthly.csv: 792 observations, 2 features\n",
      "Processed Heavy_Truck_Sales.csv: 698 observations, 2 features\n",
      "Processed Manufacturing_Production_Motor_and_Vehicle_Parts.csv: 637 observations, 2 features\n",
      "Processed Consumer_Confidence.csv: 768 observations, 2 features\n",
      "Final monthly dataset: 926 observations, 14 features\n",
      "Processing quarterly data...\n",
      "Processed GDP_quaterly.csv: 311 observations, 2 features\n",
      "Final quarterly dataset: 311 observations, 2 features\n",
      "Processed data: monthly=(926, 14), quarterly=(311, 2)\n",
      "Data overview saved to ./output/data_overview.png\n",
      "\n",
      "3. Calculating Technical Indicators...\n",
      "Applied indicators to CPI_mon_monthly_Value_raw: 45 new features\n",
      "Applied indicators to CPI_mon_monthly_Value_pct_change: 45 new features\n",
      "Applied indicators to Unemployment_monthly_Value_raw: 45 new features\n",
      "Applied indicators to Unemployment_monthly_Value_diff: 45 new features\n",
      "Applied indicators to InterestRate_monthly_Value_raw: 45 new features\n",
      "Applied indicators to InterestRate_monthly_Value_diff: 45 new features\n",
      "Applied indicators to HousingStarts_monthly_Value_raw: 45 new features\n",
      "Applied indicators to HousingStarts_monthly_Value_pct_change: 45 new features\n",
      "Applied indicators to Heavy_Truck_Sales_Value_raw: 45 new features\n",
      "Applied indicators to Heavy_Truck_Sales_Value_pct_change: 45 new features\n",
      "Applied indicators to Manufacturing_Production_Motor_and_Vehicle_Parts_Value_raw: 45 new features\n",
      "Applied indicators to Manufacturing_Production_Motor_and_Vehicle_Parts_Value_pct_change: 45 new features\n",
      "Applied indicators to Consumer_Confidence_Value_raw: 45 new features\n",
      "Applied indicators to Consumer_Confidence_Value_diff: 45 new features\n",
      "Applied indicators to GDP_quaterly_Value_raw: 45 new features\n",
      "Applied indicators to GDP_quaterly_Value_pct_change: 45 new features\n",
      "Calculated technical indicators: monthly=(926, 630), quarterly=(311, 90)\n",
      "\n",
      "4. Aligning Data for Model...\n",
      "Aligned monthly to quarterly: (311, 630)\n",
      "\n",
      "5. Creating Train-Test Split...\n",
      "Train-test split at 2009-03-31 00:00:00: train=248, test=63\n",
      "\n",
      "6. Building Models...\n",
      "Building Monthly-to-Quarterly Model...\n",
      "Fitting monthly model with 630 features\n",
      "Fitting monthly model with 630 features\n",
      "Extracted 3 monthly factors\n",
      "Auto-detected start date: 1948-03-31 00:00:00 (first quarter with complete data)\n",
      "Using 244 quarters of data\n",
      "Fitting MIDAS model with 3 monthly factors, 4 GDP lags\n",
      "Using 238 observations after trimming 6 periods with lag-induced NaNs\n",
      "Monthly_MIDAS Model successfully built\n",
      "Monthly_MIDAS Model saved to ./output/monthly_midas_model.pkl\n",
      "Building Baseline Models...\n",
      "AR Baseline Model successfully built\n",
      "MA-4 Baseline Model defined\n",
      "\n",
      "7. Evaluating Models...\n",
      "Generated predictions for Monthly_MIDAS: 63 quarters\n",
      "Generated predictions for AR_Baseline: 63 quarters\n",
      "Generated predictions for MA_4_Baseline: 63 quarters\n",
      "Calculating evaluation metrics...\n",
      "\n",
      "Key Performance Metrics:\n",
      "--------------------------------------------------------------------------------\n",
      "Model                           RMSE        MAE    Dir Acc\n",
      "--------------------------------------------------------------------------------\n",
      "Monthly_MIDAS                 1.8460     0.9276     0.4194\n",
      "AR_Baseline                   1.9823     0.8485     0.4516\n",
      "MA_4_Baseline                 1.9151     0.9072     0.4032\n",
      "\n",
      "Generating evaluation plots...\n",
      "pandas_datareader not available for recession shading\n",
      "Forecasts plot saved to ./output/gdp_forecasts.png\n",
      "Error distribution plot saved to ./output/error_distribution.png\n",
      "Rolling metrics plot saved to ./output/rolling_metrics.png\n",
      "\n",
      "8. Generating Final Report...\n",
      "pandas_datareader not available for recession shading\n",
      "Comprehensive evaluation report saved to ./output/gdp_forecast_evaluation.md\n",
      "\n",
      "9. Workflow Completed\n",
      "================================================================================\n",
      "Monthly-Only GDP Forecasting Workflow completed at 2025-05-09 14:09:48.479672\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import copy\n",
    "import warnings\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Silence warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def trace_nans(name, df, threshold=0):\n",
    "    \"\"\"\n",
    "    Comprehensive NaN tracing function for pandas DataFrames.\n",
    "    \"\"\"\n",
    "    if isinstance(df, pd.Series):\n",
    "        nan_count = df.isna().sum()\n",
    "        total = len(df)\n",
    "        if nan_count > 0:\n",
    "            print(f\"WARNING: {name} Series contains {nan_count}/{total} NaNs ({nan_count/total:.2%})\")\n",
    "        return\n",
    "        \n",
    "    nan_count = df.isna().sum().sum()\n",
    "    if nan_count > 0:\n",
    "        rows, cols = df.shape\n",
    "        total_cells = rows * cols\n",
    "        \n",
    "        print(f\"WARNING: {name} contains {nan_count}/{total_cells} NaNs ({nan_count/total_cells:.2%})\")\n",
    "        \n",
    "        cols_with_nans = df.columns[df.isna().sum() > threshold]\n",
    "        if len(cols_with_nans) > 0:\n",
    "            print(f\"  Columns with > {threshold} NaNs:\")\n",
    "            for col in cols_with_nans:\n",
    "                col_nans = df[col].isna().sum()\n",
    "                print(f\"    {col}: {col_nans}/{rows} NaNs ({col_nans/rows:.2%})\")\n",
    "        \n",
    "        row_nan_counts = df.isna().sum(axis=1)\n",
    "        rows_with_many_nans = row_nan_counts[row_nan_counts > cols//4].sort_values(ascending=False)\n",
    "        if len(rows_with_many_nans) > 0:\n",
    "            print(f\"  Rows with significant NaNs:\")\n",
    "            for idx, count in rows_with_many_nans.head(5).items():\n",
    "                print(f\"    Row at {idx}: {count}/{cols} NaNs ({count/cols:.2%})\")\n",
    "        \n",
    "        first_rows_nan_pct = df.head(rows//10).isna().sum().sum() / (rows//10 * cols)\n",
    "        last_rows_nan_pct = df.tail(rows//10).isna().sum().sum() / (rows//10 * cols)\n",
    "        if first_rows_nan_pct > 0.1:\n",
    "            print(f\"  First 10% of rows have {first_rows_nan_pct:.2%} NaNs - possible lag/window effect\")\n",
    "        if last_rows_nan_pct > 0.1:\n",
    "            print(f\"  Last 10% of rows have {last_rows_nan_pct:.2%} NaNs - possible trailing window effect\")\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Module 1: Multi-Frequency Data Preprocessor\n",
    "#-----------------------------------------------------------------------------\n",
    "class MultiFrequencyPreprocessor:\n",
    "    \"\"\"\n",
    "    Enhanced data preprocessor for multi-frequency economic data.\n",
    "    This class handles different time frequencies (daily, weekly, monthly, quarterly)\n",
    "    and ensures proper alignment and processing for hierarchical modeling.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_folder):\n",
    "        \"\"\"\n",
    "        Initialize the MultiFrequencyPreprocessor with the folder containing CSV files.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_folder: str\n",
    "            Path to the folder containing CSV files\n",
    "        \"\"\"\n",
    "        self.data_folder = data_folder\n",
    "        self.available_files = self._get_available_files()\n",
    "        self.data_config = {}\n",
    "        self.frequency_data = {\n",
    "            'daily': None,\n",
    "            'weekly': None,\n",
    "            'monthly': None,\n",
    "            'quarterly': None\n",
    "        }\n",
    "        self.start_date = None\n",
    "        self.end_date = None\n",
    "        # Dictionaries to store processed data and factors\n",
    "        self.processed_data = {}\n",
    "        self.factors = {}\n",
    "        print(f\"Found {len(self.available_files)} files in {data_folder}\")\n",
    "    \n",
    "    def _get_available_files(self):\n",
    "        \"\"\"List all CSV files in the data folder.\"\"\"\n",
    "        # Normalize path to handle both forward and backward slashes\n",
    "        norm_path = os.path.normpath(self.data_folder)\n",
    "        files = glob.glob(os.path.join(norm_path, '*.csv'))\n",
    "        return [os.path.basename(f) for f in files]\n",
    "    \n",
    "    def set_config(self, data_config):\n",
    "        \"\"\"\n",
    "        Set the configuration for data loading and preprocessing.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_config: dict\n",
    "            Configuration dictionary for data loading\n",
    "        \"\"\"\n",
    "        self.data_config = data_config\n",
    "    \n",
    "    def set_date_range(self, start_date=None, end_date=None):\n",
    "        \"\"\"\n",
    "        Set the global date range for data processing.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        start_date: str or datetime\n",
    "            Start date for data processing (format: 'YYYY-MM-DD')\n",
    "        end_date: str or datetime\n",
    "            End date for data processing (format: 'YYYY-MM-DD')\n",
    "        \"\"\"\n",
    "        if start_date:\n",
    "            self.start_date = pd.to_datetime(start_date) if isinstance(start_date, str) else start_date\n",
    "        if end_date:\n",
    "            self.end_date = pd.to_datetime(end_date) if isinstance(end_date, str) else end_date\n",
    "    \n",
    "    def _load_csv(self, file_name, frequency):\n",
    "        \"\"\"\n",
    "        Load a CSV file and parse the date column.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        file_name: str\n",
    "            Name of the CSV file\n",
    "        frequency: str\n",
    "            Data frequency ('daily', 'weekly', 'monthly', 'quarterly')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            Loaded dataframe with date index\n",
    "        \"\"\"\n",
    "        # Normalize path\n",
    "        norm_path = os.path.normpath(self.data_folder)\n",
    "        file_path = os.path.join(norm_path, file_name)\n",
    "        \n",
    "        try:\n",
    "            # First try standard CSV loading\n",
    "            df = pd.read_csv(file_path, parse_dates=[0], index_col=0)\n",
    "            \n",
    "            # Check if index is datetime\n",
    "            if not pd.api.types.is_datetime64_any_dtype(df.index):\n",
    "                # Convert index to datetime\n",
    "                df.index = pd.to_datetime(df.index)\n",
    "            \n",
    "            # Apply frequency-specific processing\n",
    "            if frequency == 'daily':\n",
    "                # For daily data, ensure the index is business days\n",
    "                df = df.asfreq('B', method='ffill')\n",
    "            elif frequency == 'weekly':\n",
    "                # For weekly data, use end of week\n",
    "                df = df.asfreq('W-FRI', method='ffill')\n",
    "            elif frequency == 'monthly':\n",
    "                # For monthly data, use end of month\n",
    "                df = df.asfreq('M', method='ffill')\n",
    "            elif frequency == 'quarterly':\n",
    "                # For quarterly data, use end of quarter\n",
    "                df = df.asfreq('Q', method='ffill')\n",
    "            \n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_name}: {e}\")\n",
    "            \n",
    "            # Try alternative approach\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                date_col = df.columns[0]\n",
    "                \n",
    "                # Try different date formats\n",
    "                try:\n",
    "                    df[date_col] = pd.to_datetime(df[date_col])\n",
    "                except:\n",
    "                    for date_format in ['%Y-%m-%d', '%d/%m/%Y', '%m/%d/%Y', '%Y/%m/%d']:\n",
    "                        try:\n",
    "                            df[date_col] = pd.to_datetime(df[date_col], format=date_format)\n",
    "                            break\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                \n",
    "                df.set_index(date_col, inplace=True)\n",
    "                \n",
    "                # Apply frequency-specific processing\n",
    "                if frequency == 'daily':\n",
    "                    df = df.asfreq('B', method='ffill')\n",
    "                elif frequency == 'weekly':\n",
    "                    df = df.asfreq('W-FRI', method='ffill')\n",
    "                elif frequency == 'monthly':\n",
    "                    df = df.asfreq('M', method='ffill')\n",
    "                elif frequency == 'quarterly':\n",
    "                    df = df.asfreq('Q', method='ffill')\n",
    "                \n",
    "                return df\n",
    "            \n",
    "            except Exception as nested_e:\n",
    "                print(f\"Failed to load {file_name} after multiple attempts: {nested_e}\")\n",
    "                raise\n",
    "    \n",
    "    def _apply_transformation(self, df, column, transformation):\n",
    "        \"\"\"\n",
    "        Apply the specified transformation to a column.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df: pd.DataFrame\n",
    "            DataFrame containing the column\n",
    "        column: str\n",
    "            Column name to transform\n",
    "        transformation: str or list\n",
    "            Transformation type ('raw', 'pct_change', 'log_return', 'diff') or list\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list of tuples\n",
    "            List of (column_name, transformed_series) tuples\n",
    "        \"\"\"\n",
    "        if column not in df.columns:\n",
    "            print(f\"Warning: Column {column} not found in DataFrame\")\n",
    "            return []\n",
    "        \n",
    "        # Handle list of transformations\n",
    "        if isinstance(transformation, list):\n",
    "            result = []\n",
    "            for t in transformation:\n",
    "                column_name = f\"{column}_{t}\"\n",
    "                series = self._apply_single_transformation(df, column, t)\n",
    "                result.append((column_name, series))\n",
    "            return result\n",
    "        else:\n",
    "            # Handle single transformation\n",
    "            column_name = f\"{column}_{transformation}\" if transformation != 'raw' else column\n",
    "            series = self._apply_single_transformation(df, column, transformation)\n",
    "            return [(column_name, series)]\n",
    "    \n",
    "    def _apply_single_transformation(self, df, column, transformation):\n",
    "        \"\"\"\n",
    "        Apply a single transformation to a column with robust handling of edge cases.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df: pd.DataFrame\n",
    "            DataFrame containing the column\n",
    "        column: str\n",
    "            Column name to transform\n",
    "        transformation: str\n",
    "            Transformation type ('raw', 'pct_change', 'log_return', 'diff', 'yoy')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.Series\n",
    "            Transformed series\n",
    "        \"\"\"\n",
    "        if transformation == 'raw':\n",
    "            return df[column]\n",
    "        elif transformation == 'pct_change':\n",
    "            # Calculate percentage change with correct usage\n",
    "            pct = df[column].ffill().pct_change() * 100\n",
    "            # Fill first value with 0 for continuity\n",
    "            if len(pct) > 0:\n",
    "                pct.iloc[0] = 0\n",
    "            return pct\n",
    "        elif transformation == 'log_return':\n",
    "            # Calculate log return (continuously compounded return)\n",
    "            log_ret = (np.log(df[column]) - np.log(df[column].shift(1))) * 100\n",
    "            # Fill first value with 0 for continuity\n",
    "            if len(log_ret) > 0:\n",
    "                log_ret.iloc[0] = 0\n",
    "            return log_ret\n",
    "        elif transformation == 'diff':\n",
    "            # Calculate first difference\n",
    "            diff = df[column].diff()\n",
    "            # Fill first value with 0 for continuity\n",
    "            if len(diff) > 0:\n",
    "                diff.iloc[0] = 0\n",
    "            return diff\n",
    "        elif transformation == 'yoy':\n",
    "            # Calculate year-over-year percentage change\n",
    "            yoy = df[column].ffill().pct_change(periods=12) * 100\n",
    "            # Forward fill NaN values\n",
    "            yoy = yoy.ffill()\n",
    "            return yoy\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown transformation: {transformation}\")\n",
    "    \n",
    "    def _calculate_ratios(self, data_dict, ratio_config):\n",
    "        \"\"\"\n",
    "        Calculate financial ratios from base time series.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_dict: dict\n",
    "            Dictionary of DataFrames\n",
    "        ratio_config: dict\n",
    "            Configuration for ratio calculation\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary with ratio DataFrames added\n",
    "        \"\"\"\n",
    "        result_dict = data_dict.copy()\n",
    "        \n",
    "        for ratio_name, config in ratio_config.items():\n",
    "            try:\n",
    "                numerator_key = config['numerator']\n",
    "                denominator_key = config['denominator']\n",
    "                transformations = config.get('transformations', ['raw'])\n",
    "                \n",
    "                # Get the component series\n",
    "                if numerator_key in data_dict and denominator_key in data_dict:\n",
    "                    numerator = data_dict[numerator_key].iloc[:, 0]  # Assume first column\n",
    "                    denominator = data_dict[denominator_key].iloc[:, 0]  # Assume first column\n",
    "                    \n",
    "                    # Calculate the ratio\n",
    "                    ratio = numerator / denominator\n",
    "                    ratio_df = pd.DataFrame({f\"{ratio_name}_raw\": ratio})\n",
    "                    \n",
    "                    # Apply transformations\n",
    "                    for transform in transformations:\n",
    "                        if transform != 'raw':\n",
    "                            transformed_series = self._apply_single_transformation(ratio_df, f\"{ratio_name}_raw\", transform)\n",
    "                            ratio_df[f\"{ratio_name}_{transform}\"] = transformed_series\n",
    "                    \n",
    "                    # Add to result\n",
    "                    result_dict[ratio_name] = ratio_df\n",
    "                    print(f\"Created ratio: {ratio_name} with {len(ratio_df)} observations\")\n",
    "                else:\n",
    "                    print(f\"Warning: Could not create ratio {ratio_name}. Missing component series.\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating ratio {ratio_name}: {e}\")\n",
    "        \n",
    "        return result_dict\n",
    "    \n",
    "    def process_frequency_data(self, frequency):\n",
    "        \"\"\"\n",
    "        Process data for a specific frequency.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        frequency: str\n",
    "            Data frequency ('daily', 'weekly', 'monthly', 'quarterly')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            Processed DataFrame for the specified frequency\n",
    "        \"\"\"\n",
    "        if frequency not in self.data_config:\n",
    "            print(f\"No configuration found for {frequency} data\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Processing {frequency} data...\")\n",
    "        freq_config = self.data_config[frequency]\n",
    "        \n",
    "        # Load and transform individual files\n",
    "        data_dict = {}\n",
    "        \n",
    "        for file_name, config in freq_config.get('files', {}).items():\n",
    "            if file_name not in self.available_files:\n",
    "                print(f\"Warning: {file_name} not found, skipping\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Load CSV file\n",
    "                df = self._load_csv(file_name, frequency)\n",
    "                \n",
    "                # Apply date filtering if specified\n",
    "                if 'start_date' in config:\n",
    "                    df = df[df.index >= pd.to_datetime(config['start_date'])]\n",
    "                elif self.start_date:\n",
    "                    df = df[df.index >= self.start_date]\n",
    "                \n",
    "                if self.end_date:\n",
    "                    df = df[df.index <= self.end_date]\n",
    "                \n",
    "                # Apply transformations\n",
    "                transformed_columns = []\n",
    "                for column in config['columns']:\n",
    "                    # Get transformation type\n",
    "                    transformation = config['transformations'].get(column, 'raw')\n",
    "                    \n",
    "                    # Apply transformation\n",
    "                    results = self._apply_transformation(df, column, transformation)\n",
    "                    \n",
    "                    # Store results\n",
    "                    for col_name, series in results:\n",
    "                        # Create descriptive name: filename_column_transformation\n",
    "                        file_prefix = file_name.split('.')[0]  # Remove extension\n",
    "                        prefixed_name = f\"{file_prefix}_{col_name}\"\n",
    "                        transformed_columns.append((prefixed_name, series))\n",
    "                \n",
    "                # Create DataFrame from transformed columns\n",
    "                if transformed_columns:\n",
    "                    processed_df = pd.DataFrame({name: series for name, series in transformed_columns})\n",
    "                    processed_df.index = df.index\n",
    "                    \n",
    "                    # Store in data dictionary\n",
    "                    key = file_name.split('.')[0]  # Use filename without extension\n",
    "                    data_dict[key] = processed_df\n",
    "                    \n",
    "                    print(f\"Processed {file_name}: {len(processed_df)} observations, {len(processed_df.columns)} features\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        # Calculate ratios if configured\n",
    "        if 'ratios' in freq_config:\n",
    "            data_dict = self._calculate_ratios(data_dict, freq_config['ratios'])\n",
    "        \n",
    "        # Merge all DataFrames\n",
    "        if data_dict:\n",
    "            merged_df = None\n",
    "            for _, df in data_dict.items():\n",
    "                if merged_df is None:\n",
    "                    merged_df = df.copy()\n",
    "                else:\n",
    "                    merged_df = merged_df.join(df, how='outer')\n",
    "            \n",
    "            # Handle missing values\n",
    "            if merged_df is not None:\n",
    "                # Forward fill for continuity\n",
    "                merged_df = merged_df.ffill()\n",
    "                # Then backward fill any remaining NaNs at the beginning\n",
    "                merged_df = merged_df.bfill()\n",
    "                \n",
    "                # Store in processed data dictionary\n",
    "                self.processed_data[frequency] = merged_df\n",
    "                \n",
    "                print(f\"Final {frequency} dataset: {len(merged_df)} observations, {len(merged_df.columns)} features\")\n",
    "                return merged_df\n",
    "            else:\n",
    "                print(f\"No valid data found for {frequency} frequency\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"No data processed for {frequency} frequency\")\n",
    "            return None\n",
    "    \n",
    "    def process_all_frequencies(self):\n",
    "        \"\"\"\n",
    "        Process data for all configured frequencies.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of processed DataFrames for each frequency\n",
    "        \"\"\"\n",
    "        for frequency in self.data_config.keys():\n",
    "            self.process_frequency_data(frequency)\n",
    "        \n",
    "        return self.processed_data\n",
    "    \n",
    "    def align_to_dates(self, source_df, target_dates, method='last'):\n",
    "        \"\"\"\n",
    "        Align source DataFrame to target dates using specified method.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        source_df: pd.DataFrame\n",
    "            Source DataFrame to align\n",
    "        target_dates: pd.DatetimeIndex\n",
    "            Target dates to align to\n",
    "        method: str\n",
    "            Method for alignment ('last', 'nearest', 'linear')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            Aligned DataFrame\n",
    "        \"\"\"\n",
    "        # Initialize aligned DataFrame with the same columns as source_df\n",
    "        aligned_df = pd.DataFrame(index=target_dates, columns=source_df.columns)\n",
    "        \n",
    "        if method == 'last':\n",
    "            # For each target date, find the last available observation\n",
    "            for date in target_dates:\n",
    "                prev_data = source_df[source_df.index <= date]\n",
    "                if not prev_data.empty:\n",
    "                    # Get the last row as a Series and assign values column by column\n",
    "                    last_row = prev_data.iloc[-1]\n",
    "                    for col in source_df.columns:\n",
    "                        aligned_df.loc[date, col] = last_row[col]\n",
    "        \n",
    "        elif method == 'nearest':\n",
    "            # For each target date, find the nearest observation\n",
    "            for date in target_dates:\n",
    "                # Calculate absolute difference in days\n",
    "                source_dates = source_df.index\n",
    "                if len(source_dates) > 0:\n",
    "                    # Convert to numpy arrays for vectorized operations\n",
    "                    days_diff = np.abs((source_dates - date).days.values)\n",
    "                    nearest_idx = np.argmin(days_diff)\n",
    "                    \n",
    "                    # Assign values column by column\n",
    "                    nearest_row = source_df.iloc[nearest_idx]\n",
    "                    for col in source_df.columns:\n",
    "                        aligned_df.loc[date, col] = nearest_row[col]\n",
    "        \n",
    "        elif method == 'linear':\n",
    "            # This method can be implemented directly with pandas reindex\n",
    "            aligned_df = source_df.reindex(index=sorted(list(source_df.index) + list(target_dates)))\n",
    "            \n",
    "            # Apply linear interpolation\n",
    "            aligned_df = aligned_df.interpolate(method='linear')\n",
    "            \n",
    "            # Extract only the target dates\n",
    "            aligned_df = aligned_df.reindex(target_dates)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown alignment method: {method}\")\n",
    "        \n",
    "        # Handle any remaining NaNs by forward filling, then backward filling\n",
    "        aligned_df = aligned_df.ffill().bfill()\n",
    "        \n",
    "        return aligned_df\n",
    "    \n",
    "    def generate_hierarchical_dataset(self, target_frequency='quarterly'):\n",
    "        \"\"\"\n",
    "        Generate hierarchical dataset with higher-frequency data aligned to lower frequency.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        target_frequency: str\n",
    "            Target frequency for alignment ('quarterly', 'monthly', 'weekly')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of aligned datasets for hierarchical modeling\n",
    "        \"\"\"\n",
    "        hierarchical_data = {}\n",
    "        \n",
    "        # Define frequency hierarchy\n",
    "        freq_hierarchy = {\n",
    "            'quarterly': ['monthly', 'weekly', 'daily'],\n",
    "            'monthly': ['weekly', 'daily'],\n",
    "            'weekly': ['daily']\n",
    "        }\n",
    "        \n",
    "        # Get target dates\n",
    "        if target_frequency not in self.processed_data:\n",
    "            raise ValueError(f\"No processed data found for {target_frequency} frequency\")\n",
    "        \n",
    "        target_dates = self.processed_data[target_frequency].index\n",
    "        hierarchical_data[target_frequency] = self.processed_data[target_frequency]\n",
    "        \n",
    "        # Align higher frequency data to target dates\n",
    "        for higher_freq in freq_hierarchy.get(target_frequency, []):\n",
    "            if higher_freq in self.processed_data:\n",
    "                aligned_df = self.align_to_dates(\n",
    "                    self.processed_data[higher_freq],\n",
    "                    target_dates,\n",
    "                    method='last'  # Use last available observation\n",
    "                )\n",
    "                hierarchical_data[f\"{higher_freq}_aligned\"] = aligned_df\n",
    "        \n",
    "        return hierarchical_data\n",
    "    \n",
    "    def plot_data_overview(self, frequency=None):\n",
    "        \"\"\"\n",
    "        Plot an overview of the processed data to help with visualization.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        frequency: str or None\n",
    "            Frequency to plot, or None to plot all\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            Matplotlib figure object\n",
    "        \"\"\"\n",
    "        if frequency:\n",
    "            if frequency not in self.processed_data:\n",
    "                raise ValueError(f\"No processed data found for {frequency} frequency\")\n",
    "            frequencies = [frequency]\n",
    "        else:\n",
    "            frequencies = list(self.processed_data.keys())\n",
    "        \n",
    "        n_freqs = len(frequencies)\n",
    "        fig, axes = plt.subplots(n_freqs, 1, figsize=(15, 6*n_freqs))\n",
    "        \n",
    "        if n_freqs == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, freq in enumerate(frequencies):\n",
    "            df = self.processed_data[freq]\n",
    "            \n",
    "            # Select a subset of columns if there are too many\n",
    "            max_cols = 10\n",
    "            if len(df.columns) > max_cols:\n",
    "                # Choose evenly spaced columns\n",
    "                indices = np.linspace(0, len(df.columns)-1, max_cols, dtype=int)\n",
    "                plot_cols = [df.columns[i] for i in indices]\n",
    "            else:\n",
    "                plot_cols = df.columns\n",
    "            \n",
    "            # Plot each column\n",
    "            for col in plot_cols:\n",
    "                axes[i].plot(df.index, df[col], label=col)\n",
    "            \n",
    "            axes[i].set_title(f\"{freq.capitalize()} Data Overview\")\n",
    "            axes[i].set_xlabel('Date')\n",
    "            axes[i].set_ylabel('Value')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            axes[i].legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Module 2: Technical Indicators for Multi-Frequency Data\n",
    "#-----------------------------------------------------------------------------\n",
    "class MultiFrequencyTechnicalIndicators:\n",
    "    \"\"\"\n",
    "    Technical indicators calculation for multi-frequency economic data.\n",
    "    This class implements SMA, RSI, and ROC with frequency-appropriate parameters\n",
    "    and enhanced metrics for economic time series.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def get_frequency_params(frequency):\n",
    "        \"\"\"\n",
    "        Get appropriate technical indicator parameters for each frequency.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        frequency: str\n",
    "            Data frequency ('daily', 'weekly', 'monthly', 'quarterly')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of parameters for each indicator type\n",
    "        \"\"\"\n",
    "        if frequency == 'daily':\n",
    "            # For daily data - standard financial parameters\n",
    "            return {\n",
    "                'sma': [5, 20, 60, 200],  # Short, medium, quarter, year\n",
    "                'rsi': [14, 21],  # Standard and extended\n",
    "                'roc': [1, 5, 20, 60]  # Daily, weekly, monthly, quarter\n",
    "            }\n",
    "        elif frequency == 'weekly':\n",
    "            # For weekly data - adjusted to weekly scale\n",
    "            return {\n",
    "                'sma': [4, 12, 26, 52],  # Month, quarter, half-year, year\n",
    "                'rsi': [8, 12],  # ~1.5-2 months\n",
    "                'roc': [1, 4, 13, 26]  # Week, month, quarter, half-year\n",
    "            }\n",
    "        elif frequency == 'monthly':\n",
    "            # For monthly data - adjusted to monthly scale\n",
    "            return {\n",
    "                'sma': [3, 6, 12, 24],  # Quarter, half-year, year, two years\n",
    "                'rsi': [6, 9],  # Half-year, three quarters\n",
    "                'roc': [1, 3, 6, 12]  # Month, quarter, half-year, year\n",
    "            }\n",
    "        elif frequency == 'quarterly':\n",
    "            # For quarterly data - adjusted to quarterly scale\n",
    "            return {\n",
    "                'sma': [2, 4, 8, 12],  # Half-year, year, two years, three years\n",
    "                'rsi': [4, 6],  # Year, year and half\n",
    "                'roc': [1, 2, 4, 8]  # Quarter, half-year, year, two years\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown frequency: {frequency}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _calculate_trend_direction(series, periods=1):\n",
    "        \"\"\"\n",
    "        Calculate trend direction for a series with proper handling of zeros and NaNs.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        series: pandas.Series\n",
    "            Series to calculate trend direction for\n",
    "        periods: int\n",
    "            Number of periods to look back\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.Series\n",
    "            Series containing trend direction values:\n",
    "            1 for rising, -1 for falling, 0 for no change\n",
    "        \"\"\"\n",
    "        # Calculate direction safely\n",
    "        diff = series.diff(periods)\n",
    "        \n",
    "        # Initialize direction series\n",
    "        direction = pd.Series(0, index=series.index)\n",
    "        \n",
    "        # Positive direction\n",
    "        direction[diff > 0] = 1\n",
    "        \n",
    "        # Negative direction\n",
    "        direction[diff < 0] = -1\n",
    "        \n",
    "        # For zero-diff values, carry forward previous direction to avoid flicker\n",
    "        # but only where series values are valid\n",
    "        zero_mask = (diff == 0) & series.notna()\n",
    "        if zero_mask.any():\n",
    "            # Forward-fill only zero-diff positions\n",
    "            direction_filled = direction.copy()\n",
    "            direction_filled[zero_mask] = np.nan\n",
    "            direction_filled = direction_filled.ffill()\n",
    "            \n",
    "            # Update direction where diff was zero\n",
    "            direction[zero_mask] = direction_filled[zero_mask]\n",
    "        \n",
    "        return direction\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_sma(df, column, windows=None, include_trend=True, include_crossovers=True):\n",
    "        \"\"\"\n",
    "        Calculate Simple Moving Averages with enhanced metrics.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df: pandas.DataFrame\n",
    "            DataFrame containing the data\n",
    "        column: str\n",
    "            Column name to calculate SMA for\n",
    "        windows: list\n",
    "            List of window sizes for SMA calculation\n",
    "        include_trend: bool\n",
    "            Whether to include trend direction\n",
    "        include_crossovers: bool\n",
    "            Whether to include crossover signals\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with SMA values and enhanced metrics\n",
    "        \"\"\"\n",
    "        if windows is None:\n",
    "            # Default parameters - will be overridden by frequency-specific ones\n",
    "            windows = [5, 20, 60, 200]\n",
    "        \n",
    "        result_df = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # Calculate SMAs for each window\n",
    "        for window in windows:\n",
    "            # Calculate SMA with proper min_periods\n",
    "            min_periods = max(1, window // 4)\n",
    "            sma = df[column].rolling(window=window, min_periods=min_periods).mean()\n",
    "            \n",
    "            sma_name = f\"{column}_SMA_{window}\"\n",
    "            result_df[sma_name] = sma\n",
    "            \n",
    "            # Calculate percentage difference from SMA\n",
    "            valid_mask = (sma != 0) & sma.notna() & df[column].notna()\n",
    "            pct_diff = pd.Series(index=df.index, dtype=float)\n",
    "            pct_diff[valid_mask] = (df[column][valid_mask] - sma[valid_mask]) / sma[valid_mask] * 100\n",
    "            result_df[f\"{sma_name}_pct_diff\"] = pct_diff\n",
    "            \n",
    "            # Calculate trend if requested\n",
    "            if include_trend:\n",
    "                trend = MultiFrequencyTechnicalIndicators._calculate_trend_direction(sma)\n",
    "                result_df[f\"{sma_name}_trend\"] = trend\n",
    "        \n",
    "        # Calculate crossovers if requested and we have at least two windows\n",
    "        if include_crossovers and len(windows) >= 2:\n",
    "            # Sort windows to ensure correct fast/slow designation\n",
    "            sorted_windows = sorted(windows)\n",
    "            \n",
    "            # Calculate crossovers between adjacent SMAs\n",
    "            for i in range(len(sorted_windows) - 1):\n",
    "                fast_window = sorted_windows[i]\n",
    "                slow_window = sorted_windows[i+1]\n",
    "                \n",
    "                fast_sma = result_df[f\"{column}_SMA_{fast_window}\"]\n",
    "                slow_sma = result_df[f\"{column}_SMA_{slow_window}\"]\n",
    "                \n",
    "                # Calculate difference between fast and slow SMAs\n",
    "                diff = fast_sma - slow_sma\n",
    "                \n",
    "                # Calculate crossover signal\n",
    "                crossover = pd.Series(0, index=df.index)\n",
    "                \n",
    "                # Find where diff changes sign\n",
    "                diff_sign = np.sign(diff)\n",
    "                sign_change = diff_sign.diff().fillna(0)\n",
    "                \n",
    "                # 1 for bullish crossover (fast crosses above slow)\n",
    "                crossover[sign_change > 0] = 1\n",
    "                \n",
    "                # -1 for bearish crossover (fast crosses below slow)\n",
    "                crossover[sign_change < 0] = -1\n",
    "                \n",
    "                crossover_name = f\"{column}_SMA_{fast_window}_{slow_window}_crossover\"\n",
    "                result_df[crossover_name] = crossover\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_rsi(df, column, windows=None, include_trend=True, include_zones=True):\n",
    "        \"\"\"\n",
    "        Calculate Relative Strength Index with enhanced metrics.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df: pandas.DataFrame\n",
    "            DataFrame containing the data\n",
    "        column: str\n",
    "            Column name to calculate RSI for\n",
    "        windows: list\n",
    "            List of window sizes for RSI calculation\n",
    "        include_trend: bool\n",
    "            Whether to include trend direction\n",
    "        include_zones: bool\n",
    "            Whether to include overbought/oversold zone indicators\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with RSI values and enhanced metrics\n",
    "        \"\"\"\n",
    "        if windows is None:\n",
    "            # Default parameters - will be overridden by frequency-specific ones\n",
    "            windows = [14, 21]\n",
    "        \n",
    "        result_df = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        for window in windows:\n",
    "            # Calculate price changes\n",
    "            delta = df[column].diff()\n",
    "            \n",
    "            # Create separate gain and loss series with proper dtype\n",
    "            gain = pd.Series(0.0, index=delta.index)  # Use float dtype\n",
    "            loss = pd.Series(0.0, index=delta.index)  # Use float dtype\n",
    "            \n",
    "            # Set values for gain and loss series using .loc for proper assignment\n",
    "            gain.loc[delta > 0] = delta[delta > 0]\n",
    "            loss.loc[delta < 0] = -delta[delta < 0]  # Make losses positive\n",
    "            \n",
    "            # First values are NaN\n",
    "            gain.iloc[0] = 0.0\n",
    "            loss.iloc[0] = 0.0\n",
    "            \n",
    "            # Calculate RSI using Wilder's method\n",
    "            # First calculate simple averages for initial periods\n",
    "            avg_gain = gain.rolling(window=window, min_periods=1).mean()\n",
    "            avg_loss = loss.rolling(window=window, min_periods=1).mean()\n",
    "            \n",
    "            # Then use the Wilder's smoothing method\n",
    "            for i in range(window, len(gain)):\n",
    "                avg_gain.iloc[i] = (avg_gain.iloc[i-1] * (window-1) + gain.iloc[i]) / window\n",
    "                avg_loss.iloc[i] = (avg_loss.iloc[i-1] * (window-1) + loss.iloc[i]) / window\n",
    "            \n",
    "            # Calculate RS and RSI\n",
    "            # Avoid division by zero with epsilon\n",
    "            epsilon = np.finfo(float).eps\n",
    "            rs = avg_gain / avg_loss.replace(0, epsilon)\n",
    "            rsi = 100 - (100 / (1 + rs))\n",
    "            \n",
    "            # Ensure RSI is within [0, 100] bounds\n",
    "            rsi = np.clip(rsi, 0, 100)\n",
    "            \n",
    "            rsi_name = f\"{column}_RSI_{window}\"\n",
    "            result_df[rsi_name] = rsi\n",
    "            \n",
    "            # Calculate trend if requested\n",
    "            if include_trend:\n",
    "                trend = MultiFrequencyTechnicalIndicators._calculate_trend_direction(rsi)\n",
    "                result_df[f\"{rsi_name}_trend\"] = trend\n",
    "            \n",
    "            # Add overbought/oversold indicators if requested\n",
    "            if include_zones:\n",
    "                # Overbought zone (RSI > 70)\n",
    "                result_df[f\"{rsi_name}_overbought\"] = (rsi > 70).astype(int)\n",
    "                \n",
    "                # Oversold zone (RSI < 30)\n",
    "                result_df[f\"{rsi_name}_oversold\"] = (rsi < 30).astype(int)\n",
    "                \n",
    "                # Initialize divergence column\n",
    "                result_df[f\"{rsi_name}_divergence\"] = 0\n",
    "                \n",
    "                # Calculate divergence between price and RSI\n",
    "                # Instead of using chained assignment, we'll create and assign a complete array\n",
    "                divergence_window = max(5, window // 3)\n",
    "                divergence_values = np.zeros(len(df))\n",
    "                \n",
    "                # Process in batches to improve performance\n",
    "                batch_size = 1000  # Process in batches\n",
    "                for start_idx in range(divergence_window, len(df), batch_size):\n",
    "                    end_idx = min(start_idx + batch_size, len(df))\n",
    "                    \n",
    "                    for i in range(start_idx, end_idx):\n",
    "                        # Get windows for analysis\n",
    "                        price_window = df[column].iloc[i-divergence_window:i+1]\n",
    "                        rsi_window = rsi.iloc[i-divergence_window:i+1]\n",
    "                        \n",
    "                        # Skip if windows contain NaN\n",
    "                        if price_window.isna().any() or rsi_window.isna().any():\n",
    "                            continue\n",
    "                        \n",
    "                        # Check for bearish divergence\n",
    "                        # Price higher high but RSI lower high\n",
    "                        if (price_window.iloc[-1] > price_window.iloc[:-1].max() and\n",
    "                            rsi_window.iloc[-1] < rsi_window.iloc[:-1].max()):\n",
    "                            divergence_values[i] = -1  # Bearish\n",
    "                        \n",
    "                        # Check for bullish divergence\n",
    "                        # Price lower low but RSI higher low\n",
    "                        elif (price_window.iloc[-1] < price_window.iloc[:-1].min() and\n",
    "                            rsi_window.iloc[-1] > rsi_window.iloc[:-1].min()):\n",
    "                            divergence_values[i] = 1  # Bullish\n",
    "                \n",
    "                # Assign the complete divergence array at once (avoids chained assignment)\n",
    "                result_df.loc[:, f\"{rsi_name}_divergence\"] = divergence_values\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_roc(df, column, windows=None, include_trend=True, include_signal=True):\n",
    "        \"\"\"\n",
    "        Calculate Rate of Change with enhanced metrics.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df: pandas.DataFrame\n",
    "            DataFrame containing the data\n",
    "        column: str\n",
    "            Column name to calculate ROC for\n",
    "        windows: list\n",
    "            List of window sizes for ROC calculation\n",
    "        include_trend: bool\n",
    "            Whether to include trend direction\n",
    "        include_signal: bool\n",
    "            Whether to include signal line\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with ROC values and enhanced metrics\n",
    "        \"\"\"\n",
    "        if windows is None:\n",
    "            # Default parameters - will be overridden by frequency-specific ones\n",
    "            windows = [1, 5, 20, 60]\n",
    "        \n",
    "        result_df = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        for window in windows:\n",
    "            # Calculate ROC (percentage change over the specified window)\n",
    "            roc = df[column].pct_change(periods=window) * 100\n",
    "            \n",
    "            # Fill first value with 0 for continuity\n",
    "            roc.iloc[:window] = 0\n",
    "            \n",
    "            roc_name = f\"{column}_ROC_{window}\"\n",
    "            result_df[roc_name] = roc\n",
    "            \n",
    "            # Calculate trend if requested\n",
    "            if include_trend:\n",
    "                trend = MultiFrequencyTechnicalIndicators._calculate_trend_direction(roc)\n",
    "                result_df[f\"{roc_name}_trend\"] = trend\n",
    "            \n",
    "            # Calculate signal line if requested\n",
    "            if include_signal:\n",
    "                # Signal line is typically a moving average of the ROC\n",
    "                signal_window = max(5, window // 4)\n",
    "                signal = roc.rolling(window=signal_window, min_periods=1).mean()\n",
    "                result_df[f\"{roc_name}_signal\"] = signal\n",
    "                \n",
    "                # Calculate crossover signal\n",
    "                crossover = pd.Series(0, index=df.index)\n",
    "                \n",
    "                # ROC crossing above signal line = bullish\n",
    "                crossover[(roc.shift(1) <= signal.shift(1)) & (roc > signal)] = 1\n",
    "                \n",
    "                # ROC crossing below signal line = bearish\n",
    "                crossover[(roc.shift(1) >= signal.shift(1)) & (roc < signal)] = -1\n",
    "                \n",
    "                result_df[f\"{roc_name}_crossover\"] = crossover\n",
    "                \n",
    "                # Calculate histogram (difference between ROC and signal)\n",
    "                histogram = roc - signal\n",
    "                result_df[f\"{roc_name}_histogram\"] = histogram\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_indicators(df, frequency='daily'):\n",
    "        \"\"\"\n",
    "        Apply all technical indicators with frequency-appropriate parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df: pandas.DataFrame\n",
    "            DataFrame containing the data\n",
    "        frequency: str\n",
    "            Data frequency ('daily', 'weekly', 'monthly', 'quarterly')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with all technical indicators\n",
    "        \"\"\"\n",
    "        # Get frequency-specific parameters\n",
    "        params = MultiFrequencyTechnicalIndicators.get_frequency_params(frequency)\n",
    "        \n",
    "        # Store all indicators in a dictionary first to avoid DataFrame fragmentation\n",
    "        all_indicators = {}\n",
    "        \n",
    "        # Process each column in the DataFrame\n",
    "        for column in df.columns:\n",
    "            try:\n",
    "                # Calculate SMA\n",
    "                sma_df = MultiFrequencyTechnicalIndicators.calculate_sma(\n",
    "                    df, column, windows=params['sma'],\n",
    "                    include_trend=True, include_crossovers=True\n",
    "                )\n",
    "                \n",
    "                # Calculate RSI\n",
    "                rsi_df = MultiFrequencyTechnicalIndicators.calculate_rsi(\n",
    "                    df, column, windows=params['rsi'],\n",
    "                    include_trend=True, include_zones=True\n",
    "                )\n",
    "                \n",
    "                # Calculate ROC\n",
    "                roc_df = MultiFrequencyTechnicalIndicators.calculate_roc(\n",
    "                    df, column, windows=params['roc'],\n",
    "                    include_trend=True, include_signal=True\n",
    "                )\n",
    "                \n",
    "                # Combine all indicators into the dictionary\n",
    "                for col in sma_df.columns:\n",
    "                    all_indicators[f\"{column}_{col}\"] = sma_df[col]\n",
    "                \n",
    "                for col in rsi_df.columns:\n",
    "                    all_indicators[f\"{column}_{col}\"] = rsi_df[col]\n",
    "                \n",
    "                for col in roc_df.columns:\n",
    "                    all_indicators[f\"{column}_{col}\"] = roc_df[col]\n",
    "                \n",
    "                print(f\"Applied indicators to {column}: {len(sma_df.columns) + len(rsi_df.columns) + len(roc_df.columns)} new features\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error applying indicators to {column}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        # Create the result DataFrame in one go to avoid fragmentation\n",
    "        result_df = pd.DataFrame(all_indicators, index=df.index)\n",
    "        \n",
    "        # Handle NaN values properly\n",
    "        if result_df.isna().any().any():\n",
    "            # Use proper forward fill and backward fill\n",
    "            result_df = result_df.ffill().bfill()\n",
    "            # If still have NaNs, fill with zeros\n",
    "            result_df = result_df.fillna(0)\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Module 6: GDP Forecast Evaluator\n",
    "#-----------------------------------------------------------------------------\n",
    "class GDPForecastEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluation framework for GDP forecasting models.\n",
    "    This class provides comprehensive evaluation metrics and visualizations\n",
    "    for GDP forecasting performance.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the evaluator.\"\"\"\n",
    "        self.results = {}\n",
    "        self.models = {}\n",
    "        self.actual = None\n",
    "    \n",
    "    def add_model(self, name, predictions, actual=None):\n",
    "        \"\"\"\n",
    "        Add a model's predictions for evaluation.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        name: str\n",
    "            Model name\n",
    "        predictions: pandas.Series\n",
    "            Predicted GDP values\n",
    "        actual: pandas.Series, optional\n",
    "            Actual GDP values (if not already set)\n",
    "        \"\"\"\n",
    "        self.models[name] = predictions\n",
    "        \n",
    "        if actual is not None and self.actual is None:\n",
    "            self.actual = actual\n",
    "    \n",
    "    def calculate_metrics(self, rolling_window=None):\n",
    "        \"\"\"\n",
    "        Calculate evaluation metrics for all models.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        rolling_window: int, optional\n",
    "            Window size for rolling metrics calculation\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        if self.actual is None:\n",
    "            raise ValueError(\"Actual values not set. Provide actual values when adding a model.\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for model_name, predictions in self.models.items():\n",
    "            # Align predictions with actual values\n",
    "            common_index = self.actual.index.intersection(predictions.index)\n",
    "            y_true = self.actual.loc[common_index]\n",
    "            y_pred = predictions.loc[common_index]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = self._calculate_model_metrics(y_true, y_pred, model_name)\n",
    "            \n",
    "            # Add rolling metrics if requested\n",
    "            if rolling_window is not None and len(y_true) > rolling_window:\n",
    "                rolling_metrics = self._calculate_rolling_metrics(y_true, y_pred, rolling_window)\n",
    "                metrics.update(rolling_metrics)\n",
    "            \n",
    "            results[model_name] = metrics\n",
    "        \n",
    "        self.results = results\n",
    "        return results\n",
    "    \n",
    "    def _calculate_model_metrics(self, y_true, y_pred, model_name):\n",
    "        \"\"\"\n",
    "        Calculate comprehensive evaluation metrics for a model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_true: pandas.Series\n",
    "            Actual values\n",
    "        y_pred: pandas.Series\n",
    "            Predicted values\n",
    "        model_name: str\n",
    "            Model name\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "        \n",
    "        # Calculate basic error metrics\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        # Calculate directional accuracy\n",
    "        direction_true = np.sign(y_true.diff().fillna(0))\n",
    "        direction_pred = np.sign(y_pred.diff().fillna(0))\n",
    "        \n",
    "        # Ignore zero changes\n",
    "        nonzero_mask = direction_true != 0\n",
    "        if nonzero_mask.any():\n",
    "            direction_accuracy = np.mean(direction_true[nonzero_mask] == direction_pred[nonzero_mask])\n",
    "        else:\n",
    "            direction_accuracy = np.nan\n",
    "        \n",
    "        # Calculate mean absolute percentage error\n",
    "        # Use a safe version to handle zeros\n",
    "        nonzero_mask = y_true != 0\n",
    "        if nonzero_mask.any():\n",
    "            mape = np.mean(np.abs((y_true[nonzero_mask] - y_pred[nonzero_mask]) / y_true[nonzero_mask])) * 100\n",
    "        else:\n",
    "            mape = np.nan\n",
    "        \n",
    "        # Calculate Theil's U statistic\n",
    "        # U = sqrt(MSE(model)) / sqrt(MSE(naive))\n",
    "        # Naive forecast is previous value (no change)\n",
    "        naive_pred = y_true.shift(1).fillna(method='bfill')\n",
    "        naive_mse = mean_squared_error(y_true[1:], naive_pred[1:])\n",
    "        \n",
    "        if naive_mse > 0:\n",
    "            theils_u = np.sqrt(mse) / np.sqrt(naive_mse)\n",
    "        else:\n",
    "            theils_u = np.nan\n",
    "        \n",
    "        # Calculate advanced forecast accuracy metrics\n",
    "        # Mean Directional Accuracy (MDA)\n",
    "        actual_changes = y_true.diff().fillna(0)\n",
    "        predicted_changes = y_pred.diff().fillna(0)\n",
    "        mda = np.mean((actual_changes * predicted_changes) > 0)\n",
    "        \n",
    "        # Confusion matrix for directional forecasts\n",
    "        direction_true_binary = (actual_changes > 0).astype(int)\n",
    "        direction_pred_binary = (predicted_changes > 0).astype(int)\n",
    "        \n",
    "        true_pos = np.sum((direction_true_binary == 1) & (direction_pred_binary == 1))\n",
    "        false_pos = np.sum((direction_true_binary == 0) & (direction_pred_binary == 1))\n",
    "        true_neg = np.sum((direction_true_binary == 0) & (direction_pred_binary == 0))\n",
    "        false_neg = np.sum((direction_true_binary == 1) & (direction_pred_binary == 0))\n",
    "        \n",
    "        # Hit rate (% of positive changes correctly predicted)\n",
    "        if (true_pos + false_neg) > 0:\n",
    "            hit_rate = true_pos / (true_pos + false_neg)\n",
    "        else:\n",
    "            hit_rate = np.nan\n",
    "        \n",
    "        # False alarm rate (% of negative changes incorrectly predicted as positive)\n",
    "        if (false_pos + true_neg) > 0:\n",
    "            false_alarm_rate = false_pos / (false_pos + true_neg)\n",
    "        else:\n",
    "            false_alarm_rate = np.nan\n",
    "        \n",
    "        # Calculate over/underprediction bias\n",
    "        bias = np.mean(y_pred - y_true)\n",
    "        \n",
    "        # Create results dictionary\n",
    "        metrics = {\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'mape': mape,\n",
    "            'r2': r2,\n",
    "            'direction_accuracy': direction_accuracy,\n",
    "            'theils_u': theils_u,\n",
    "            'mean_directional_accuracy': mda,\n",
    "            'hit_rate': hit_rate,\n",
    "            'false_alarm_rate': false_alarm_rate,\n",
    "            'bias': bias,\n",
    "            'confusion_matrix': {\n",
    "                'true_pos': true_pos,\n",
    "                'false_pos': false_pos,\n",
    "                'true_neg': true_neg,\n",
    "                'false_neg': false_neg\n",
    "            },\n",
    "            'forecast_errors': y_pred - y_true\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _calculate_rolling_metrics(self, y_true, y_pred, window):\n",
    "        \"\"\"\n",
    "        Calculate rolling evaluation metrics.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_true: pandas.Series\n",
    "            Actual values\n",
    "        y_pred: pandas.Series\n",
    "            Predicted values\n",
    "        window: int\n",
    "            Rolling window size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of rolling metrics\n",
    "        \"\"\"\n",
    "        # Initialize rolling metrics\n",
    "        rolling_rmse = []\n",
    "        rolling_mae = []\n",
    "        rolling_direction_accuracy = []\n",
    "        \n",
    "        # Loop through rolling windows\n",
    "        for i in range(len(y_true) - window + 1):\n",
    "            window_true = y_true.iloc[i:i+window]\n",
    "            window_pred = y_pred.iloc[i:i+window]\n",
    "            \n",
    "            # Calculate metrics for this window\n",
    "            mse = np.mean((window_true - window_pred) ** 2)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = np.mean(np.abs(window_true - window_pred))\n",
    "            \n",
    "            # Calculate directional accuracy\n",
    "            direction_true = np.sign(window_true.diff().fillna(0))\n",
    "            direction_pred = np.sign(window_pred.diff().fillna(0))\n",
    "            \n",
    "            # Ignore zero changes\n",
    "            nonzero_mask = direction_true != 0\n",
    "            if nonzero_mask.any():\n",
    "                direction_accuracy = np.mean(direction_true[nonzero_mask] == direction_pred[nonzero_mask])\n",
    "            else:\n",
    "                direction_accuracy = np.nan\n",
    "            \n",
    "            # Add to lists\n",
    "            rolling_rmse.append(rmse)\n",
    "            rolling_mae.append(mae)\n",
    "            rolling_direction_accuracy.append(direction_accuracy)\n",
    "        \n",
    "        # Convert to pandas Series with appropriate index\n",
    "        index = y_true.index[window-1:]\n",
    "        rolling_metrics = {\n",
    "            'rolling_rmse': pd.Series(rolling_rmse, index=index[:len(rolling_rmse)]),\n",
    "            'rolling_mae': pd.Series(rolling_mae, index=index[:len(rolling_mae)]),\n",
    "            'rolling_direction_accuracy': pd.Series(rolling_direction_accuracy, index=index[:len(rolling_direction_accuracy)])\n",
    "        }\n",
    "        \n",
    "        return rolling_metrics\n",
    "    \n",
    "    def diebold_mariano_test(self, model1, model2, alternative='two-sided'):\n",
    "        \"\"\"\n",
    "        Perform Diebold-Mariano test to compare forecast accuracy.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model1: str\n",
    "            First model name\n",
    "        model2: str\n",
    "            Second model name\n",
    "        alternative: str\n",
    "            Alternative hypothesis ('two-sided', 'less', 'greater')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            DM statistic and p-value\n",
    "        \"\"\"\n",
    "        import statsmodels.api as sm\n",
    "        from scipy.stats import norm\n",
    "        \n",
    "        if model1 not in self.models or model2 not in self.models:\n",
    "            raise ValueError(f\"Models {model1} and/or {model2} not found\")\n",
    "        \n",
    "        # Get predictions\n",
    "        pred1 = self.models[model1]\n",
    "        pred2 = self.models[model2]\n",
    "        \n",
    "        # Align predictions with actual values\n",
    "        common_index = self.actual.index.intersection(pred1.index).intersection(pred2.index)\n",
    "        y_true = self.actual.loc[common_index]\n",
    "        y_pred1 = pred1.loc[common_index]\n",
    "        y_pred2 = pred2.loc[common_index]\n",
    "        \n",
    "        # Calculate squared errors\n",
    "        error1 = (y_true - y_pred1) ** 2\n",
    "        error2 = (y_true - y_pred2) ** 2\n",
    "        \n",
    "        # Calculate loss differential\n",
    "        d = error1 - error2\n",
    "        \n",
    "        # Calculate DM statistic\n",
    "        n = len(d)\n",
    "        if n <= 1:\n",
    "            return np.nan, np.nan\n",
    "        \n",
    "        # Estimate lag-1 autocorrelation of loss differential\n",
    "        acf_result = sm.tsa.acf(d, nlags=1, fft=False)\n",
    "        gamma_0 = acf_result[0]  # This is the variance of d\n",
    "        gamma_1 = acf_result[1] * gamma_0  # Autocovariance at lag 1\n",
    "        \n",
    "        # Calculate long-run variance with Newey-West correction for autocorrelation\n",
    "        lrvar = gamma_0 + 2 * gamma_1\n",
    "        \n",
    "        # Calculate DM statistic\n",
    "        dm_stat = d.mean() / np.sqrt(lrvar / n)\n",
    "        \n",
    "        # Calculate p-value based on alternative hypothesis\n",
    "        if alternative == 'two-sided':\n",
    "            p_value = 2 * (1 - norm.cdf(np.abs(dm_stat)))\n",
    "        elif alternative == 'less':\n",
    "            p_value = norm.cdf(dm_stat)\n",
    "        elif alternative == 'greater':\n",
    "            p_value = 1 - norm.cdf(dm_stat)\n",
    "        else:\n",
    "            raise ValueError(\"alternative must be 'two-sided', 'less', or 'greater'\")\n",
    "        \n",
    "        return dm_stat, p_value\n",
    "    \n",
    "    def plot_forecasts(self, start_date=None, end_date=None, figsize=(12, 6)):\n",
    "        \"\"\"\n",
    "        Plot actual vs predicted GDP.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        start_date: str or datetime, optional\n",
    "            Start date for plot\n",
    "        end_date: str or datetime, optional\n",
    "            End date for plot\n",
    "        figsize: tuple\n",
    "            Figure size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            Figure object\n",
    "        \"\"\"\n",
    "        if self.actual is None:\n",
    "            raise ValueError(\"Actual values not set\")\n",
    "        \n",
    "        # Filter by date range if provided\n",
    "        actual = self.actual\n",
    "        if start_date is not None:\n",
    "            actual = actual[actual.index >= pd.to_datetime(start_date)]\n",
    "        if end_date is not None:\n",
    "            actual = actual[actual.index <= pd.to_datetime(end_date)]\n",
    "        \n",
    "        # Create plot\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        # Plot actual values\n",
    "        ax.plot(actual.index, actual, 'k-', linewidth=2, label='Actual GDP')\n",
    "        \n",
    "        # Plot predictions for each model\n",
    "        colors = plt.cm.tab10.colors\n",
    "        for i, (model_name, predictions) in enumerate(self.models.items()):\n",
    "            # Filter predictions by date range\n",
    "            pred = predictions\n",
    "            if start_date is not None:\n",
    "                pred = pred[pred.index >= pd.to_datetime(start_date)]\n",
    "            if end_date is not None:\n",
    "                pred = pred[pred.index <= pd.to_datetime(end_date)]\n",
    "            \n",
    "            # Only use shared dates\n",
    "            common_index = actual.index.intersection(pred.index)\n",
    "            pred = pred.loc[common_index]\n",
    "            \n",
    "            color = colors[i % len(colors)]\n",
    "            ax.plot(pred.index, pred, 'o-', color=color, linewidth=1.5, label=f'{model_name}')\n",
    "        \n",
    "        # Add recession shading if available\n",
    "        try:\n",
    "            from pandas_datareader.data import DataReader\n",
    "            from pandas_datareader._utils import RemoteDataError\n",
    "            \n",
    "            try:\n",
    "                # Get US recession data from FRED\n",
    "                recession = DataReader('USREC', 'fred', start=actual.index[0], end=actual.index[-1])\n",
    "                \n",
    "                # Create shaded regions for recessions\n",
    "                last_date = None\n",
    "                for date, value in recession.itertuples():\n",
    "                    if value == 1.0:  # Recession period\n",
    "                        if last_date is None:\n",
    "                            last_date = date\n",
    "                    elif last_date is not None:\n",
    "                        # End of recession period\n",
    "                        ax.axvspan(last_date, date, alpha=0.2, color='gray')\n",
    "                        last_date = None\n",
    "                \n",
    "                # Handle case where we're still in a recession at the end of the data\n",
    "                if last_date is not None:\n",
    "                    ax.axvspan(last_date, actual.index[-1], alpha=0.2, color='gray')\n",
    "            \n",
    "            except RemoteDataError:\n",
    "                print(\"Could not retrieve recession data from FRED\")\n",
    "        \n",
    "        except ImportError:\n",
    "            print(\"pandas_datareader not available for recession shading\")\n",
    "        \n",
    "        # Add legend, grid, labels, etc.\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('GDP Growth (%)')\n",
    "        ax.set_title('GDP Growth: Actual vs Predicted')\n",
    "        ax.legend(loc='best')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Format y-axis to show percentage\n",
    "        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.1f}%'))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def plot_error_distribution(self, figsize=(12, 8)):\n",
    "        \"\"\"\n",
    "        Plot error distributions for all models.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        figsize: tuple\n",
    "            Figure size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            Figure object\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            self.calculate_metrics()\n",
    "        \n",
    "        n_models = len(self.models)\n",
    "        fig, axes = plt.subplots(n_models, 2, figsize=figsize)\n",
    "        \n",
    "        # Handle case with single model\n",
    "        if n_models == 1:\n",
    "            axes = axes.reshape(1, 2)\n",
    "        \n",
    "        # Iterate through models\n",
    "        for i, (model_name, metrics) in enumerate(self.results.items()):\n",
    "            errors = metrics['forecast_errors']\n",
    "            \n",
    "            # Histogram of errors\n",
    "            bins = min(20, max(5, int(np.sqrt(len(errors)))))\n",
    "            axes[i, 0].hist(errors, bins=bins, alpha=0.7, edgecolor='black')\n",
    "            axes[i, 0].axvline(x=0, color='r', linestyle='--')\n",
    "            axes[i, 0].set_title(f'{model_name}: Error Distribution')\n",
    "            axes[i, 0].set_xlabel('Forecast Error (Predicted - Actual)')\n",
    "            axes[i, 0].set_ylabel('Frequency')\n",
    "            \n",
    "            # Add metrics to plot\n",
    "            metrics_text = (\n",
    "                f\"RMSE: {metrics['rmse']:.4f}\\n\"\n",
    "                f\"MAE: {metrics['mae']:.4f}\\n\"\n",
    "                f\"Bias: {metrics['bias']:.4f}\\n\"\n",
    "                f\"Dir. Acc: {metrics['direction_accuracy']:.2f}\"\n",
    "            )\n",
    "            \n",
    "            axes[i, 0].annotate(\n",
    "                metrics_text, xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                va='top', ha='left', bbox=dict(boxstyle='round', fc='white', alpha=0.7)\n",
    "            )\n",
    "            \n",
    "            # Q-Q plot\n",
    "            from scipy import stats\n",
    "            \n",
    "            # Get z-scores for normal distribution\n",
    "            z = (errors - errors.mean()) / errors.std()\n",
    "            \n",
    "            # Create Q-Q plot\n",
    "            stats.probplot(z, dist=\"norm\", plot=axes[i, 1])\n",
    "            axes[i, 1].set_title(f'{model_name}: Q-Q Plot')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def plot_rolling_metrics(self, window=8, figsize=(12, 15)):\n",
    "        \"\"\"\n",
    "        Plot rolling metrics for all models.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        window: int\n",
    "            Rolling window size\n",
    "        figsize: tuple\n",
    "            Figure size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            Figure object\n",
    "        \"\"\"\n",
    "        # Ensure we have rolling metrics\n",
    "        if not self.results or 'rolling_rmse' not in next(iter(self.results.values())):\n",
    "            self.calculate_metrics(rolling_window=window)\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 1, figsize=figsize)\n",
    "        \n",
    "        # Plot rolling RMSE\n",
    "        for model_name, metrics in self.results.items():\n",
    "            axes[0].plot(\n",
    "                metrics['rolling_rmse'].index,\n",
    "                metrics['rolling_rmse'],\n",
    "                'o-',\n",
    "                label=model_name\n",
    "            )\n",
    "        \n",
    "        axes[0].set_title(f'Rolling RMSE ({window}-quarter window)')\n",
    "        axes[0].set_ylabel('RMSE')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].legend(loc='best')\n",
    "        \n",
    "        # Plot rolling MAE\n",
    "        for model_name, metrics in self.results.items():\n",
    "            axes[1].plot(\n",
    "                metrics['rolling_mae'].index,\n",
    "                metrics['rolling_mae'],\n",
    "                'o-',\n",
    "                label=model_name\n",
    "            )\n",
    "        \n",
    "        axes[1].set_title(f'Rolling MAE ({window}-quarter window)')\n",
    "        axes[1].set_ylabel('MAE')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].legend(loc='best')\n",
    "        \n",
    "        # Plot rolling direction accuracy\n",
    "        for model_name, metrics in self.results.items():\n",
    "            axes[2].plot(\n",
    "                metrics['rolling_direction_accuracy'].index,\n",
    "                metrics['rolling_direction_accuracy'],\n",
    "                'o-',\n",
    "                label=model_name\n",
    "            )\n",
    "        \n",
    "        axes[2].set_title(f'Rolling Direction Accuracy ({window}-quarter window)')\n",
    "        axes[2].set_ylabel('Direction Accuracy')\n",
    "        axes[2].set_ylim(0, 1)\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        axes[2].legend(loc='best')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def generate_report(self, output_file=None, include_plots=True):\n",
    "        \"\"\"\n",
    "        Generate comprehensive evaluation report.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_file: str, optional\n",
    "            Path to save report (HTML or markdown)\n",
    "        include_plots: bool\n",
    "            Whether to include plots in the report\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            Report content\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            self.calculate_metrics()\n",
    "        \n",
    "        # Start building report\n",
    "        report = \"# GDP Forecasting Model Evaluation Report\\n\\n\"\n",
    "        report += f\"Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\\n\\n\"\n",
    "        \n",
    "        # Add model summary\n",
    "        report += \"## Models Evaluated\\n\\n\"\n",
    "        report += f\"Number of models: {len(self.models)}\\n\"\n",
    "        report += f\"Evaluation period: {self.actual.index[0]} to {self.actual.index[-1]}\\n\"\n",
    "        report += f\"Number of observations: {len(self.actual)}\\n\\n\"\n",
    "        \n",
    "        # Add performance metrics table\n",
    "        report += \"## Performance Metrics\\n\\n\"\n",
    "        report += \"| Model | RMSE | MAE | MAPE | R² | Direction Accuracy | Theil's U | Bias |\\n\"\n",
    "        report += \"|-------|------|-----|------|----|--------------------|-----------|------|\\n\"\n",
    "        \n",
    "        for model_name, metrics in self.results.items():\n",
    "            report += (\n",
    "                f\"| {model_name} | \"\n",
    "                f\"{metrics['rmse']:.4f} | \"\n",
    "                f\"{metrics['mae']:.4f} | \"\n",
    "                f\"{metrics['mape']:.2f}% | \"\n",
    "                f\"{metrics['r2']:.4f} | \"\n",
    "                f\"{metrics['direction_accuracy']:.2f} | \"\n",
    "                f\"{metrics['theils_u']:.4f} | \"\n",
    "                f\"{metrics['bias']:.4f} |\\n\"\n",
    "            )\n",
    "        \n",
    "        report += \"\\n\"\n",
    "        \n",
    "        # Add detailed analysis for each model\n",
    "        report += \"## Detailed Model Analysis\\n\\n\"\n",
    "        \n",
    "        for model_name, metrics in self.results.items():\n",
    "            report += f\"### {model_name}\\n\\n\"\n",
    "            \n",
    "            # Confusion matrix\n",
    "            cm = metrics['confusion_matrix']\n",
    "            report += \"#### Directional Forecast Confusion Matrix\\n\\n\"\n",
    "            report += \"| | Predicted Up | Predicted Down |\\n\"\n",
    "            report += \"|------------|--------------|----------------|\\n\"\n",
    "            report += f\"| **Actual Up** | {cm['true_pos']} | {cm['false_neg']} |\\n\"\n",
    "            report += f\"| **Actual Down** | {cm['false_pos']} | {cm['true_neg']} |\\n\\n\"\n",
    "            \n",
    "            # Additional metrics\n",
    "            report += \"#### Additional Metrics\\n\\n\"\n",
    "            report += f\"* Mean Directional Accuracy: {metrics['mean_directional_accuracy']:.4f}\\n\"\n",
    "            report += f\"* Hit Rate (% of Up movements correctly predicted): {metrics['hit_rate']:.4f}\\n\"\n",
    "            report += f\"* False Alarm Rate: {metrics['false_alarm_rate']:.4f}\\n\"\n",
    "            report += f\"* Bias (Average overestimation): {metrics['bias']:.4f}\\n\\n\"\n",
    "        \n",
    "        # Add model comparison using Diebold-Mariano test\n",
    "        if len(self.models) > 1:\n",
    "            report += \"## Model Comparison: Diebold-Mariano Test\\n\\n\"\n",
    "            report += \"| Model 1 | Model 2 | DM Statistic | p-value | Conclusion |\\n\"\n",
    "            report += \"|---------|---------|--------------|---------|------------|\\n\"\n",
    "            \n",
    "            models = list(self.models.keys())\n",
    "            for i in range(len(models)):\n",
    "                for j in range(i+1, len(models)):\n",
    "                    dm_stat, p_value = self.diebold_mariano_test(models[i], models[j])\n",
    "                    \n",
    "                    # Determine conclusion\n",
    "                    if p_value < 0.01:\n",
    "                        significance = \"***\"\n",
    "                    elif p_value < 0.05:\n",
    "                        significance = \"**\"\n",
    "                    elif p_value < 0.1:\n",
    "                        significance = \"*\"\n",
    "                    else:\n",
    "                        significance = \"\"\n",
    "                    \n",
    "                    if np.isnan(dm_stat) or np.isnan(p_value):\n",
    "                        conclusion = \"Insufficient data\"\n",
    "                    elif p_value < 0.05:\n",
    "                        if dm_stat > 0:\n",
    "                            conclusion = f\"Model 2 is more accurate {significance}\"\n",
    "                        else:\n",
    "                            conclusion = f\"Model 1 is more accurate {significance}\"\n",
    "                    else:\n",
    "                        conclusion = \"No significant difference\"\n",
    "                    \n",
    "                    report += (\n",
    "                        f\"| {models[i]} | {models[j]} | \"\n",
    "                        f\"{dm_stat:.4f} | {p_value:.4f} | {conclusion} |\\n\"\n",
    "                    )\n",
    "            \n",
    "            report += \"\\n*Significance levels: *** = 1%, ** = 5%, * = 10%\\n\\n\"\n",
    "        \n",
    "        # Add conclusion\n",
    "        report += \"## Conclusion\\n\\n\"\n",
    "        \n",
    "        # Determine best model based on metrics\n",
    "        rmse_ranking = {model: metrics['rmse'] for model, metrics in self.results.items()}\n",
    "        best_rmse = min(rmse_ranking.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        dir_acc_ranking = {model: metrics['direction_accuracy'] for model, metrics in self.results.items()}\n",
    "        best_dir_acc = max(dir_acc_ranking.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        report += f\"Based on RMSE, the best performing model is **{best_rmse}**.\\n\\n\"\n",
    "        report += f\"Based on directional accuracy, the best performing model is **{best_dir_acc}**.\\n\\n\"\n",
    "        \n",
    "        # If plots are included and an output file is specified\n",
    "        if include_plots and output_file:\n",
    "            # Save plots to files\n",
    "            import os\n",
    "            \n",
    "            output_dir = os.path.dirname(output_file)\n",
    "            if output_dir and not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            \n",
    "            # Base file name without extension\n",
    "            base_name = os.path.splitext(output_file)[0]\n",
    "            \n",
    "            # Forecast plot\n",
    "            forecast_plot_path = f\"{base_name}_forecasts.png\"\n",
    "            fig = self.plot_forecasts()\n",
    "            fig.savefig(forecast_plot_path)\n",
    "            plt.close(fig)\n",
    "            \n",
    "            # Error distribution plot\n",
    "            error_plot_path = f\"{base_name}_errors.png\"\n",
    "            fig = self.plot_error_distribution()\n",
    "            fig.savefig(error_plot_path)\n",
    "            plt.close(fig)\n",
    "            \n",
    "            # Rolling metrics plot\n",
    "            rolling_plot_path = f\"{base_name}_rolling.png\"\n",
    "            fig = self.plot_rolling_metrics()\n",
    "            fig.savefig(rolling_plot_path)\n",
    "            plt.close(fig)\n",
    "            \n",
    "            # Add images to report\n",
    "            report += \"## Visualizations\\n\\n\"\n",
    "            report += \"### Forecast Comparison\\n\\n\"\n",
    "            report += f\"![Forecast Comparison]({os.path.basename(forecast_plot_path)})\\n\\n\"\n",
    "            report += \"### Error Distribution\\n\\n\"\n",
    "            report += f\"![Error Distribution]({os.path.basename(error_plot_path)})\\n\\n\"\n",
    "            report += \"### Rolling Metrics\\n\\n\"\n",
    "            report += f\"![Rolling Metrics]({os.path.basename(rolling_plot_path)})\\n\\n\"\n",
    "        \n",
    "        # Save report to file if specified\n",
    "        if output_file:\n",
    "            with open(output_file, 'w') as f:\n",
    "                f.write(report)\n",
    "        \n",
    "        return report\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Monthly Factor Model\n",
    "#-----------------------------------------------------------------------------\n",
    "class MonthlyFactorModel:\n",
    "    \"\"\"\n",
    "    Simplified Dynamic Factor Model for working with monthly data only.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_factors=3, max_iter=100, tol=1e-4, random_state=None):\n",
    "        \"\"\"\n",
    "        Initialize the Monthly Factor Model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_factors: int\n",
    "            Number of factors to extract\n",
    "        max_iter: int\n",
    "            Maximum number of iterations\n",
    "        tol: float\n",
    "            Tolerance for convergence\n",
    "        random_state: int or None\n",
    "            Random state for initialization\n",
    "        \"\"\"\n",
    "        self.n_factors = n_factors\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        self.loadings = None\n",
    "        self.factors = None\n",
    "        self.column_names = None\n",
    "        self.index = None\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the model using PCA instead of full DFM to avoid memory issues.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: DataFrame or ndarray\n",
    "            Data matrix (time × variables)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self\n",
    "            Fitted model instance\n",
    "        \"\"\"\n",
    "        # Convert to numpy array if DataFrame\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.column_names = X.columns\n",
    "            self.index = X.index\n",
    "            X_values = X.values\n",
    "        else:\n",
    "            self.column_names = [f\"Var{i}\" for i in range(X.shape[1])]\n",
    "            self.index = np.arange(X.shape[0])\n",
    "            X_values = X\n",
    "        \n",
    "        # Standardize data\n",
    "        X_std = (X_values - np.nanmean(X_values, axis=0)) / np.nanstd(X_values, axis=0)\n",
    "        \n",
    "        # Handle missing values\n",
    "        X_filled = np.nan_to_num(X_std, nan=0.0)\n",
    "        \n",
    "        # Use PCA to extract factors (simpler and memory efficient)\n",
    "        pca = PCA(n_components=self.n_factors, random_state=self.random_state)\n",
    "        self.factors = pca.fit_transform(X_filled)\n",
    "        self.loadings = pca.components_.T\n",
    "        \n",
    "        # Create factors DataFrame\n",
    "        self.factors_df = pd.DataFrame(\n",
    "            self.factors, \n",
    "            index=self.index,\n",
    "            columns=[f\"Factor{i+1}\" for i in range(self.n_factors)]\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X=None):\n",
    "        \"\"\"\n",
    "        Extract factors from data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: DataFrame or ndarray, optional\n",
    "            New data to transform. If None, use the data used for fitting.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        ndarray\n",
    "            Extracted factors\n",
    "        \"\"\"\n",
    "        if X is None:\n",
    "            # Return factors estimated during fitting\n",
    "            return self.factors\n",
    "        \n",
    "        # Convert to numpy array if DataFrame\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X_values = X.values\n",
    "        else:\n",
    "            X_values = X\n",
    "        \n",
    "        # Standardize data\n",
    "        X_std = (X_values - np.nanmean(X_values, axis=0)) / np.nanstd(X_values, axis=0)\n",
    "        \n",
    "        # Handle missing values\n",
    "        X_filled = np.nan_to_num(X_std, nan=0.0)\n",
    "        \n",
    "        # Project data onto loadings\n",
    "        factors = X_filled @ self.loadings\n",
    "        \n",
    "        return factors\n",
    "    \n",
    "    def get_factor_loadings(self):\n",
    "        \"\"\"\n",
    "        Get factor loadings as a DataFrame.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            Factor loadings\n",
    "        \"\"\"\n",
    "        if self.loadings is None:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        factor_names = [f\"Factor{i+1}\" for i in range(self.n_factors)]\n",
    "        return pd.DataFrame(self.loadings, index=self.column_names, columns=factor_names)\n",
    "    \n",
    "    def get_factors(self):\n",
    "        \"\"\"\n",
    "        Get extracted factors as a DataFrame.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            Extracted factors\n",
    "        \"\"\"\n",
    "        if self.factors is None:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        factor_names = [f\"Factor{i+1}\" for i in range(self.n_factors)]\n",
    "        return pd.DataFrame(self.factors, index=self.index, columns=factor_names)\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Simplified MIDAS Regressor\n",
    "#-----------------------------------------------------------------------------\n",
    "class SimplifiedMIDASRegressor:\n",
    "    \"\"\"\n",
    "    Simplified MIDAS regression for mixed-frequency time series.\n",
    "    \"\"\"\n",
    "    def __init__(self, weight_function='exponential_almon', max_lags=12,\n",
    "                n_weight_params=2, ar_lags=4, regularization=0.0,\n",
    "                max_iter=1000, tol=1e-6, random_state=None):\n",
    "        \"\"\"\n",
    "        Initialize simplified MIDAS regressor.\n",
    "        \"\"\"\n",
    "        self.max_lags = max_lags\n",
    "        self.n_weight_params = n_weight_params\n",
    "        self.ar_lags = ar_lags\n",
    "        self.regularization = regularization\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Set weighting function\n",
    "        if weight_function == 'exponential_almon':\n",
    "            self.weight_function = self._exponential_almon_weights\n",
    "        elif callable(weight_function):\n",
    "            self.weight_function = weight_function\n",
    "        else:\n",
    "            raise ValueError(\"weight_function must be 'exponential_almon' or a callable\")\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.weight_params = None\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.fit_intercept = True\n",
    "        \n",
    "        # Initialize random number generator\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "    \n",
    "    def _exponential_almon_weights(self, lag, params):\n",
    "        \"\"\"\n",
    "        Exponential Almon lag polynomial weighting function.\n",
    "        \"\"\"\n",
    "        if len(params) < 2:\n",
    "            # Need at least two parameters\n",
    "            params = np.array([params[0], 0.0])\n",
    "        \n",
    "        # Normalize lags to [0, 1]\n",
    "        x = lag / (self.max_lags - 1) if self.max_lags > 1 else 0\n",
    "        \n",
    "        # Calculate weights\n",
    "        exponent = params[0] * x + params[1] * x**2\n",
    "        weights = np.exp(exponent)\n",
    "        \n",
    "        # Normalize weights to sum to 1\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def _aggregate_high_frequency(self, X_hf, weight_params):\n",
    "        \"\"\"\n",
    "        Aggregate high-frequency variables using weighting function with explicit type handling.\n",
    "        \"\"\"\n",
    "        n_samples = X_hf[0].shape[0]\n",
    "        n_hf_vars = len(X_hf)\n",
    "        \n",
    "        # Initialize aggregated variables\n",
    "        X_aggregated = np.zeros((n_samples, n_hf_vars), dtype=np.float64)\n",
    "        \n",
    "        # Calculate weights\n",
    "        lags = np.arange(self.max_lags)\n",
    "        weights = self.weight_function(lags, weight_params)\n",
    "        weights = np.asarray(weights, dtype=np.float64)\n",
    "        \n",
    "        # Apply weights to each high-frequency variable\n",
    "        for i, X_var in enumerate(X_hf):\n",
    "            # Ensure X_var is float64\n",
    "            X_var_float = np.asarray(X_var, dtype=np.float64)\n",
    "            \n",
    "            # Weighted sum across lags\n",
    "            X_aggregated[:, i] = np.sum(X_var_float * weights, axis=1)\n",
    "        \n",
    "        return X_aggregated\n",
    "    \n",
    "    def _objective_function(self, weight_params, X_hf, X_ar, y):\n",
    "        \"\"\"\n",
    "        Objective function for MIDAS parameter optimization with NaN tracing.\n",
    "        \"\"\"\n",
    "        # Add debugging for arrays\n",
    "        for i, X in enumerate(X_hf):\n",
    "            if np.isnan(X).any():\n",
    "                print(f\"WARNING: X_hf[{i}] has {np.isnan(X).sum()} NaNs in objective function\")\n",
    "        \n",
    "        if X_ar is not None and np.isnan(X_ar).any():\n",
    "            print(f\"WARNING: X_ar has {np.isnan(X_ar).sum()} NaNs in objective function\")\n",
    "        \n",
    "        X_hf_processed = []\n",
    "        for X in X_hf:\n",
    "            X_hf_processed.append(np.asarray(X, dtype=np.float64))\n",
    "        \n",
    "        if X_ar is not None:\n",
    "            X_ar = np.asarray(X_ar, dtype=np.float64)\n",
    "        \n",
    "        # Aggregate high-frequency variables with type-safe arrays\n",
    "        X_midas = self._aggregate_high_frequency(X_hf_processed, weight_params)\n",
    "        \n",
    "        # Continue with regular processing...\n",
    "        if X_ar is not None:\n",
    "            X = np.column_stack([X_ar, X_midas])\n",
    "        else:\n",
    "            X = X_midas\n",
    "        \n",
    "        # Add intercept\n",
    "        if self.fit_intercept:\n",
    "            X = np.column_stack([np.ones(X.shape[0]), X])\n",
    "        \n",
    "        # Regularized regression\n",
    "        try:\n",
    "            reg_term = self.regularization * np.eye(X.shape[1])\n",
    "            XtX = X.T @ X + reg_term\n",
    "            Xty = X.T @ y\n",
    "            coef = np.linalg.solve(XtX, Xty)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # If direct solve fails, use pseudoinverse\n",
    "            coef = np.linalg.pinv(XtX) @ Xty\n",
    "        \n",
    "        # Calculate predictions\n",
    "        y_pred = X @ coef\n",
    "        \n",
    "        # Calculate MSE\n",
    "        mse = np.mean((y - y_pred) ** 2)\n",
    "        return mse\n",
    "    \n",
    "    def fit(self, X_hf, y, X_ar=None):\n",
    "        \"\"\"\n",
    "        Fit MIDAS regression model with proper type handling.\n",
    "        \"\"\"\n",
    "        # Convert target to numeric array\n",
    "        if isinstance(y, pd.Series) or isinstance(y, pd.DataFrame):\n",
    "            y = y.values.flatten().astype(np.float64)\n",
    "        else:\n",
    "            y = np.asarray(y, dtype=np.float64).flatten()\n",
    "        \n",
    "        # Process high-frequency variables with explicit type conversion\n",
    "        X_hf_arrays = []\n",
    "        for X in X_hf:\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X_hf_arrays.append(X.values.astype(np.float64))\n",
    "            else:\n",
    "                X_hf_arrays.append(np.asarray(X, dtype=np.float64))\n",
    "        \n",
    "        # Process autoregressive features with explicit type conversion\n",
    "        if X_ar is not None:\n",
    "            if isinstance(X_ar, pd.DataFrame):\n",
    "                X_ar = X_ar.values.astype(np.float64)\n",
    "            else:\n",
    "                X_ar = np.asarray(X_ar, dtype=np.float64)\n",
    "        \n",
    "        # Initialize weight parameters\n",
    "        init_params = self.rng.normal(0, 0.01, self.n_weight_params)\n",
    "        \n",
    "        # Use bounded optimization\n",
    "        bounds = [(-5, 5)] * self.n_weight_params\n",
    "        \n",
    "        # Try different optimization methods\n",
    "        optimization_methods = ['BFGS', 'Nelder-Mead', 'Powell']\n",
    "        \n",
    "        for method in optimization_methods:\n",
    "            try:\n",
    "                # Use a method that doesn't rely on SVD for gradients\n",
    "                result = minimize(\n",
    "                    self._objective_function,\n",
    "                    init_params,\n",
    "                    args=(X_hf_arrays, X_ar, y),\n",
    "                    method=method,\n",
    "                    options={'maxiter': self.max_iter, 'gtol': self.tol}\n",
    "                )\n",
    "                \n",
    "                if result.success:\n",
    "                    self.weight_params = result.x\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Optimization with {method} failed: {e}.\")\n",
    "        \n",
    "        # If all optimization methods failed, use a simple exponential decay\n",
    "        if not hasattr(self, 'weight_params') or self.weight_params is None:\n",
    "            print(\"All optimization methods failed. Using default exponential decay weights.\")\n",
    "            self.weight_params = np.array([-1.0, -0.5])  # Simple exponential decay\n",
    "        \n",
    "        # Calculate final weights\n",
    "        lags = np.arange(self.max_lags)\n",
    "        self.weights_ = self.weight_function(lags, self.weight_params)\n",
    "        \n",
    "        # Aggregate high-frequency variables with optimized weights\n",
    "        X_midas = self._aggregate_high_frequency(X_hf_arrays, self.weight_params)\n",
    "        \n",
    "        # Combine with autoregressive features\n",
    "        if X_ar is not None:\n",
    "            X = np.column_stack([X_ar, X_midas])\n",
    "        else:\n",
    "            X = X_midas\n",
    "        \n",
    "        # Ensure final regression matrix is float64\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        \n",
    "        # Fit Ridge regression\n",
    "        ridge = Ridge(alpha=max(self.regularization, 1e-5), fit_intercept=self.fit_intercept)\n",
    "        ridge.fit(X, y)\n",
    "        \n",
    "        # Store coefficients\n",
    "        if self.fit_intercept:\n",
    "            self.intercept_ = ridge.intercept_\n",
    "            self.coef_ = ridge.coef_\n",
    "        else:\n",
    "            self.intercept_ = 0.0\n",
    "            self.coef_ = ridge.coef_\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_hf, X_ar=None):\n",
    "        \"\"\"\n",
    "        Make predictions with fitted model.\n",
    "        \"\"\"\n",
    "        if self.weight_params is None:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        # Convert inputs to numpy arrays if needed\n",
    "        X_hf_arrays = []\n",
    "        for X in X_hf:\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X_hf_arrays.append(X.values)\n",
    "            else:\n",
    "                X_hf_arrays.append(np.asarray(X))\n",
    "        \n",
    "        if X_ar is not None:\n",
    "            if isinstance(X_ar, pd.DataFrame):\n",
    "                X_ar = X_ar.values\n",
    "            else:\n",
    "                X_ar = np.asarray(X_ar)\n",
    "        \n",
    "        # Aggregate high-frequency variables with fitted weights\n",
    "        X_midas = self._aggregate_high_frequency(X_hf_arrays, self.weight_params)\n",
    "        \n",
    "        # Combine with autoregressive features\n",
    "        if X_ar is not None:\n",
    "            X = np.column_stack([X_ar, X_midas])\n",
    "        else:\n",
    "            X = X_midas\n",
    "        \n",
    "        # Make predictions\n",
    "        if self.fit_intercept:\n",
    "            y_pred = self.intercept_ + X @ self.coef_\n",
    "        else:\n",
    "            y_pred = X @ self.coef_\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def get_midas_weights(self):\n",
    "        \"\"\"\n",
    "        Get the MIDAS weighting function parameters and weights.\n",
    "        \"\"\"\n",
    "        if self.weight_params is None:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        lags = np.arange(self.max_lags)\n",
    "        weights = self.weight_function(lags, self.weight_params)\n",
    "        \n",
    "        return {\n",
    "            'parameters': self.weight_params,\n",
    "            'weights': weights,\n",
    "            'lags': lags\n",
    "        }\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Monthly GDP Predictor\n",
    "#-----------------------------------------------------------------------------\n",
    "class MonthlyGDPPredictor:\n",
    "    \"\"\"\n",
    "    Simplified GDP prediction system that only uses monthly data.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                monthly_factors=3,\n",
    "                gdp_ar_lags=4,\n",
    "                use_midas=True, \n",
    "                midas_max_lags=6, \n",
    "                random_state=None):\n",
    "        \"\"\"\n",
    "        Initialize the GDP prediction system.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        monthly_factors: int\n",
    "            Number of factors to extract from monthly data\n",
    "        gdp_ar_lags: int\n",
    "            Autoregressive lags for GDP\n",
    "        use_midas: bool\n",
    "            Whether to use MIDAS for the final GDP prediction\n",
    "        midas_max_lags: int\n",
    "            Maximum number of lags for MIDAS\n",
    "        random_state: int or None\n",
    "            Random state for reproducibility\n",
    "        \"\"\"\n",
    "        self.monthly_factors = monthly_factors\n",
    "        self.gdp_ar_lags = gdp_ar_lags\n",
    "        self.use_midas = use_midas\n",
    "        self.midas_max_lags = midas_max_lags\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Initialize models\n",
    "        self.monthly_model = MonthlyFactorModel(\n",
    "            n_factors=monthly_factors,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Initialize GDP model based on configuration\n",
    "        if use_midas:\n",
    "            self.gdp_model = SimplifiedMIDASRegressor(\n",
    "                weight_function='exponential_almon',\n",
    "                max_lags=midas_max_lags,\n",
    "                n_weight_params=2,\n",
    "                ar_lags=gdp_ar_lags,\n",
    "                regularization=0.01,\n",
    "                random_state=random_state\n",
    "            )\n",
    "        else:\n",
    "            # Use Ridge regression as fallback\n",
    "            self.gdp_model = Ridge(alpha=0.01)\n",
    "        \n",
    "        # Storage for fitted factors\n",
    "        self.monthly_factors_df = None\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit_monthly_model(self, monthly_df):\n",
    "        \"\"\"\n",
    "        Fit the monthly factor model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        monthly_df: pandas.DataFrame\n",
    "            Monthly data with technical indicators\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            Extracted monthly factors\n",
    "        \"\"\"\n",
    "        print(f\"Fitting monthly model with {monthly_df.shape[1]} features\")\n",
    "        self.monthly_model.fit(monthly_df)\n",
    "        self.monthly_factors_df = self.monthly_model.get_factors()\n",
    "        \n",
    "        # Add debugging\n",
    "        trace_nans(\"Monthly data before factor extraction\", monthly_df)\n",
    "        \n",
    "        print(f\"Fitting monthly model with {monthly_df.shape[1]} features\")\n",
    "        self.monthly_model.fit(monthly_df)\n",
    "        self.monthly_factors_df = self.monthly_model.get_factors()\n",
    "        \n",
    "        # Add debugging\n",
    "        trace_nans(\"Monthly factors after extraction\", self.monthly_factors_df)\n",
    "        \n",
    "        print(f\"Extracted {self.monthly_factors_df.shape[1]} monthly factors\")\n",
    "        return self.monthly_factors_df\n",
    "    \n",
    "    def fit_gdp_model(self, gdp_series, monthly_factors, use_ar=True):\n",
    "        \"\"\"\n",
    "        Fit the GDP prediction model without using synthetic data.\n",
    "        \"\"\"\n",
    "        # Align indices\n",
    "        common_index = gdp_series.index.intersection(monthly_factors.index)\n",
    "        y = gdp_series.loc[common_index]\n",
    "        X_monthly = monthly_factors.loc[common_index]\n",
    "        \n",
    "        if self.use_midas:\n",
    "            # Prepare data for MIDAS model\n",
    "            X_lags = []\n",
    "            for col in X_monthly.columns:\n",
    "                # Create lag matrix for each factor\n",
    "                lag_matrix = pd.DataFrame(index=X_monthly.index)\n",
    "                for lag in range(self.midas_max_lags):\n",
    "                    lag_matrix[f\"{col}_lag{lag}\"] = X_monthly[col].shift(lag)\n",
    "                X_lags.append(lag_matrix.values)\n",
    "            \n",
    "            # Prepare autoregressive features\n",
    "            if use_ar and self.gdp_ar_lags > 0:\n",
    "                X_ar = pd.DataFrame(index=y.index)\n",
    "                for lag in range(1, self.gdp_ar_lags + 1):\n",
    "                    X_ar[f\"GDP_lag{lag}\"] = y.shift(lag)\n",
    "                \n",
    "                # Determine maximum lag period\n",
    "                max_lag = max(self.midas_max_lags, self.gdp_ar_lags)\n",
    "                \n",
    "                # Skip the first 'max_lag' periods to eliminate all NaNs\n",
    "                # This is the key change - trim data rather than fill NaNs\n",
    "                valid_indices = X_ar.index[max_lag:]\n",
    "                \n",
    "                # Filter all data to these valid indices\n",
    "                y_valid = y.loc[valid_indices]\n",
    "                X_ar_valid = X_ar.loc[valid_indices]\n",
    "                X_lags_valid = [X[max_lag:] for X in X_lags]\n",
    "                \n",
    "                print(f\"Fitting MIDAS model with {len(X_lags_valid)} monthly factors, {X_ar_valid.shape[1]} GDP lags\")\n",
    "                print(f\"Using {len(y_valid)} observations after trimming {max_lag} periods with lag-induced NaNs\")\n",
    "                \n",
    "                self.gdp_model.fit(X_lags_valid, y_valid, X_ar_valid)\n",
    "            else:\n",
    "                # No autoregressive features, but still trim for factor lags\n",
    "                valid_indices = X_monthly.index[self.midas_max_lags:]\n",
    "                y_valid = y.loc[valid_indices]\n",
    "                X_lags_valid = [X[self.midas_max_lags:] for X in X_lags]\n",
    "                \n",
    "                print(f\"Fitting MIDAS model with {len(X_lags_valid)} monthly factors\")\n",
    "                print(f\"Using {len(y_valid)} observations after trimming {self.midas_max_lags} periods with lag-induced NaNs\")\n",
    "                \n",
    "                self.gdp_model.fit(X_lags_valid, y_valid)\n",
    "        else:\n",
    "            # Using standard Ridge regression\n",
    "            # Handle autoregressive features\n",
    "            if use_ar and self.gdp_ar_lags > 0:\n",
    "                X_ar = pd.DataFrame(index=y.index)\n",
    "                for lag in range(1, self.gdp_ar_lags + 1):\n",
    "                    X_ar[f\"GDP_lag{lag}\"] = y.shift(lag)\n",
    "                \n",
    "                # Combine with monthly factors\n",
    "                X_combined = pd.concat([X_monthly, X_ar], axis=1)\n",
    "                \n",
    "                # Skip the first 'gdp_ar_lags' periods to eliminate all NaNs\n",
    "                valid_indices = X_combined.index[self.gdp_ar_lags:]\n",
    "                X_combined_valid = X_combined.loc[valid_indices]\n",
    "                y_valid = y.loc[valid_indices]\n",
    "                \n",
    "                print(f\"Fitting Ridge regression with {X_combined_valid.shape[1]} features\")\n",
    "                print(f\"Using {len(y_valid)} observations after trimming {self.gdp_ar_lags} periods with lag-induced NaNs\")\n",
    "                \n",
    "                self.gdp_model.fit(X_combined_valid, y_valid)\n",
    "            else:\n",
    "                # No lagged features, use data as is\n",
    "                self.gdp_model.fit(X_monthly, y)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def fit(self, monthly_df, gdp_series, align_dates=True, use_ar=True):\n",
    "        \"\"\"\n",
    "        Fit the complete model with auto-detection of valid date range.\n",
    "        \"\"\"\n",
    "        # 1. Extract monthly factors\n",
    "        monthly_factors = self.fit_monthly_model(monthly_df)\n",
    "        \n",
    "        # 2. Align monthly factors to quarterly GDP dates\n",
    "        if align_dates:\n",
    "            # Find quarterly dates\n",
    "            quarterly_dates = gdp_series.index\n",
    "            \n",
    "            # Create aligned DataFrame with the same columns as monthly_factors\n",
    "            aligned_monthly_factors = pd.DataFrame(\n",
    "                index=quarterly_dates, \n",
    "                columns=monthly_factors.columns\n",
    "            )\n",
    "            \n",
    "            # Align monthly factors to quarterly dates\n",
    "            for quarterly_date in quarterly_dates:\n",
    "                monthly_data = monthly_factors[monthly_factors.index <= quarterly_date]\n",
    "                if not monthly_data.empty:\n",
    "                    # Get the last row of data for each column\n",
    "                    for col in monthly_factors.columns:\n",
    "                        aligned_monthly_factors.loc[quarterly_date, col] = monthly_data.iloc[-1][col]\n",
    "            \n",
    "            # Check for NaNs after alignment to auto-detect valid date range\n",
    "            nan_rows = aligned_monthly_factors.isna().any(axis=1)\n",
    "            if nan_rows.any():\n",
    "                # Find first date where all data is available\n",
    "                first_valid_date = aligned_monthly_factors[~nan_rows].index[0]\n",
    "                print(f\"Auto-detected start date: {first_valid_date} (first quarter with complete data)\")\n",
    "                \n",
    "                # Filter to only use data from the valid range\n",
    "                aligned_monthly_factors = aligned_monthly_factors.loc[first_valid_date:]\n",
    "                gdp_series_valid = gdp_series.loc[first_valid_date:]\n",
    "                \n",
    "                print(f\"Using {len(aligned_monthly_factors)} quarters of data\")\n",
    "            else:\n",
    "                gdp_series_valid = gdp_series\n",
    "        else:\n",
    "            aligned_monthly_factors = monthly_factors\n",
    "            gdp_series_valid = gdp_series\n",
    "        \n",
    "        # 3. Fit GDP model with monthly factors\n",
    "        self.fit_gdp_model(gdp_series_valid, aligned_monthly_factors, use_ar)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, monthly_df=None, gdp_history=None, predict_date=None):\n",
    "        \"\"\"\n",
    "        Generate GDP predictions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        monthly_df: pandas.DataFrame, optional\n",
    "            Monthly data for prediction period\n",
    "        gdp_history: pandas.Series, optional\n",
    "            Historical GDP data for autoregressive features\n",
    "        predict_date: datetime or str, optional\n",
    "            Date for which to generate prediction\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.Series\n",
    "            GDP growth predictions\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model has not been fitted yet. Call fit() first.\")\n",
    "        \n",
    "        # 1. Process monthly data and extract factors\n",
    "        if monthly_df is not None:\n",
    "            # Transform monthly data to factors\n",
    "            monthly_factors = pd.DataFrame(\n",
    "                self.monthly_model.transform(monthly_df),\n",
    "                index=monthly_df.index,\n",
    "                columns=[f\"MonthlyFactor{i+1}\" for i in range(self.monthly_factors)]\n",
    "            )\n",
    "        else:\n",
    "            # Use existing monthly factors\n",
    "            monthly_factors = self.monthly_factors_df\n",
    "        \n",
    "        # 2. Filter data if needed\n",
    "        if predict_date is not None:\n",
    "            # Filter data up to predict_date\n",
    "            monthly_factors = monthly_factors[monthly_factors.index <= predict_date]\n",
    "        \n",
    "        # 3. Use monthly factors to predict GDP\n",
    "        if self.use_midas:\n",
    "            # Prepare data for MIDAS model\n",
    "            # We need to create lag structure for monthly factors\n",
    "            X_lags = []\n",
    "            for col in monthly_factors.columns:\n",
    "                # Create lag matrix for each factor\n",
    "                lag_matrix = pd.DataFrame(index=monthly_factors.index)\n",
    "                for lag in range(self.midas_max_lags):\n",
    "                    lag_matrix[f\"{col}_lag{lag}\"] = monthly_factors[col].shift(lag)\n",
    "                # Forward fill any NaNs at the beginning\n",
    "                lag_matrix = lag_matrix.fillna(method='ffill')\n",
    "                X_lags.append(lag_matrix.values)\n",
    "            \n",
    "            # Prepare autoregressive features if needed\n",
    "            if gdp_history is not None and self.gdp_ar_lags > 0:\n",
    "                X_ar = pd.DataFrame(index=monthly_factors.index)\n",
    "                for lag in range(1, self.gdp_ar_lags + 1):\n",
    "                    X_ar[f\"GDP_lag{lag}\"] = gdp_history.shift(lag)\n",
    "                # Forward fill any NaNs at the beginning\n",
    "                X_ar = X_ar.fillna(method='ffill')\n",
    "                \n",
    "                # Make prediction\n",
    "                gdp_pred = self.gdp_model.predict(X_lags, X_ar)\n",
    "            else:\n",
    "                # No autoregressive features\n",
    "                gdp_pred = self.gdp_model.predict(X_lags)\n",
    "        else:\n",
    "            # Using standard Ridge regression\n",
    "            # Prepare autoregressive features if needed\n",
    "            if gdp_history is not None and self.gdp_ar_lags > 0:\n",
    "                X_ar = pd.DataFrame(index=monthly_factors.index)\n",
    "                for lag in range(1, self.gdp_ar_lags + 1):\n",
    "                    X_ar[f\"GDP_lag{lag}\"] = gdp_history.shift(lag)\n",
    "                # Combine with monthly factors\n",
    "                X_combined = pd.concat([monthly_factors, X_ar], axis=1)\n",
    "            else:\n",
    "                X_combined = monthly_factors\n",
    "            \n",
    "            # Handle NaNs\n",
    "            X_combined = X_combined.fillna(method='ffill')\n",
    "            \n",
    "            # Make prediction\n",
    "            gdp_pred = self.gdp_model.predict(X_combined)\n",
    "        \n",
    "        # Convert to pandas Series\n",
    "        gdp_predictions = pd.Series(gdp_pred, index=monthly_factors.index, name=\"GDP_prediction\")\n",
    "        \n",
    "        return gdp_predictions\n",
    "    \n",
    "    def get_factor_loadings(self):\n",
    "        \"\"\"\n",
    "        Get factor loadings.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of factor loadings\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        loadings = {\n",
    "            'monthly': self.monthly_model.get_factor_loadings()\n",
    "        }\n",
    "        \n",
    "        return loadings\n",
    "    \n",
    "    def get_factors(self):\n",
    "        \"\"\"\n",
    "        Get extracted factors.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of factors\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        factors = {\n",
    "            'monthly': self.monthly_factors_df\n",
    "        }\n",
    "        \n",
    "        return factors\n",
    "    \n",
    "    def get_midas_weights(self):\n",
    "        \"\"\"\n",
    "        Get MIDAS weights if using MIDAS model.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of MIDAS weights\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        if not self.use_midas:\n",
    "            return None\n",
    "        \n",
    "        return self.gdp_model.get_midas_weights()\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Main Workflow Function\n",
    "#-----------------------------------------------------------------------------\n",
    "def run_gdp_forecast_workflow_monthly_only(\n",
    "    data_folder,\n",
    "    output_folder='./output',\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    train_test_split=0.8,\n",
    "    use_midas=True,\n",
    "    monthly_factors=3,\n",
    "    gdp_ar_lags=4,\n",
    "    random_state=42,\n",
    "    save_models=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a simplified GDP forecasting workflow using only monthly data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_folder: str\n",
    "        Path to the data folder\n",
    "    output_folder: str\n",
    "        Path to the output folder\n",
    "    start_date: str or None\n",
    "        Start date for analysis\n",
    "    end_date: str or None\n",
    "        End date for analysis\n",
    "    train_test_split: float\n",
    "        Proportion of data to use for training\n",
    "    use_midas: bool\n",
    "        Whether to use MIDAS for GDP prediction\n",
    "    monthly_factors: int\n",
    "        Number of factors to extract from monthly data\n",
    "    gdp_ar_lags: int\n",
    "        Number of autoregressive lags for GDP\n",
    "    random_state: int\n",
    "        Random seed for reproducibility\n",
    "    save_models: bool\n",
    "        Whether to save the models\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (evaluator, models, preprocessor)\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Set up logging\n",
    "    log_file = os.path.join(output_folder, 'workflow_log.txt')\n",
    "    def log(message):\n",
    "        \"\"\"Log message to file and print to console.\"\"\"\n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(f\"{pd.Timestamp.now()}: {message}\\n\")\n",
    "        print(message)\n",
    "    \n",
    "    log(\"=\" * 80)\n",
    "    log(f\"Starting Monthly-Only GDP Forecasting Workflow at {pd.Timestamp.now()}\")\n",
    "    log(\"=\" * 80)\n",
    "    \n",
    "    # 1. Configuration - only monthly and quarterly data\n",
    "    log(\"\\n1. Setting up configuration...\")\n",
    "    \n",
    "    # Monthly data configuration\n",
    "    monthly_config = {\n",
    "        'monthly': {\n",
    "            'files': {\n",
    "                'CPI_mon_monthly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1955-01-01'\n",
    "                },\n",
    "                'Unemployment_monthly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'diff']},\n",
    "                    'start_date': '1948-01-01'\n",
    "                },\n",
    "                'InterestRate_monthly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'diff']},\n",
    "                    'start_date': '1954-01-01'\n",
    "                },\n",
    "                'HousingStarts_monthly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1959-01-01'\n",
    "                },\n",
    "                'Heavy_Truck_Sales.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1967-01-01'\n",
    "                },\n",
    "                'Manufacturing_Production_Motor_and_Vehicle_Parts.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1972-01-01'\n",
    "                },\n",
    "                'Consumer_Confidence.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'diff']},\n",
    "                    'start_date': '1960-01-01'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Quarterly data configuration\n",
    "    quarterly_config = {\n",
    "        'quarterly': {\n",
    "            'files': {\n",
    "                'GDP_quaterly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1947-01-01'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Combine configurations\n",
    "    data_config = {}\n",
    "    data_config.update(monthly_config)\n",
    "    data_config.update(quarterly_config)\n",
    "    \n",
    "    log(f\"Configuration set up with {len(monthly_config['monthly']['files'])} monthly files, \" +\n",
    "        f\"{len(quarterly_config['quarterly']['files'])} quarterly files\")\n",
    "    \n",
    "    # 2. Data Preprocessing\n",
    "    log(\"\\n2. Data Preprocessing...\")\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = MultiFrequencyPreprocessor(data_folder)\n",
    "    preprocessor.set_config(data_config)\n",
    "    \n",
    "    # Set date range if provided\n",
    "    if start_date is not None:\n",
    "        preprocessor.set_date_range(start_date=start_date)\n",
    "    if end_date is not None:\n",
    "        preprocessor.set_date_range(end_date=end_date)\n",
    "    \n",
    "    # Process data for monthly and quarterly only\n",
    "    monthly_df = preprocessor.process_frequency_data('monthly')\n",
    "    trace_nans(\"Raw monthly data\", monthly_df)\n",
    "    quarterly_df = preprocessor.process_frequency_data('quarterly')\n",
    "    trace_nans(\"Raw quarterly data\", quarterly_df)\n",
    "    \n",
    "    log(f\"Processed data: monthly={monthly_df.shape}, quarterly={quarterly_df.shape}\")\n",
    "    \n",
    "    # Plot data overview\n",
    "    try:\n",
    "        fig = preprocessor.plot_data_overview()\n",
    "        fig.savefig(os.path.join(output_folder, 'data_overview.png'))\n",
    "        plt.close(fig)\n",
    "        log(f\"Data overview saved to {os.path.join(output_folder, 'data_overview.png')}\")\n",
    "    except Exception as e:\n",
    "        log(f\"Warning: Could not create data overview plot: {e}\")\n",
    "    \n",
    "    # 3. Technical Indicators\n",
    "    log(\"\\n3. Calculating Technical Indicators...\")\n",
    "    \n",
    "    # Initialize technical indicators calculator\n",
    "    tech_indicators = MultiFrequencyTechnicalIndicators()\n",
    "    \n",
    "    # Calculate technical indicators for monthly and quarterly\n",
    "    monthly_indicators = tech_indicators.apply_indicators(monthly_df, frequency='monthly')\n",
    "    trace_nans(\"Monthly data before indicators\", monthly_df)\n",
    "    quarterly_indicators = tech_indicators.apply_indicators(quarterly_df, frequency='quarterly')\n",
    "    trace_nans(\"Monthly data after indicators\", monthly_indicators)\n",
    "    \n",
    "    log(f\"Calculated technical indicators: monthly={monthly_indicators.shape}, \" +\n",
    "        f\"quarterly={quarterly_indicators.shape}\")\n",
    "    \n",
    "    # 4. Data Alignment for Model\n",
    "    log(\"\\n4. Aligning Data for Model...\")\n",
    "    \n",
    "    # Get GDP target series\n",
    "    gdp_target = quarterly_df['GDP_quaterly_Value_pct_change']\n",
    "    \n",
    "    # Align monthly data to quarterly dates\n",
    "    monthly_to_quarterly = preprocessor.align_to_dates(monthly_indicators, gdp_target.index, method='last')\n",
    "    trace_nans(\"Monthly data after alignment to quarterly\", monthly_to_quarterly)\n",
    "    log(f\"Aligned monthly to quarterly: {monthly_to_quarterly.shape}\")\n",
    "    \n",
    "    # 5. Train-Test Split\n",
    "    log(\"\\n5. Creating Train-Test Split...\")\n",
    "    \n",
    "    # Determine split point\n",
    "    n_quarters = len(gdp_target)\n",
    "    n_train = int(n_quarters * train_test_split)\n",
    "    split_date = gdp_target.index[n_train]\n",
    "    \n",
    "    # Split GDP data\n",
    "    train_gdp = gdp_target.iloc[:n_train]\n",
    "    test_gdp = gdp_target.iloc[n_train:]\n",
    "    \n",
    "    # Split aligned data\n",
    "    train_monthly_aligned = monthly_to_quarterly.loc[train_gdp.index]\n",
    "    test_monthly_aligned = monthly_to_quarterly.loc[test_gdp.index]\n",
    "\n",
    "    trace_nans(\"Training monthly aligned data\", train_monthly_aligned)\n",
    "    trace_nans(\"Training GDP data\", train_gdp)\n",
    "    \n",
    "    log(f\"Train-test split at {split_date}: train={len(train_gdp)}, test={len(test_gdp)}\")\n",
    "    \n",
    "    # 6. Model Building\n",
    "    log(\"\\n6. Building Models...\")\n",
    "    \n",
    "    # Initialize models dictionary\n",
    "    models = {}\n",
    "    \n",
    "    # 6.1. Monthly-to-Quarterly Model with MIDAS option\n",
    "    log(\"Building Monthly-to-Quarterly Model...\")\n",
    "    try:\n",
    "        # Initialize predictor\n",
    "        monthly_model = MonthlyGDPPredictor(\n",
    "            monthly_factors=monthly_factors,\n",
    "            gdp_ar_lags=gdp_ar_lags,\n",
    "            use_midas=use_midas,\n",
    "            midas_max_lags=6,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Fit the model with monthly data\n",
    "        monthly_model.fit(\n",
    "            monthly_df=monthly_indicators,\n",
    "            gdp_series=train_gdp,\n",
    "            align_dates=True,\n",
    "            use_ar=True\n",
    "        )\n",
    "        \n",
    "        # Store in models dictionary\n",
    "        model_name = \"Monthly_MIDAS\" if use_midas else \"Monthly_Direct\"\n",
    "        models[model_name] = monthly_model\n",
    "        \n",
    "        log(f\"{model_name} Model successfully built\")\n",
    "        \n",
    "        # Save model if requested\n",
    "        if save_models:\n",
    "            model_path = os.path.join(output_folder, f'{model_name.lower()}_model.pkl')\n",
    "            with open(model_path, 'wb') as f:\n",
    "                pickle.dump(monthly_model, f)\n",
    "            log(f\"{model_name} Model saved to {model_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        log(f\"Error building Monthly-to-Quarterly Model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # 6.2. Baseline Models\n",
    "    log(\"Building Baseline Models...\")\n",
    "    \n",
    "    # 6.2.1. AR Model (autoregressive)\n",
    "    try:\n",
    "        # Create lag features\n",
    "        X_ar = pd.DataFrame(index=train_gdp.index)\n",
    "        for lag in range(1, gdp_ar_lags + 1):\n",
    "            X_ar[f'lag_{lag}'] = train_gdp.shift(lag)\n",
    "        \n",
    "        # Drop rows with NaN values\n",
    "        valid_rows = ~X_ar.isna().any(axis=1)\n",
    "        X_ar_valid = X_ar[valid_rows]\n",
    "        y_ar_valid = train_gdp[valid_rows]\n",
    "        \n",
    "        # Fit AR model\n",
    "        ar_model = Ridge(alpha=0.1, random_state=random_state)\n",
    "        ar_model.fit(X_ar_valid, y_ar_valid)\n",
    "        \n",
    "        # Store model\n",
    "        models['AR_Baseline'] = ar_model\n",
    "        log(\"AR Baseline Model successfully built\")\n",
    "        \n",
    "        # Save lag columns for prediction\n",
    "        models['AR_lag_columns'] = X_ar.columns.tolist()\n",
    "    except Exception as e:\n",
    "        log(f\"Error building AR Baseline Model: {e}\")\n",
    "    \n",
    "    # 6.2.2. MA Model (moving average of previous quarters)\n",
    "    try:\n",
    "        # Create different MA versions\n",
    "        ma_windows = [4]  # 1-year moving average\n",
    "        for window in ma_windows:\n",
    "            ma_model = {'window': window}\n",
    "            models[f'MA_{window}_Baseline'] = ma_model\n",
    "            log(f\"MA-{window} Baseline Model defined\")\n",
    "    except Exception as e:\n",
    "        log(f\"Error defining MA Baseline Models: {e}\")\n",
    "    \n",
    "    # 7. Model Evaluation\n",
    "    log(\"\\n7. Evaluating Models...\")\n",
    "    \n",
    "    # Create evaluator\n",
    "    evaluator = GDPForecastEvaluator()\n",
    "    \n",
    "    # Set actual values\n",
    "    evaluator.add_model('Actual', test_gdp, test_gdp)\n",
    "    \n",
    "    # Generate predictions for each model\n",
    "    for model_name, model in models.items():\n",
    "        try:\n",
    "            if model_name in [\"Monthly_MIDAS\", \"Monthly_Direct\"]:\n",
    "                # Generate predictions using the monthly model\n",
    "                predictions = model.predict(\n",
    "                    monthly_df=monthly_indicators,\n",
    "                    gdp_history=gdp_target,\n",
    "                    predict_date=None  # Use all data\n",
    "                )\n",
    "                \n",
    "                # Filter to test period\n",
    "                test_predictions = predictions.loc[test_gdp.index]\n",
    "                evaluator.add_model(model_name, test_predictions)\n",
    "                log(f\"Generated predictions for {model_name}: {len(test_predictions)} quarters\")\n",
    "            \n",
    "            elif model_name == 'AR_Baseline':\n",
    "                # Create features for test period\n",
    "                X_ar_test = pd.DataFrame(index=test_gdp.index)\n",
    "                for lag, col in enumerate(models['AR_lag_columns'], 1):\n",
    "                    X_ar_test[col] = gdp_target.shift(lag).loc[test_gdp.index]\n",
    "                \n",
    "                # Make predictions\n",
    "                ar_predictions = pd.Series(\n",
    "                    model.predict(X_ar_test),\n",
    "                    index=test_gdp.index,\n",
    "                    name=model_name\n",
    "                )\n",
    "                evaluator.add_model(model_name, ar_predictions)\n",
    "                log(f\"Generated predictions for {model_name}: {len(ar_predictions)} quarters\")\n",
    "            \n",
    "            elif 'MA_' in model_name:\n",
    "                # Get window size from model\n",
    "                window = model['window']\n",
    "                \n",
    "                # Calculate moving average for each test point\n",
    "                ma_predictions = pd.Series(index=test_gdp.index)\n",
    "                for i, date in enumerate(test_gdp.index):\n",
    "                    # Get previous window periods\n",
    "                    hist_data = gdp_target[gdp_target.index < date]\n",
    "                    if len(hist_data) >= window:\n",
    "                        ma_predictions[date] = hist_data[-window:].mean()\n",
    "                    else:\n",
    "                        # Use all available data if less than window\n",
    "                        ma_predictions[date] = hist_data.mean() if len(hist_data) > 0 else np.nan\n",
    "                \n",
    "                # Fill any missing values\n",
    "                ma_predictions = ma_predictions.fillna(method='ffill').fillna(0)\n",
    "                evaluator.add_model(model_name, ma_predictions)\n",
    "                log(f\"Generated predictions for {model_name}: {len(ma_predictions)} quarters\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            log(f\"Error generating predictions for {model_name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    log(\"Calculating evaluation metrics...\")\n",
    "    metrics = evaluator.calculate_metrics(rolling_window=8)\n",
    "    \n",
    "    # Output key metrics\n",
    "    log(\"\\nKey Performance Metrics:\")\n",
    "    log(\"-\" * 80)\n",
    "    log(f\"{'Model':<25} {'RMSE':>10} {'MAE':>10} {'Dir Acc':>10}\")\n",
    "    log(\"-\" * 80)\n",
    "    for model_name, model_metrics in metrics.items():\n",
    "        if model_name != 'Actual':\n",
    "            log(f\"{model_name:<25} {model_metrics['rmse']:>10.4f} {model_metrics['mae']:>10.4f} {model_metrics['direction_accuracy']:>10.4f}\")\n",
    "    \n",
    "    # Create plots\n",
    "    log(\"\\nGenerating evaluation plots...\")\n",
    "    try:\n",
    "        # Forecasts plot\n",
    "        fig = evaluator.plot_forecasts()\n",
    "        fig.savefig(os.path.join(output_folder, 'gdp_forecasts.png'))\n",
    "        plt.close(fig)\n",
    "        log(f\"Forecasts plot saved to {os.path.join(output_folder, 'gdp_forecasts.png')}\")\n",
    "        \n",
    "        # Error distribution plot\n",
    "        fig = evaluator.plot_error_distribution()\n",
    "        fig.savefig(os.path.join(output_folder, 'error_distribution.png'))\n",
    "        plt.close(fig)\n",
    "        log(f\"Error distribution plot saved to {os.path.join(output_folder, 'error_distribution.png')}\")\n",
    "        \n",
    "        # Rolling metrics plot\n",
    "        fig = evaluator.plot_rolling_metrics()\n",
    "        fig.savefig(os.path.join(output_folder, 'rolling_metrics.png'))\n",
    "        plt.close(fig)\n",
    "        log(f\"Rolling metrics plot saved to {os.path.join(output_folder, 'rolling_metrics.png')}\")\n",
    "    except Exception as e:\n",
    "        log(f\"Error generating evaluation plots: {e}\")\n",
    "    \n",
    "    # 8. Generate comprehensive report\n",
    "    log(\"\\n8. Generating Final Report...\")\n",
    "    try:\n",
    "        report_path = os.path.join(output_folder, 'gdp_forecast_evaluation.md')\n",
    "        report_content = evaluator.generate_report(report_path, include_plots=True)\n",
    "        log(f\"Comprehensive evaluation report saved to {report_path}\")\n",
    "    except Exception as e:\n",
    "        log(f\"Error generating evaluation report: {e}\")\n",
    "    \n",
    "    # 9. Conclusion\n",
    "    log(\"\\n9. Workflow Completed\")\n",
    "    log(\"=\" * 80)\n",
    "    log(f\"Monthly-Only GDP Forecasting Workflow completed at {pd.Timestamp.now()}\")\n",
    "    log(\"=\" * 80)\n",
    "    \n",
    "    return evaluator, models, preprocessor\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set parameters\n",
    "    DATA_FOLDER = \"./Project_Data\"\n",
    "    OUTPUT_FOLDER = \"./output\"\n",
    "    \n",
    "    # Run the workflow\n",
    "    evaluator, models, preprocessor = run_gdp_forecast_workflow_monthly_only(\n",
    "        data_folder=DATA_FOLDER,\n",
    "        output_folder=OUTPUT_FOLDER,\n",
    "        start_date='1980-01-01',  # Start date for analysis\n",
    "        end_date=None,  # End date (use None for all available data)\n",
    "        train_test_split=0.8,  # Use 80% of data for training\n",
    "        use_midas=True,  # Use MIDAS for final GDP prediction\n",
    "        monthly_factors=3,  # Number of monthly factors\n",
    "        gdp_ar_lags=4,  # Number of AR lags for GDP\n",
    "        random_state=42,  # For reproducibility\n",
    "        save_models=True  # Save models to files\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4930dd51-1b32-46aa-b360-35ec8f820cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting Advanced GDP Forecasting Workflow at 2025-05-09 14:19:10.866593\n",
      "================================================================================\n",
      "\n",
      "1. Setting up configuration...\n",
      "Configuration set up with 7 monthly files, 1 quarterly files\n",
      "\n",
      "2. Data Preprocessing...\n",
      "Found 18 files in ./Project_Data\n",
      "Processing monthly data...\n",
      "Processed CPI_mon_monthly.csv: 829 observations, 2 features\n",
      "Processed Unemployment_monthly.csv: 925 observations, 2 features\n",
      "Processed InterestRate_monthly.csv: 847 observations, 2 features\n",
      "Processed HousingStarts_monthly.csv: 792 observations, 2 features\n",
      "Processed Heavy_Truck_Sales.csv: 698 observations, 2 features\n",
      "Processed Manufacturing_Production_Motor_and_Vehicle_Parts.csv: 637 observations, 2 features\n",
      "Processed Consumer_Confidence.csv: 768 observations, 2 features\n",
      "Final monthly dataset: 926 observations, 14 features\n",
      "Processing quarterly data...\n",
      "Processed GDP_quaterly.csv: 311 observations, 2 features\n",
      "Final quarterly dataset: 311 observations, 2 features\n",
      "Processed data: monthly=(926, 14), quarterly=(311, 2)\n",
      "\n",
      "3. Initializing Advanced GDP Forecasting System...\n",
      "TensorFlow not available. Will use RandomForest only.\n",
      "\n",
      "================================================================================\n",
      "Advanced GDP Forecasting System - Training Phase\n",
      "================================================================================\n",
      "\n",
      "1. Data Overview:\n",
      "   - Monthly features: 14 variables, 926 time periods\n",
      "   - Quarterly features: 1 variables, 311 time periods\n",
      "   - Target variable: GDP_quaterly_Value_pct_change with 311 observations\n",
      "\n",
      "2. Train-Test Split:\n",
      "   - Training period: 1947-03-31 00:00:00 to 2009-03-31 00:00:00 (249 quarters)\n",
      "   - Testing period: 2009-06-30 00:00:00 to 2024-09-30 00:00:00 (62 quarters)\n",
      "\n",
      "3. Feature Selection:\n",
      "Processing 14 monthly features and 1 quarterly features\n",
      "Cleaning data by handling infinities and outliers...\n",
      "  Column CPI_mon_monthly_Value_pct_change_last contains 24 infinity values - replacing\n",
      "  Column CPI_mon_monthly_Value_pct_change_mean contains 54 infinity values - replacing\n",
      "Applying rf_importance feature selection method...\n",
      "Selected 49 monthly-derived features and 1 quarterly features\n",
      "   - Selected 50 features in total\n",
      "\n",
      "4. Model Training:\n",
      "   - Training Quantile Regression Forest...\n",
      "\n",
      "Training completed successfully.\n",
      "\n",
      "4. Generating GDP forecasts...\n",
      "\n",
      "5. Evaluating forecast performance...\n",
      "\n",
      "Key Statistical Metrics:\n",
      "--------------------------------------------------------------------------------\n",
      "Model                 RMSE        MAE    Dir Acc\n",
      "--------------------------------------------------------------------------------\n",
      "qrf                 1.7048     0.7959     0.7049\n",
      "\n",
      "Economic Value Metrics:\n",
      "--------------------------------------------------------------------------------\n",
      "Model               Return     Sharpe    Utility\n",
      "--------------------------------------------------------------------------------\n",
      "qrf                 0.6157     0.7013    -1.3111\n",
      "\n",
      "6. Creating visualization plots...\n",
      "pandas_datareader not available for recession shading\n",
      "Forecast plot saved to ./output/advanced/advanced_gdp_forecasts.png\n",
      "Feature importance plot saved to ./output/advanced/feature_importance.png\n",
      "\n",
      "7. Generating comprehensive report...\n",
      "Report saved to ./output/advanced/advanced_gdp_forecast_report.md\n",
      "Comprehensive report saved to ./output/advanced/advanced_gdp_forecast_report.md\n",
      "\n",
      "9. Workflow completed\n",
      "================================================================================\n",
      "Advanced GDP Forecasting Workflow completed at 2025-05-09 14:19:19.913576\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Advanced GDP Forecasting System\n",
    "\n",
    "This module implements state-of-the-art methods for GDP forecasting:\n",
    "1. Neural MIDAS with GRU for mixed-frequency modeling\n",
    "2. Intelligent feature selection for high-dimensional economic data\n",
    "3. Quantile Regression Forests for uncertainty quantification\n",
    "4. Enhanced forecast evaluation with economic significance metrics\n",
    "\n",
    "References to scientific literature are included throughout the implementation.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import warnings\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Silence warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Utility Functions\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "def trace_nans(name, df, threshold=0):\n",
    "    \"\"\"\n",
    "    Comprehensive NaN tracing function for pandas DataFrames.\n",
    "    \"\"\"\n",
    "    if isinstance(df, pd.Series):\n",
    "        nan_count = df.isna().sum()\n",
    "        total = len(df)\n",
    "        if nan_count > 0:\n",
    "            print(f\"WARNING: {name} Series contains {nan_count}/{total} NaNs ({nan_count/total:.2%})\")\n",
    "        return\n",
    "        \n",
    "    nan_count = df.isna().sum().sum()\n",
    "    if nan_count > 0:\n",
    "        rows, cols = df.shape\n",
    "        total_cells = rows * cols\n",
    "        \n",
    "        print(f\"WARNING: {name} contains {nan_count}/{total_cells} NaNs ({nan_count/total_cells:.2%})\")\n",
    "        \n",
    "        cols_with_nans = df.columns[df.isna().sum() > threshold]\n",
    "        if len(cols_with_nans) > 0:\n",
    "            print(f\"  Columns with > {threshold} NaNs:\")\n",
    "            for col in cols_with_nans:\n",
    "                col_nans = df[col].isna().sum()\n",
    "                print(f\"    {col}: {col_nans}/{rows} NaNs ({col_nans/rows:.2%})\")\n",
    "        \n",
    "        row_nan_counts = df.isna().sum(axis=1)\n",
    "        rows_with_many_nans = row_nan_counts[row_nan_counts > cols//4].sort_values(ascending=False)\n",
    "        if len(rows_with_many_nans) > 0:\n",
    "            print(f\"  Rows with significant NaNs:\")\n",
    "            for idx, count in rows_with_many_nans.head(5).items():\n",
    "                print(f\"    Row at {idx}: {count}/{cols} NaNs ({count/cols:.2%})\")\n",
    "        \n",
    "        first_rows_nan_pct = df.head(rows//10).isna().sum().sum() / (rows//10 * cols)\n",
    "        last_rows_nan_pct = df.tail(rows//10).isna().sum().sum() / (rows//10 * cols)\n",
    "        if first_rows_nan_pct > 0.1:\n",
    "            print(f\"  First 10% of rows have {first_rows_nan_pct:.2%} NaNs - possible lag/window effect\")\n",
    "        if last_rows_nan_pct > 0.1:\n",
    "            print(f\"  Last 10% of rows have {last_rows_nan_pct:.2%} NaNs - possible trailing window effect\")\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Feature Selection Module\n",
    "#-----------------------------------------------------------------------------\n",
    "class AdvancedFeatureSelector:\n",
    "    \"\"\"\n",
    "    Intelligent feature selection for economic time series data.\n",
    "    \n",
    "    Based on research by Bai & Ng (2020), who demonstrated that targeted feature \n",
    "    selection can dramatically improve macroeconomic forecasting with large datasets.\n",
    "    \n",
    "    References:\n",
    "    -----------\n",
    "    Bai, J., & Ng, S. (2020). Acute vs. Chronic Impulses in High-Dimensional Dynamic \n",
    "    Factor Models. Journal of Econometrics, 214(1), 101-120.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method='boruta', max_features=50, n_estimators=100, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the feature selector.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        method : str\n",
    "            Feature selection method ('boruta', 'rf_importance', 'mutual_info')\n",
    "        max_features : int\n",
    "            Maximum number of features to select\n",
    "        n_estimators : int\n",
    "            Number of estimators for ensemble methods\n",
    "        random_state : int\n",
    "            Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.max_features = max_features\n",
    "        self.n_estimators = n_estimators\n",
    "        self.random_state = random_state\n",
    "        self.selected_features = None\n",
    "        self.feature_importance = None\n",
    "    \n",
    "    def selective_feature_engineering(self, X_monthly, X_quarterly, target_column=None):\n",
    "        \"\"\"\n",
    "        Intelligent feature selection across mixed-frequency data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_monthly : DataFrame\n",
    "            Monthly features\n",
    "        X_quarterly : DataFrame\n",
    "            Quarterly features (including target if target_column is None)\n",
    "        target_column : str, optional\n",
    "            Name of target column in X_quarterly\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Selected features for monthly and quarterly data\n",
    "        \"\"\"\n",
    "        from scipy import stats\n",
    "        \n",
    "        # Extract target variable\n",
    "        if target_column is None:\n",
    "            # Assume first column is target\n",
    "            y = X_quarterly.iloc[:, 0]\n",
    "            X_q = X_quarterly.iloc[:, 1:]\n",
    "        else:\n",
    "            y = X_quarterly[target_column]\n",
    "            X_q = X_quarterly.drop(columns=[target_column])\n",
    "        \n",
    "        print(f\"Processing {len(X_monthly.columns)} monthly features and {len(X_q.columns)} quarterly features\")\n",
    "        \n",
    "        # Extract quarterly features from monthly data\n",
    "        quarterly_features = pd.DataFrame(index=y.index)\n",
    "        \n",
    "        for col in X_monthly.columns:\n",
    "            # For each monthly feature, create 5 quarterly aggregations\n",
    "            for q_date in y.index:\n",
    "                # Get monthly data for the quarter (last 3 months)\n",
    "                quarter_start = pd.Timestamp(q_date) - pd.DateOffset(months=3)\n",
    "                month_data = X_monthly[col][(X_monthly.index > quarter_start) & \n",
    "                                         (X_monthly.index <= q_date)]\n",
    "                \n",
    "                if len(month_data) > 0:\n",
    "                    # Last value\n",
    "                    quarterly_features.loc[q_date, f\"{col}_last\"] = month_data.iloc[-1]\n",
    "                    \n",
    "                    # Mean value\n",
    "                    quarterly_features.loc[q_date, f\"{col}_mean\"] = month_data.mean()\n",
    "                    \n",
    "                    # Standard deviation (volatility)\n",
    "                    quarterly_features.loc[q_date, f\"{col}_std\"] = month_data.std() if len(month_data) > 1 else 0\n",
    "                    \n",
    "                    # Trend (slope of linear regression)\n",
    "                    if len(month_data) > 1:\n",
    "                        try:\n",
    "                            x = np.arange(len(month_data))\n",
    "                            slope = stats.linregress(x, month_data.values).slope\n",
    "                            quarterly_features.loc[q_date, f\"{col}_slope\"] = slope\n",
    "                        except:\n",
    "                            # Handle case where linear regression fails\n",
    "                            quarterly_features.loc[q_date, f\"{col}_slope\"] = 0\n",
    "                    else:\n",
    "                        quarterly_features.loc[q_date, f\"{col}_slope\"] = 0\n",
    "                    \n",
    "                    # Acceleration (second difference)\n",
    "                    if len(month_data) > 2:\n",
    "                        try:\n",
    "                            diff2 = np.diff(month_data.values, 2)\n",
    "                            if len(diff2) > 0 and np.isfinite(diff2[-1]):\n",
    "                                quarterly_features.loc[q_date, f\"{col}_accel\"] = diff2[-1]\n",
    "                            else:\n",
    "                                quarterly_features.loc[q_date, f\"{col}_accel\"] = 0\n",
    "                        except:\n",
    "                            quarterly_features.loc[q_date, f\"{col}_accel\"] = 0\n",
    "                    else:\n",
    "                        quarterly_features.loc[q_date, f\"{col}_accel\"] = 0\n",
    "        \n",
    "        # Combine all features\n",
    "        combined_features = pd.concat([quarterly_features, X_q], axis=1)\n",
    "        \n",
    "        # Handle missing values\n",
    "        combined_features = combined_features.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "        \n",
    "        # Handle infinite values and extreme outliers\n",
    "        print(\"Cleaning data by handling infinities and outliers...\")\n",
    "        for col in combined_features.columns:\n",
    "            # Replace inf values with NaN and then fill\n",
    "            inf_mask = ~np.isfinite(combined_features[col])\n",
    "            if inf_mask.any():\n",
    "                print(f\"  Column {col} contains {inf_mask.sum()} infinity values - replacing\")\n",
    "                combined_features.loc[inf_mask, col] = np.nan\n",
    "                \n",
    "            # Handle extreme values using winsorization (capping)\n",
    "            if combined_features[col].count() > 0:  # Only process if we have values\n",
    "                q1 = combined_features[col].quantile(0.01)\n",
    "                q99 = combined_features[col].quantile(0.99)\n",
    "                iqr = q99 - q1\n",
    "                \n",
    "                # Set very extreme values to boundaries (prevent numeric overflow)\n",
    "                lower_bound = q1 - 3 * iqr\n",
    "                upper_bound = q99 + 3 * iqr\n",
    "                \n",
    "                # Count extreme values\n",
    "                extreme_mask = (combined_features[col] < lower_bound) | (combined_features[col] > upper_bound)\n",
    "                if extreme_mask.any():\n",
    "                    print(f\"  Column {col} contains {extreme_mask.sum()} extreme values - winsorizing\")\n",
    "                    combined_features.loc[combined_features[col] < lower_bound, col] = lower_bound\n",
    "                    combined_features.loc[combined_features[col] > upper_bound, col] = upper_bound\n",
    "        \n",
    "        # Fill any remaining NaN values\n",
    "        combined_features = combined_features.fillna(0)\n",
    "        \n",
    "        # Double-check for any remaining infinities or NaNs\n",
    "        if not np.isfinite(combined_features.values).all():\n",
    "            print(\"Warning: Some infinite values remain after cleaning\")\n",
    "            # Force replace any remaining problematic values\n",
    "            combined_features = combined_features.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "        \n",
    "        # Standardize features with robust scaler to handle outliers better\n",
    "        try:\n",
    "            # Try using RobustScaler which is less affected by outliers\n",
    "            from sklearn.preprocessing import RobustScaler\n",
    "            scaler = RobustScaler()\n",
    "            X_scaled = scaler.fit_transform(combined_features)\n",
    "        except Exception as e:\n",
    "            print(f\"RobustScaler failed: {e}\")\n",
    "            # Fallback to manual standardization\n",
    "            print(\"Falling back to manual standardization...\")\n",
    "            X_scaled = np.zeros_like(combined_features.values)\n",
    "            for i, col in enumerate(combined_features.columns):\n",
    "                col_data = combined_features[col].values\n",
    "                col_median = np.median(col_data)\n",
    "                col_mad = np.median(np.abs(col_data - col_median)) + 1e-10  # avoid division by zero\n",
    "                X_scaled[:, i] = (col_data - col_median) / col_mad\n",
    "        \n",
    "        X_scaled_df = pd.DataFrame(X_scaled, index=combined_features.index, columns=combined_features.columns)\n",
    "        \n",
    "        # Apply feature selection\n",
    "        print(f\"Applying {self.method} feature selection method...\")\n",
    "        \n",
    "        if self.method == 'boruta':\n",
    "            try:\n",
    "                # Boruta feature selection (wrapper method)\n",
    "                from boruta import BorutaPy\n",
    "                \n",
    "                # Base estimator\n",
    "                rf = RandomForestRegressor(\n",
    "                    n_estimators=self.n_estimators,\n",
    "                    max_depth=7,\n",
    "                    random_state=self.random_state,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                \n",
    "                # Initialize Boruta\n",
    "                boruta_selector = BorutaPy(\n",
    "                    rf, \n",
    "                    n_estimators='auto', \n",
    "                    verbose=2, \n",
    "                    random_state=self.random_state,\n",
    "                    max_iter=100\n",
    "                )\n",
    "                \n",
    "                # Fit\n",
    "                boruta_selector.fit(X_scaled, y.values)\n",
    "                \n",
    "                # Get results\n",
    "                self.feature_importance = pd.Series(\n",
    "                    boruta_selector.ranking_,\n",
    "                    index=combined_features.columns\n",
    "                ).sort_values()\n",
    "                \n",
    "                # Get selected features\n",
    "                selected_mask = boruta_selector.support_\n",
    "                \n",
    "                if sum(selected_mask) > self.max_features:\n",
    "                    # Too many features selected, use ranking to narrow down\n",
    "                    top_indices = np.argsort(boruta_selector.ranking_)[:self.max_features]\n",
    "                    selected_mask = np.zeros_like(selected_mask, dtype=bool)\n",
    "                    selected_mask[top_indices] = True\n",
    "                \n",
    "                self.selected_features = combined_features.columns[selected_mask].tolist()\n",
    "                \n",
    "            except ImportError:\n",
    "                print(\"Boruta not available, falling back to random forest importance\")\n",
    "                self.method = 'rf_importance'\n",
    "        \n",
    "        if self.method == 'rf_importance':\n",
    "            # Random forest feature importance\n",
    "            rf = RandomForestRegressor(\n",
    "                n_estimators=self.n_estimators,\n",
    "                max_depth=7,\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            # Fit\n",
    "            rf.fit(X_scaled, y)\n",
    "            \n",
    "            # Get feature importance\n",
    "            self.feature_importance = pd.Series(\n",
    "                rf.feature_importances_,\n",
    "                index=combined_features.columns\n",
    "            ).sort_values(ascending=False)\n",
    "            \n",
    "            # Select top features\n",
    "            self.selected_features = self.feature_importance.index[:self.max_features].tolist()\n",
    "        \n",
    "        elif self.method == 'mutual_info':\n",
    "            # Mutual information regression\n",
    "            mi_scores = mutual_info_regression(X_scaled, y, random_state=self.random_state)\n",
    "            \n",
    "            # Create feature importance\n",
    "            self.feature_importance = pd.Series(\n",
    "                mi_scores,\n",
    "                index=combined_features.columns\n",
    "            ).sort_values(ascending=False)\n",
    "            \n",
    "            # Select top features\n",
    "            self.selected_features = self.feature_importance.index[:self.max_features].tolist()\n",
    "        \n",
    "        # Split selected features into monthly and quarterly groups\n",
    "        monthly_features_prefix = [col.split('_')[0] for col in quarterly_features.columns]\n",
    "        \n",
    "        monthly_selected = [col for col in self.selected_features \n",
    "                           if any(col.startswith(prefix) for prefix in monthly_features_prefix)]\n",
    "        \n",
    "        quarterly_selected = [col for col in self.selected_features \n",
    "                             if col in X_q.columns]\n",
    "        \n",
    "        print(f\"Selected {len(monthly_selected)} monthly-derived features and {len(quarterly_selected)} quarterly features\")\n",
    "        \n",
    "        return {\n",
    "            'monthly': monthly_selected,\n",
    "            'quarterly': quarterly_selected,\n",
    "            'all': self.selected_features,\n",
    "            'importance': self.feature_importance\n",
    "        }\n",
    "    \n",
    "    def transform(self, X_monthly, X_quarterly, quarterly_dates=None, align_dates=True):\n",
    "        \"\"\"\n",
    "        Transform data using selected features.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_monthly : DataFrame\n",
    "            Monthly features\n",
    "        X_quarterly : DataFrame\n",
    "            Quarterly features\n",
    "        quarterly_dates : DatetimeIndex, optional\n",
    "            Quarterly dates to use for alignment\n",
    "        align_dates : bool\n",
    "            Whether to align monthly data to quarterly dates\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame\n",
    "            Transformed data with selected features\n",
    "        \"\"\"\n",
    "        from scipy import stats\n",
    "        \n",
    "        if self.selected_features is None:\n",
    "            raise ValueError(\"No features selected. Call selective_feature_engineering first.\")\n",
    "        \n",
    "        # Extract quarterly features from monthly data (if applicable)\n",
    "        if align_dates:\n",
    "            if quarterly_dates is None:\n",
    "                quarterly_dates = X_quarterly.index\n",
    "            \n",
    "            quarterly_features = pd.DataFrame(index=quarterly_dates)\n",
    "            \n",
    "            # Get list of base monthly columns that were included\n",
    "            monthly_cols = set()\n",
    "            for feature in self.selected_features:\n",
    "                parts = feature.split('_')\n",
    "                if len(parts) > 1 and f\"{parts[0]}_last\" in self.selected_features:\n",
    "                    monthly_cols.add(parts[0])\n",
    "            \n",
    "            # Process only needed monthly columns\n",
    "            for col in monthly_cols:\n",
    "                # For each needed monthly feature, create quarterly aggregations\n",
    "                for q_date in quarterly_dates:\n",
    "                    # Get monthly data for the quarter (last 3 months)\n",
    "                    quarter_start = pd.Timestamp(q_date) - pd.DateOffset(months=3)\n",
    "                    month_data = X_monthly[col][(X_monthly.index > quarter_start) & \n",
    "                                             (X_monthly.index <= q_date)]\n",
    "                    \n",
    "                    if len(month_data) > 0:\n",
    "                        # Create only needed aggregations\n",
    "                        if f\"{col}_last\" in self.selected_features:\n",
    "                            quarterly_features.loc[q_date, f\"{col}_last\"] = month_data.iloc[-1]\n",
    "                        \n",
    "                        if f\"{col}_mean\" in self.selected_features:\n",
    "                            quarterly_features.loc[q_date, f\"{col}_mean\"] = month_data.mean()\n",
    "                        \n",
    "                        if f\"{col}_std\" in self.selected_features:\n",
    "                            quarterly_features.loc[q_date, f\"{col}_std\"] = month_data.std() if len(month_data) > 1 else 0\n",
    "                        \n",
    "                        if f\"{col}_slope\" in self.selected_features:\n",
    "                            if len(month_data) > 1:\n",
    "                                x = np.arange(len(month_data))\n",
    "                                slope = stats.linregress(x, month_data.values).slope\n",
    "                                quarterly_features.loc[q_date, f\"{col}_slope\"] = slope\n",
    "                            else:\n",
    "                                quarterly_features.loc[q_date, f\"{col}_slope\"] = 0\n",
    "                        \n",
    "                        if f\"{col}_accel\" in self.selected_features:\n",
    "                            if len(month_data) > 2:\n",
    "                                diff2 = np.diff(month_data.values, 2)\n",
    "                                quarterly_features.loc[q_date, f\"{col}_accel\"] = diff2[-1]\n",
    "                            else:\n",
    "                                quarterly_features.loc[q_date, f\"{col}_accel\"] = 0\n",
    "            \n",
    "            # Combine with quarterly features\n",
    "            X_q_selected = X_quarterly[\n",
    "                [col for col in self.selected_features if col in X_quarterly.columns]\n",
    "            ]\n",
    "            \n",
    "            transformed_data = pd.concat([quarterly_features, X_q_selected], axis=1)\n",
    "            \n",
    "            # Handle missing values\n",
    "            transformed_data = transformed_data.fillna(method='ffill').fillna(method='bfill')\n",
    "            \n",
    "        else:\n",
    "            # Just select columns from the provided data\n",
    "            transformed_data = X_quarterly[\n",
    "                [col for col in self.selected_features if col in X_quarterly.columns]\n",
    "            ]\n",
    "        \n",
    "        return transformed_data\n",
    "    \n",
    "    def plot_feature_importance(self, top_n=20, figsize=(10, 12)):\n",
    "        \"\"\"\n",
    "        Plot feature importance.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        top_n : int\n",
    "            Number of top features to show\n",
    "        figsize : tuple\n",
    "            Figure size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            Feature importance plot\n",
    "        \"\"\"\n",
    "        if self.feature_importance is None:\n",
    "            raise ValueError(\"No feature importance available. Call selective_feature_engineering first.\")\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Plot top N features\n",
    "        top_features = self.feature_importance.sort_values(ascending=True).tail(top_n)\n",
    "        ax = top_features.plot.barh()\n",
    "        \n",
    "        ax.set_title(f'Top {top_n} Features by Importance')\n",
    "        ax.set_xlabel('Importance')\n",
    "        ax.set_ylabel('Feature')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return plt.gcf()\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Neural MIDAS Implementation\n",
    "#-----------------------------------------------------------------------------\n",
    "class MIDASGRU:\n",
    "    \"\"\"\n",
    "    Neural MIDAS with GRU for mixed-frequency time series forecasting.\n",
    "    \n",
    "    Based on research by Babii et al. (2022) and Goulet Coulombe (2020), who demonstrated\n",
    "    that neural networks with recurrent architectures can capture complex nonlinear\n",
    "    patterns in mixed-frequency macroeconomic data.\n",
    "    \n",
    "    References:\n",
    "    -----------\n",
    "    Babii, A., Ghysels, E., & Striaukas, J. (2022). Machine Learning Time Series \n",
    "    Regressions with an Application to Nowcasting. Journal of Business & Economic \n",
    "    Statistics, 40(3), 1094-1106.\n",
    "    \n",
    "    Goulet Coulombe, P. (2020). The Macroeconomy as a Random Forest. \n",
    "    Working Paper, arXiv:2006.12724.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, high_freq_dim=None, low_freq_dim=None, hidden_size=32, max_lags=12, \n",
    "                 dropout_rate=0.2, learning_rate=0.001, batch_size=32, epochs=200,\n",
    "                 random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the Neural MIDAS model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        high_freq_dim : int\n",
    "            Dimension of high-frequency data\n",
    "        low_freq_dim : int\n",
    "            Dimension of low-frequency/autoregressive data\n",
    "        hidden_size : int\n",
    "            Size of hidden layers\n",
    "        max_lags : int\n",
    "            Maximum number of lags for high-frequency data\n",
    "        dropout_rate : float\n",
    "            Dropout rate for regularization\n",
    "        learning_rate : float\n",
    "            Learning rate for optimization\n",
    "        batch_size : int\n",
    "            Batch size for training\n",
    "        epochs : int\n",
    "            Maximum number of training epochs\n",
    "        random_state : int\n",
    "            Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.high_freq_dim = high_freq_dim\n",
    "        self.low_freq_dim = low_freq_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_lags = max_lags\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.random_state = random_state\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        \n",
    "        # Set random seed\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        # Try importing TensorFlow\n",
    "        try:\n",
    "            import tensorflow as tf\n",
    "            tf.random.set_seed(random_state)\n",
    "            self.tf = tf\n",
    "        except ImportError:\n",
    "            print(\"TensorFlow not available. Neural MIDAS model cannot be used.\")\n",
    "            self.tf = None\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"Build the Neural MIDAS model architecture.\"\"\"\n",
    "        if self.tf is None:\n",
    "            raise ImportError(\"TensorFlow is required for Neural MIDAS.\")\n",
    "        \n",
    "        # High-frequency input (time steps × features)\n",
    "        high_freq_input = self.tf.keras.Input(shape=(self.max_lags, self.high_freq_dim))\n",
    "        \n",
    "        # Process high-frequency data with GRU\n",
    "        gru_output = self.tf.keras.layers.GRU(\n",
    "            self.hidden_size, \n",
    "            return_sequences=False,\n",
    "            dropout=self.dropout_rate,\n",
    "            recurrent_dropout=0.0\n",
    "        )(high_freq_input)\n",
    "        \n",
    "        # Low-frequency/autoregressive input (if provided)\n",
    "        if self.low_freq_dim > 0:\n",
    "            low_freq_input = self.tf.keras.Input(shape=(self.low_freq_dim,))\n",
    "            \n",
    "            # Process low-frequency data with a dense layer\n",
    "            low_freq_processed = self.tf.keras.layers.Dense(self.hidden_size//2)(low_freq_input)\n",
    "            low_freq_processed = self.tf.keras.layers.BatchNormalization()(low_freq_processed)\n",
    "            low_freq_processed = self.tf.keras.layers.Activation('relu')(low_freq_processed)\n",
    "            \n",
    "            # Combine high and low frequency information\n",
    "            combined = self.tf.keras.layers.Concatenate()([gru_output, low_freq_processed])\n",
    "        else:\n",
    "            combined = gru_output\n",
    "            low_freq_input = None\n",
    "        \n",
    "        # Final prediction layers\n",
    "        x = self.tf.keras.layers.Dense(self.hidden_size, activation='relu')(combined)\n",
    "        x = self.tf.keras.layers.BatchNormalization()(x)\n",
    "        x = self.tf.keras.layers.Dropout(self.dropout_rate)(x)\n",
    "        \n",
    "        # Output layer for GDP prediction\n",
    "        output = self.tf.keras.layers.Dense(1)(x)\n",
    "        \n",
    "        # Create model with appropriate inputs\n",
    "        if self.low_freq_dim > 0:\n",
    "            model = self.tf.keras.Model(inputs=[high_freq_input, low_freq_input], outputs=output)\n",
    "        else:\n",
    "            model = self.tf.keras.Model(inputs=high_freq_input, outputs=output)\n",
    "        \n",
    "        # Compile model\n",
    "        optimizer = self.tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def prepare_midas_data(self, X_monthly, y, X_quarterly=None, align_dates=True):\n",
    "        \"\"\"\n",
    "        Prepare data for Neural MIDAS model, with proper alignment of frequencies.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_monthly : DataFrame\n",
    "            Monthly features\n",
    "        y : Series\n",
    "            Target variable (quarterly)\n",
    "        X_quarterly : DataFrame, optional\n",
    "            Quarterly features\n",
    "        align_dates : bool\n",
    "            Whether to align data based on dates\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            Prepared data for MIDAS model\n",
    "        \"\"\"\n",
    "        # Create lag structure for monthly data\n",
    "        quarterly_dates = y.index\n",
    "        monthly_features = []\n",
    "        \n",
    "        for date in quarterly_dates:\n",
    "            # Get data for each quarterly date\n",
    "            # We need the preceding months for each quarter\n",
    "            month_end = pd.Timestamp(date)\n",
    "            month_start = month_end - pd.DateOffset(months=self.max_lags)\n",
    "            \n",
    "            # Get monthly data in this window\n",
    "            window_data = X_monthly[(X_monthly.index > month_start) & \n",
    "                                   (X_monthly.index <= month_end)]\n",
    "            \n",
    "            # Ensure we have the right number of months\n",
    "            if len(window_data) < self.max_lags:\n",
    "                # Pad with zeros if needed\n",
    "                pad_size = self.max_lags - len(window_data)\n",
    "                pad_df = pd.DataFrame(0, \n",
    "                                     index=range(pad_size), \n",
    "                                     columns=window_data.columns)\n",
    "                window_data = pd.concat([pad_df, window_data.reset_index(drop=True)])\n",
    "            \n",
    "            # If we have too many months, take the most recent ones\n",
    "            elif len(window_data) > self.max_lags:\n",
    "                window_data = window_data.iloc[-self.max_lags:]\n",
    "            \n",
    "            # Add to the list\n",
    "            monthly_features.append(window_data.values)\n",
    "        \n",
    "        # Convert to numpy array [n_samples, n_lags, n_features]\n",
    "        X_hf = np.array(monthly_features)\n",
    "        \n",
    "        # Handle quarterly features if provided\n",
    "        if X_quarterly is not None:\n",
    "            # Align quarterly data with target\n",
    "            common_idx = y.index.intersection(X_quarterly.index)\n",
    "            X_lf = X_quarterly.loc[common_idx].values\n",
    "            y_aligned = y.loc[common_idx].values\n",
    "            \n",
    "            # Keep only matching samples for high-freq data\n",
    "            date_indices = [i for i, date in enumerate(quarterly_dates) if date in common_idx]\n",
    "            X_hf = X_hf[date_indices]\n",
    "        else:\n",
    "            X_lf = None\n",
    "            y_aligned = y.values\n",
    "        \n",
    "        # Update dimensions\n",
    "        self.high_freq_dim = X_hf.shape[2]\n",
    "        \n",
    "        if X_lf is not None:\n",
    "            self.low_freq_dim = X_lf.shape[1]\n",
    "        else:\n",
    "            self.low_freq_dim = 0\n",
    "        \n",
    "        # Return prepared data\n",
    "        if X_lf is not None:\n",
    "            return [X_hf, X_lf], y_aligned\n",
    "        else:\n",
    "            return X_hf, y_aligned\n",
    "    \n",
    "    def fit(self, X_monthly, y, X_quarterly=None, validation_split=0.2, verbose=1):\n",
    "        \"\"\"\n",
    "        Fit the Neural MIDAS model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_monthly : DataFrame\n",
    "            Monthly data\n",
    "        y : Series\n",
    "            Quarterly target variable\n",
    "        X_quarterly : DataFrame, optional\n",
    "            Quarterly data for AR component\n",
    "        validation_split : float\n",
    "            Proportion of data to use for validation\n",
    "        verbose : int\n",
    "            Verbosity level\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self\n",
    "            Fitted model instance\n",
    "        \"\"\"\n",
    "        if self.tf is None:\n",
    "            raise ImportError(\"TensorFlow is required for Neural MIDAS.\")\n",
    "        \n",
    "        # Prepare data\n",
    "        inputs, targets = self.prepare_midas_data(X_monthly, y, X_quarterly)\n",
    "        \n",
    "        # Build model\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "        # Add early stopping and learning rate reduction\n",
    "        callbacks = [\n",
    "            self.tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=20,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            self.tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=10,\n",
    "                min_lr=1e-6\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Fit model\n",
    "        history = self.model.fit(\n",
    "            inputs, targets,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=callbacks,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        self.history = history\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_monthly, X_quarterly=None, quarterly_dates=None):\n",
    "        \"\"\"\n",
    "        Generate predictions with the fitted model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_monthly : DataFrame\n",
    "            Monthly data\n",
    "        X_quarterly : DataFrame, optional\n",
    "            Quarterly data for AR component\n",
    "        quarterly_dates : DatetimeIndex, optional\n",
    "            Quarterly dates for prediction\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        Series\n",
    "            Predicted values\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        # Default to all dates if not specified\n",
    "        if quarterly_dates is None and X_quarterly is not None:\n",
    "            quarterly_dates = X_quarterly.index\n",
    "        elif quarterly_dates is None:\n",
    "            # Try to infer from monthly data\n",
    "            # We'll use the end of each quarter in the monthly data\n",
    "            all_months = pd.DatetimeIndex(X_monthly.index)\n",
    "            quarterly_dates = pd.DatetimeIndex([date for date in all_months \n",
    "                                              if date.month in [3, 6, 9, 12] and \n",
    "                                              date.day >= 28])\n",
    "        \n",
    "        # Create lag structure for monthly data\n",
    "        monthly_features = []\n",
    "        \n",
    "        for date in quarterly_dates:\n",
    "            # Get data for each quarterly date\n",
    "            month_end = pd.Timestamp(date)\n",
    "            month_start = month_end - pd.DateOffset(months=self.max_lags)\n",
    "            \n",
    "            # Get monthly data in this window\n",
    "            window_data = X_monthly[(X_monthly.index > month_start) & \n",
    "                                   (X_monthly.index <= month_end)]\n",
    "            \n",
    "            # Ensure we have the right number of months\n",
    "            if len(window_data) < self.max_lags:\n",
    "                # Pad with zeros if needed\n",
    "                pad_size = self.max_lags - len(window_data)\n",
    "                pad_df = pd.DataFrame(0, \n",
    "                                     index=range(pad_size), \n",
    "                                     columns=window_data.columns)\n",
    "                window_data = pd.concat([pad_df, window_data.reset_index(drop=True)])\n",
    "            \n",
    "            # If we have too many months, take the most recent ones\n",
    "            elif len(window_data) > self.max_lags:\n",
    "                window_data = window_data.iloc[-self.max_lags:]\n",
    "            \n",
    "            # Add to the list\n",
    "            monthly_features.append(window_data.values)\n",
    "        \n",
    "        # Convert to numpy array [n_samples, n_lags, n_features]\n",
    "        X_hf = np.array(monthly_features)\n",
    "        \n",
    "        # Handle quarterly features if provided\n",
    "        if X_quarterly is not None:\n",
    "            # Get matching quarterly data\n",
    "            common_idx = quarterly_dates.intersection(X_quarterly.index)\n",
    "            X_lf = X_quarterly.loc[common_idx].values\n",
    "            \n",
    "            # Keep only matching samples for high-freq data\n",
    "            date_indices = [i for i, date in enumerate(quarterly_dates) if date in common_idx]\n",
    "            X_hf = X_hf[date_indices]\n",
    "            \n",
    "            # Update quarterly dates\n",
    "            quarterly_dates = common_idx\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = self.model.predict([X_hf, X_lf])\n",
    "        else:\n",
    "            # Make predictions with only high-frequency data\n",
    "            y_pred = self.model.predict(X_hf)\n",
    "        \n",
    "        # Convert to Series\n",
    "        predictions = pd.Series(y_pred.flatten(), index=quarterly_dates, name='MIDAS_GRU')\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def plot_training_history(self, figsize=(10, 6)):\n",
    "        \"\"\"\n",
    "        Plot training history.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        figsize : tuple\n",
    "            Figure size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            Training history plot\n",
    "        \"\"\"\n",
    "        if self.history is None:\n",
    "            raise ValueError(\"Model has not been trained yet\")\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.history.history['loss'], label='Training Loss')\n",
    "        plt.plot(self.history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss (MSE)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot MAE\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.history.history['mae'], label='Training MAE')\n",
    "        plt.plot(self.history.history['val_mae'], label='Validation MAE')\n",
    "        plt.title('Model MAE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return plt.gcf()\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Quantile Regression Forests\n",
    "#-----------------------------------------------------------------------------\n",
    "class QuantileGDPForecaster:\n",
    "    \"\"\"\n",
    "    Quantile Regression Forests for GDP forecasting with uncertainty quantification.\n",
    "    \n",
    "    Based on research by Adrian et al. (2022) and Meinshausen (2006), who demonstrated\n",
    "    that quantile-based approaches can effectively capture the entire distribution of \n",
    "    potential economic outcomes, particularly during downturns.\n",
    "    \n",
    "    References:\n",
    "    -----------\n",
    "    Adrian, T., Boyarchenko, N., & Giannone, D. (2022). Multimodal Density Forecasts for \n",
    "    the U.S. Economy. Review of Economics and Statistics, 104(5), 926-942.\n",
    "    \n",
    "    Meinshausen, N. (2006). Quantile Regression Forests. Journal of Machine Learning \n",
    "    Research, 7, 983-999.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=500, max_features='sqrt', min_samples_leaf=5,\n",
    "                 quantiles=[0.1, 0.25, 0.5, 0.75, 0.9], random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the Quantile Regression Forest model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_estimators : int\n",
    "            Number of trees in the forest\n",
    "        max_features : str or int\n",
    "            Maximum number of features for splits\n",
    "        min_samples_leaf : int\n",
    "            Minimum samples in each leaf node\n",
    "        quantiles : list\n",
    "            Quantiles to compute\n",
    "        random_state : int\n",
    "            Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.quantiles = quantiles\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Initialize model\n",
    "        self.rf_model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_features=max_features,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            bootstrap=True,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Storage for training data\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Quantile Regression Forest model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Training features\n",
    "        y : array-like\n",
    "            Target values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self\n",
    "            Fitted model instance\n",
    "        \"\"\"\n",
    "        # Store training data\n",
    "        self.X_train = X.copy() if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
    "        self.y_train = y.copy() if isinstance(y, pd.Series) else pd.Series(y)\n",
    "        \n",
    "        # Fit random forest\n",
    "        self.rf_model.fit(X, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _get_leaves_for_sample(self, X_sample):\n",
    "        \"\"\"Helper method to get leaf indices for a sample.\"\"\"\n",
    "        leaves = []\n",
    "        \n",
    "        # Convert to numpy array with explicit dtype\n",
    "        if isinstance(X_sample, pd.Series):\n",
    "            X_sample = X_sample.values\n",
    "        \n",
    "        # Ensure correct dtype - sklearn's trees can be picky about dtype\n",
    "        X_sample = np.asarray(X_sample, dtype=np.float32)\n",
    "        \n",
    "        for tree in self.rf_model.estimators_:\n",
    "            try:\n",
    "                # Get the leaf node index for each tree\n",
    "                leaf_id = tree.tree_.apply(X_sample.reshape(1, -1))[0]\n",
    "                leaves.append(leaf_id)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error getting leaf node - {e}\")\n",
    "                # Use a fallback - just use a random leaf\n",
    "                # This isn't ideal but prevents the process from breaking\n",
    "                leaf_id = np.random.randint(0, tree.tree_.node_count)\n",
    "                leaves.append(leaf_id)\n",
    "            \n",
    "        return leaves\n",
    "    \n",
    "    def _get_weights(self, X_sample):\n",
    "        \"\"\"\n",
    "        Get weights for each training sample based on leaf co-occurrence.\n",
    "        \n",
    "        This is the core of the quantile regression forest algorithm from\n",
    "        Meinshausen (2006).\n",
    "        \"\"\"\n",
    "        # Get leaf indices for the test sample\n",
    "        try:\n",
    "            sample_leaves = self._get_leaves_for_sample(X_sample)\n",
    "            \n",
    "            # Initialize weights\n",
    "            n_trees = len(self.rf_model.estimators_)\n",
    "            n_train_samples = len(self.y_train)\n",
    "            weights = np.zeros(n_train_samples)\n",
    "            \n",
    "            # For each tree, find training samples in the same leaf\n",
    "            for t, tree in enumerate(self.rf_model.estimators_):\n",
    "                # Get all leaf assignments for training data\n",
    "                # Convert training data to float32 for consistency\n",
    "                X_train_float32 = self.X_train.values.astype(np.float32)\n",
    "                train_leaves = tree.tree_.apply(X_train_float32)\n",
    "                \n",
    "                # Find samples in the same leaf\n",
    "                same_leaf = (train_leaves == sample_leaves[t])\n",
    "                \n",
    "                # Increment weights\n",
    "                weights[same_leaf] += 1.0 / n_trees\n",
    "            \n",
    "            return weights\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error calculating weights - {e}\")\n",
    "            # Return uniform weights as fallback\n",
    "            return np.ones(len(self.y_train)) / len(self.y_train)\n",
    "    \n",
    "    def predict_quantiles(self, X, return_all=False, compute_intervals=True):\n",
    "        \"\"\"\n",
    "        Generate quantile predictions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Features\n",
    "        return_all : bool\n",
    "            Whether to return all sample predictions\n",
    "        compute_intervals : bool\n",
    "            Whether to compute prediction intervals\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame\n",
    "            Predicted quantiles for each sample\n",
    "        \"\"\"\n",
    "        # Ensure X is DataFrame\n",
    "        X = X.copy() if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
    "        \n",
    "        # For storing results\n",
    "        results = {\n",
    "            'mean': np.zeros(len(X)),\n",
    "            'median': np.zeros(len(X))\n",
    "        }\n",
    "        \n",
    "        # Add quantile columns\n",
    "        for q in self.quantiles:\n",
    "            q_name = f\"q{int(q*100)}\"\n",
    "            results[q_name] = np.zeros(len(X))\n",
    "        \n",
    "        # If requested, store all predictions\n",
    "        if return_all:\n",
    "            all_predictions = []\n",
    "        \n",
    "        # Get predictions for each sample\n",
    "        for i in range(len(X)):\n",
    "            X_sample = X.iloc[i].values\n",
    "            \n",
    "            # Option 1: Use random forest predictions directly (faster but less accurate)\n",
    "            if len(X) > 100:  # Use faster method for larger datasets\n",
    "                # Get predictions from all trees\n",
    "                tree_preds = np.array([tree.predict(X_sample.reshape(1, -1))[0] \n",
    "                                     for tree in self.rf_model.estimators_])\n",
    "                \n",
    "                # Calculate quantiles and mean\n",
    "                results['mean'][i] = np.mean(tree_preds)\n",
    "                results['median'][i] = np.median(tree_preds)\n",
    "                \n",
    "                for q in self.quantiles:\n",
    "                    q_name = f\"q{int(q*100)}\"\n",
    "                    results[q_name][i] = np.quantile(tree_preds, q)\n",
    "                \n",
    "                if return_all:\n",
    "                    all_predictions.append(tree_preds)\n",
    "            \n",
    "            # Option 2: Use the proper quantile regression forest weighting (more accurate)\n",
    "            else:\n",
    "                # Get weights for training samples\n",
    "                weights = self._get_weights(X_sample)\n",
    "                \n",
    "                # Calculate weighted quantiles\n",
    "                results['mean'][i] = np.average(self.y_train, weights=weights)\n",
    "                results['median'][i] = weighted_quantile(self.y_train.values, 0.5, weights)\n",
    "                \n",
    "                for q in self.quantiles:\n",
    "                    q_name = f\"q{int(q*100)}\"\n",
    "                    results[q_name][i] = weighted_quantile(self.y_train.values, q, weights)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        result_df = pd.DataFrame(results, index=X.index if hasattr(X, 'index') else None)\n",
    "        \n",
    "        # Add prediction intervals if requested\n",
    "        if compute_intervals:\n",
    "            lower_q = min(self.quantiles)\n",
    "            upper_q = max(self.quantiles)\n",
    "            \n",
    "            result_df['prediction_interval'] = result_df[f\"q{int(upper_q*100)}\"] - result_df[f\"q{int(lower_q*100)}\"]\n",
    "            result_df['uncertainty_ratio'] = result_df['prediction_interval'] / result_df['median'].abs()\n",
    "        \n",
    "        # Add additional information if requested\n",
    "        if return_all:\n",
    "            return result_df, all_predictions\n",
    "        else:\n",
    "            return result_df\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Generate point predictions (median).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Features\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        Series\n",
    "            Predicted median values\n",
    "        \"\"\"\n",
    "        # Get quantile predictions\n",
    "        quantile_preds = self.predict_quantiles(X)\n",
    "        \n",
    "        # Return median predictions\n",
    "        return quantile_preds['median']\n",
    "    \n",
    "    def plot_forecast_distribution(self, X, y_true=None, figsize=(12, 6)):\n",
    "        \"\"\"\n",
    "        Plot the forecast distribution.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Features\n",
    "        y_true : array-like, optional\n",
    "            Actual values\n",
    "        figsize : tuple\n",
    "            Figure size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            Forecast distribution plot\n",
    "        \"\"\"\n",
    "        # Get quantile predictions\n",
    "        quantile_preds = self.predict_quantiles(X)\n",
    "        \n",
    "        # Create figure\n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Create x-axis (time or sample index)\n",
    "        x = np.arange(len(X)) if not hasattr(X, 'index') else X.index\n",
    "        \n",
    "        # Shade prediction intervals\n",
    "        plt.fill_between(x, \n",
    "                       quantile_preds[f\"q{int(min(self.quantiles)*100)}\"],\n",
    "                       quantile_preds[f\"q{int(max(self.quantiles)*100)}\"],\n",
    "                       alpha=0.3, label=f\"{int(min(self.quantiles)*100)}-{int(max(self.quantiles)*100)} Percentile\")\n",
    "        \n",
    "        # Shade narrower prediction intervals if we have more quantiles\n",
    "        if len(self.quantiles) > 2:\n",
    "            # Find the 25th and 75th percentile columns if they exist\n",
    "            q25_col = next((col for col in quantile_preds.columns if col == 'q25'), None)\n",
    "            q75_col = next((col for col in quantile_preds.columns if col == 'q75'), None)\n",
    "            \n",
    "            if q25_col and q75_col:\n",
    "                plt.fill_between(x, \n",
    "                               quantile_preds[q25_col],\n",
    "                               quantile_preds[q75_col],\n",
    "                               alpha=0.5, label=\"25-75 Percentile\")\n",
    "        \n",
    "        # Plot median\n",
    "        plt.plot(x, quantile_preds['median'], 'b-', linewidth=2, label='Median Forecast')\n",
    "        \n",
    "        # Plot actuals if provided\n",
    "        if y_true is not None:\n",
    "            plt.plot(x, y_true, 'k-', linewidth=2, label='Actual Values')\n",
    "        \n",
    "        # Add highlights for recessions if available\n",
    "        try:\n",
    "            from pandas_datareader.data import DataReader\n",
    "            \n",
    "            # Get recession data if X has date index\n",
    "            if hasattr(X, 'index') and isinstance(X.index, pd.DatetimeIndex):\n",
    "                start_date = X.index[0]\n",
    "                end_date = X.index[-1]\n",
    "                \n",
    "                try:\n",
    "                    # Get US recession data from FRED\n",
    "                    recession = DataReader('USREC', 'fred', start=start_date, end=end_date)\n",
    "                    \n",
    "                    # Create shaded regions for recessions\n",
    "                    last_date = None\n",
    "                    for date, value in recession.itertuples():\n",
    "                        if value == 1.0:  # Recession period\n",
    "                            if last_date is None:\n",
    "                                last_date = date\n",
    "                        elif last_date is not None:\n",
    "                            # End of recession period\n",
    "                            plt.axvspan(last_date, date, alpha=0.2, color='gray')\n",
    "                            last_date = None\n",
    "                    \n",
    "                    # Handle case where we're still in a recession at the end\n",
    "                    if last_date is not None:\n",
    "                        plt.axvspan(last_date, end_date, alpha=0.2, color='gray')\n",
    "                except:\n",
    "                    pass  # Silently ignore if recession data not available\n",
    "        except ImportError:\n",
    "            pass  # pandas_datareader not available\n",
    "        \n",
    "        # Add grid and legend\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(loc='best')\n",
    "        plt.title('GDP Forecast with Uncertainty Bands')\n",
    "        plt.xlabel('Date' if hasattr(X, 'index') else 'Sample')\n",
    "        plt.ylabel('GDP Growth (%)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return plt.gcf()\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Utility Functions for Quantile Regression\n",
    "#-----------------------------------------------------------------------------\n",
    "def weighted_quantile(values, quantile, weights=None):\n",
    "    \"\"\"\n",
    "    Compute the weighted quantile of a 1D array.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    values : array-like\n",
    "        Input array\n",
    "    quantile : float\n",
    "        Quantile to compute (0.0 to 1.0)\n",
    "    weights : array-like, optional\n",
    "        Weights for each value\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Weighted quantile\n",
    "    \"\"\"\n",
    "    values = np.array(values)\n",
    "    \n",
    "    if weights is None:\n",
    "        # Use standard numpy quantile\n",
    "        return np.quantile(values, quantile)\n",
    "    \n",
    "    # Sort values and weights\n",
    "    sorted_idx = np.argsort(values)\n",
    "    sorted_values = values[sorted_idx]\n",
    "    sorted_weights = weights[sorted_idx]\n",
    "    \n",
    "    # Calculate cumulative sum of weights\n",
    "    cumsum_weights = np.cumsum(sorted_weights)\n",
    "    \n",
    "    # Normalize weights\n",
    "    total_weight = cumsum_weights[-1]\n",
    "    normalized_cumsum = cumsum_weights / total_weight\n",
    "    \n",
    "    # Find index where normalized cumsum exceeds quantile\n",
    "    idx = np.searchsorted(normalized_cumsum, quantile)\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if idx == 0:\n",
    "        return sorted_values[0]\n",
    "    elif idx == len(values):\n",
    "        return sorted_values[-1]\n",
    "    else:\n",
    "        # Interpolate between values if necessary\n",
    "        prev_idx = idx - 1\n",
    "        prev_val = sorted_values[prev_idx]\n",
    "        prev_cumsum = normalized_cumsum[prev_idx]\n",
    "        \n",
    "        val = sorted_values[idx]\n",
    "        cumsum = normalized_cumsum[idx]\n",
    "        \n",
    "        # Linear interpolation\n",
    "        fraction = (quantile - prev_cumsum) / (cumsum - prev_cumsum) if cumsum > prev_cumsum else 0\n",
    "        return prev_val + fraction * (val - prev_val)\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Advanced GDP Forecast System - Main Class\n",
    "#-----------------------------------------------------------------------------\n",
    "class AdvancedGDPForecastSystem:\n",
    "    \"\"\"\n",
    "    Advanced GDP Forecasting System combining state-of-the-art methods.\n",
    "    \n",
    "    This system integrates:\n",
    "    1. Intelligent feature selection for economic time series\n",
    "    2. Neural MIDAS with GRU for mixed-frequency modeling\n",
    "    3. Quantile Regression Forests for uncertainty quantification\n",
    "    \n",
    "    Based on research by Adrian et al. (2022), Bai & Ng (2020), and Babii et al. (2022),\n",
    "    who demonstrated significant improvements in GDP forecasting accuracy and uncertainty\n",
    "    quantification using these approaches.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_features=50, midas_lags=12, n_trees=500, \n",
    "                 selection_method='boruta', quantiles=[0.1, 0.25, 0.5, 0.75, 0.9],\n",
    "                 random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the GDP forecasting system.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        max_features : int\n",
    "            Maximum number of features to select\n",
    "        midas_lags : int\n",
    "            Maximum lag periods for MIDAS\n",
    "        n_trees : int\n",
    "            Number of trees for Quantile Regression Forest\n",
    "        selection_method : str\n",
    "            Feature selection method ('boruta', 'rf_importance', 'mutual_info')\n",
    "        quantiles : list\n",
    "            Quantiles to estimate\n",
    "        random_state : int\n",
    "            Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.max_features = max_features\n",
    "        self.midas_lags = midas_lags\n",
    "        self.n_trees = n_trees\n",
    "        self.selection_method = selection_method\n",
    "        self.quantiles = quantiles\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Initialize components\n",
    "        self.feature_selector = AdvancedFeatureSelector(\n",
    "            method=selection_method,\n",
    "            max_features=max_features,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Try to import TensorFlow for MIDAS-GRU\n",
    "        try:\n",
    "            import tensorflow as tf\n",
    "            self.midas_model = MIDASGRU(\n",
    "                max_lags=midas_lags,\n",
    "                hidden_size=32,\n",
    "                epochs=200,\n",
    "                random_state=random_state\n",
    "            )\n",
    "            self.use_neural_midas = True\n",
    "        except ImportError:\n",
    "            print(\"TensorFlow not available. Will use RandomForest only.\")\n",
    "            self.midas_model = None\n",
    "            self.use_neural_midas = False\n",
    "        \n",
    "        # Initialize Quantile Regression Forest\n",
    "        self.qrf_model = QuantileGDPForecaster(\n",
    "            n_estimators=n_trees,\n",
    "            quantiles=quantiles,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Storage for preprocessed data\n",
    "        self.train_data = {}\n",
    "        self.test_data = {}\n",
    "        self.features_info = None\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, monthly_df, quarterly_df, target_column, \n",
    "            test_size=0.2, use_neural_midas=True, validation_split=0.2):\n",
    "        \"\"\"\n",
    "        Fit the complete GDP forecasting system.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        monthly_df : DataFrame\n",
    "            Monthly economic data\n",
    "        quarterly_df : DataFrame\n",
    "            Quarterly data including GDP\n",
    "        target_column : str\n",
    "            Name of the GDP target column\n",
    "        test_size : float\n",
    "            Proportion of data to hold out for testing\n",
    "        use_neural_midas : bool\n",
    "            Whether to use Neural MIDAS model\n",
    "        validation_split : float\n",
    "            Proportion of training data to use for validation\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self\n",
    "            Fitted model instance\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Advanced GDP Forecasting System - Training Phase\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # 1. Extract target variable\n",
    "        y_full = quarterly_df[target_column]\n",
    "        X_quarterly = quarterly_df.drop(columns=[target_column])\n",
    "        \n",
    "        print(f\"\\n1. Data Overview:\")\n",
    "        print(f\"   - Monthly features: {monthly_df.shape[1]} variables, {monthly_df.shape[0]} time periods\")\n",
    "        print(f\"   - Quarterly features: {X_quarterly.shape[1]} variables, {X_quarterly.shape[0]} time periods\")\n",
    "        print(f\"   - Target variable: {target_column} with {len(y_full)} observations\")\n",
    "        \n",
    "        # 2. Train-test split (time series)\n",
    "        n_test = int(len(y_full) * test_size)\n",
    "        n_train = len(y_full) - n_test\n",
    "        \n",
    "        train_idx = y_full.index[:n_train]\n",
    "        test_idx = y_full.index[n_train:]\n",
    "        \n",
    "        # Split data\n",
    "        y_train = y_full.loc[train_idx]\n",
    "        y_test = y_full.loc[test_idx]\n",
    "        \n",
    "        X_quarterly_train = X_quarterly.loc[train_idx]\n",
    "        X_quarterly_test = X_quarterly.loc[test_idx]\n",
    "        \n",
    "        # Split monthly data based on dates\n",
    "        last_train_date = train_idx[-1]\n",
    "        X_monthly_train = monthly_df[monthly_df.index <= last_train_date]\n",
    "        X_monthly_test = monthly_df[monthly_df.index > last_train_date]\n",
    "        \n",
    "        print(f\"\\n2. Train-Test Split:\")\n",
    "        print(f\"   - Training period: {train_idx[0]} to {train_idx[-1]} ({len(train_idx)} quarters)\")\n",
    "        print(f\"   - Testing period: {test_idx[0]} to {test_idx[-1]} ({len(test_idx)} quarters)\")\n",
    "        \n",
    "        # Store data\n",
    "        self.train_data = {\n",
    "            'monthly': X_monthly_train,\n",
    "            'quarterly': X_quarterly_train,\n",
    "            'target': y_train\n",
    "        }\n",
    "        \n",
    "        self.test_data = {\n",
    "            'monthly': X_monthly_test,\n",
    "            'quarterly': X_quarterly_test,\n",
    "            'target': y_test\n",
    "        }\n",
    "        \n",
    "        # 3. Feature selection\n",
    "        print(\"\\n3. Feature Selection:\")\n",
    "        self.features_info = self.feature_selector.selective_feature_engineering(\n",
    "            X_monthly_train, \n",
    "            pd.concat([y_train, X_quarterly_train], axis=1)\n",
    "        )\n",
    "        \n",
    "        # Apply transformation to get selected features\n",
    "        X_train_selected = self.feature_selector.transform(\n",
    "            X_monthly_train, \n",
    "            X_quarterly_train,\n",
    "            quarterly_dates=y_train.index\n",
    "        )\n",
    "        \n",
    "        print(f\"   - Selected {len(self.features_info['all'])} features in total\")\n",
    "        \n",
    "        # Store the selected training data\n",
    "        self.train_data['selected_features'] = X_train_selected\n",
    "        \n",
    "        # 4. Train models\n",
    "        print(\"\\n4. Model Training:\")\n",
    "        \n",
    "        # Train Quantile Regression Forest\n",
    "        print(\"   - Training Quantile Regression Forest...\")\n",
    "        self.qrf_model.fit(X_train_selected, y_train)\n",
    "        \n",
    "        # Train Neural MIDAS if available and requested\n",
    "        if self.midas_model is not None and use_neural_midas and self.use_neural_midas:\n",
    "            print(\"   - Training Neural MIDAS-GRU model...\")\n",
    "            self.midas_model.fit(\n",
    "                X_monthly_train, \n",
    "                y_train, \n",
    "                X_quarterly=X_train_selected,\n",
    "                validation_split=validation_split\n",
    "            )\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        print(\"\\nTraining completed successfully.\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, monthly_df=None, quarterly_df=None, return_quantiles=True):\n",
    "        \"\"\"\n",
    "        Generate GDP forecasts with the fitted system.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        monthly_df : DataFrame, optional\n",
    "            Monthly data for prediction (uses test data if None)\n",
    "        quarterly_df : DataFrame, optional\n",
    "            Quarterly data for prediction (uses test data if None)\n",
    "        return_quantiles : bool\n",
    "            Whether to return quantile predictions\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Forecast results\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        # Use test data if not provided\n",
    "        if monthly_df is None:\n",
    "            monthly_df = self.test_data['monthly']\n",
    "        \n",
    "        if quarterly_df is None:\n",
    "            quarterly_df = self.test_data['quarterly']\n",
    "        \n",
    "        # Get target dates\n",
    "        if hasattr(quarterly_df, 'index'):\n",
    "            target_dates = quarterly_df.index\n",
    "        else:\n",
    "            # Try to infer from monthly data\n",
    "            all_months = pd.DatetimeIndex(monthly_df.index)\n",
    "            target_dates = pd.DatetimeIndex([date for date in all_months \n",
    "                                          if date.month in [3, 6, 9, 12] and \n",
    "                                          date.day >= 28])\n",
    "        \n",
    "        # Transform data using selected features\n",
    "        X_selected = self.feature_selector.transform(\n",
    "            monthly_df,\n",
    "            quarterly_df,\n",
    "            quarterly_dates=target_dates\n",
    "        )\n",
    "        \n",
    "        # Generate forecasts\n",
    "        results = {}\n",
    "        \n",
    "        try:\n",
    "            # QRF prediction\n",
    "            if return_quantiles:\n",
    "                try:\n",
    "                    qrf_pred = self.qrf_model.predict_quantiles(X_selected)\n",
    "                    results['qrf'] = qrf_pred\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: QRF quantile prediction failed - {e}\")\n",
    "                    # Fall back to point prediction\n",
    "                    try:\n",
    "                        pred = self.qrf_model.predict(X_selected)\n",
    "                        results['qrf'] = pd.DataFrame({'median': pred}, index=X_selected.index)\n",
    "                    except Exception as e2:\n",
    "                        print(f\"Warning: QRF point prediction also failed - {e2}\")\n",
    "            else:\n",
    "                pred = self.qrf_model.predict(X_selected)\n",
    "                results['qrf'] = pd.DataFrame({'median': pred}, index=X_selected.index)\n",
    "            \n",
    "            # Neural MIDAS prediction if available\n",
    "            if self.midas_model is not None and self.use_neural_midas:\n",
    "                try:\n",
    "                    midas_pred = self.midas_model.predict(\n",
    "                        monthly_df, \n",
    "                        X_quarterly=X_selected,\n",
    "                        quarterly_dates=target_dates\n",
    "                    )\n",
    "                    \n",
    "                    # Ensure midas_pred is a DataFrame for consistency\n",
    "                    if isinstance(midas_pred, pd.Series):\n",
    "                        midas_pred = pd.DataFrame({'median': midas_pred})\n",
    "                    \n",
    "                    results['midas_gru'] = midas_pred\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: MIDAS-GRU prediction failed - {e}\")\n",
    "            \n",
    "            # Create ensemble prediction if both models are available\n",
    "            if 'midas_gru' in results and 'qrf' in results:\n",
    "                try:\n",
    "                    # Get qrf median\n",
    "                    if isinstance(results['qrf'], pd.DataFrame) and 'median' in results['qrf'].columns:\n",
    "                        qrf_median = results['qrf']['median']\n",
    "                    elif isinstance(results['qrf'], pd.Series):\n",
    "                        qrf_median = results['qrf']\n",
    "                    else:\n",
    "                        qrf_median = results['qrf'].iloc[:, 0]\n",
    "                    \n",
    "                    # Get midas median\n",
    "                    if isinstance(results['midas_gru'], pd.DataFrame) and 'median' in results['midas_gru'].columns:\n",
    "                        midas_median = results['midas_gru']['median']\n",
    "                    elif isinstance(results['midas_gru'], pd.Series):\n",
    "                        midas_median = results['midas_gru']\n",
    "                    else:\n",
    "                        midas_median = results['midas_gru'].iloc[:, 0]\n",
    "                    \n",
    "                    # Align indices\n",
    "                    common_idx = qrf_median.index.intersection(midas_median.index)\n",
    "                    if len(common_idx) > 0:\n",
    "                        # Simple average ensemble\n",
    "                        ensemble_pred = pd.DataFrame({\n",
    "                            'median': (qrf_median.loc[common_idx] + midas_median.loc[common_idx]) / 2\n",
    "                        }, index=common_idx)\n",
    "                        \n",
    "                        # If quantiles are available, add uncertainty from QRF\n",
    "                        if return_quantiles and isinstance(results['qrf'], pd.DataFrame):\n",
    "                            for col in results['qrf'].columns:\n",
    "                                if col not in ['mean', 'median'] and col.startswith('q'):\n",
    "                                    ensemble_pred[col] = results['qrf'][col].loc[common_idx]\n",
    "                        \n",
    "                        results['ensemble'] = ensemble_pred\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Ensemble prediction failed - {e}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in prediction process: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate(self, predictions=None, actuals=None, rolling_window=8):\n",
    "        \"\"\"\n",
    "        Evaluate forecast performance.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        predictions : dict, optional\n",
    "            Dictionary of predictions\n",
    "        actuals : Series, optional\n",
    "            Actual values\n",
    "        rolling_window : int\n",
    "            Window size for rolling metrics\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Evaluation metrics\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        # Get predictions if not provided\n",
    "        if predictions is None:\n",
    "            predictions = self.predict(return_quantiles=True)\n",
    "        \n",
    "        # Get actuals if not provided\n",
    "        if actuals is None:\n",
    "            actuals = self.test_data['target']\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        metrics = {}\n",
    "        \n",
    "        for model_name, preds in predictions.items():\n",
    "            # Get point prediction based on type of prediction object\n",
    "            if isinstance(preds, pd.DataFrame) and 'median' in preds.columns:\n",
    "                point_pred = preds['median']\n",
    "            elif isinstance(preds, pd.Series):\n",
    "                point_pred = preds\n",
    "            elif isinstance(preds, pd.DataFrame):\n",
    "                # Use the first column as fallback\n",
    "                point_pred = preds.iloc[:, 0]\n",
    "            else:\n",
    "                print(f\"Warning: Unsupported prediction type for {model_name}: {type(preds)}\")\n",
    "                continue\n",
    "            \n",
    "            # Align predictions with actuals\n",
    "            common_idx = actuals.index.intersection(point_pred.index)\n",
    "            if len(common_idx) == 0:\n",
    "                print(f\"Warning: No common dates between predictions and actuals for {model_name}\")\n",
    "                continue\n",
    "                \n",
    "            y_true = actuals.loc[common_idx]\n",
    "            y_pred = point_pred.loc[common_idx]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            model_metrics = {\n",
    "                'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "                'mae': np.mean(np.abs(y_true - y_pred)),\n",
    "                'r2': r2_score(y_true, y_pred),\n",
    "            }\n",
    "            \n",
    "            # Calculate directional accuracy - FIXED IMPLEMENTATION\n",
    "            # Drop the first observation since diff() creates NaN\n",
    "            y_true_diff = y_true.diff().dropna()\n",
    "            \n",
    "            # Get corresponding predictions for the same periods\n",
    "            matching_idx = y_true_diff.index.intersection(y_pred.index)\n",
    "            if len(matching_idx) > 0:\n",
    "                y_pred_for_diff = y_pred.loc[matching_idx]\n",
    "                \n",
    "                # Calculate directional prediction accuracy\n",
    "                actual_direction = (y_true_diff > 0).astype(int)\n",
    "                pred_direction = (y_pred_for_diff > y_true.shift(1).loc[matching_idx]).astype(int)\n",
    "                \n",
    "                dir_acc = np.mean(actual_direction == pred_direction)\n",
    "                model_metrics['direction_accuracy'] = dir_acc\n",
    "                \n",
    "                # Calculate separate accuracy for up and down movements\n",
    "                up_idx = actual_direction == 1\n",
    "                down_idx = actual_direction == 0\n",
    "                \n",
    "                if up_idx.any():\n",
    "                    up_acc = np.mean(pred_direction[up_idx] == actual_direction[up_idx])\n",
    "                    model_metrics['up_direction_accuracy'] = up_acc\n",
    "                \n",
    "                if down_idx.any():\n",
    "                    down_acc = np.mean(pred_direction[down_idx] == actual_direction[down_idx])\n",
    "                    model_metrics['down_direction_accuracy'] = down_acc\n",
    "                \n",
    "                # Count of correct predictions by direction\n",
    "                model_metrics['correct_up_predictions'] = np.sum((actual_direction == 1) & (pred_direction == 1))\n",
    "                model_metrics['correct_down_predictions'] = np.sum((actual_direction == 0) & (pred_direction == 0))\n",
    "                model_metrics['total_up_movements'] = np.sum(actual_direction == 1)\n",
    "                model_metrics['total_down_movements'] = np.sum(actual_direction == 0)\n",
    "            else:\n",
    "                model_metrics['direction_accuracy'] = np.nan\n",
    "            \n",
    "            # Calculate rolling metrics if enough data\n",
    "            if len(y_true) > rolling_window:\n",
    "                rolling_rmse = []\n",
    "                rolling_mae = []\n",
    "                rolling_dir_acc = []\n",
    "                \n",
    "                for i in range(len(y_true) - rolling_window + 1):\n",
    "                    window_true = y_true.iloc[i:i+rolling_window]\n",
    "                    window_pred = y_pred.iloc[i:i+rolling_window]\n",
    "                    \n",
    "                    # Calculate metrics for this window\n",
    "                    rmse = np.sqrt(mean_squared_error(window_true, window_pred))\n",
    "                    mae = np.mean(np.abs(window_true - window_pred))\n",
    "                    \n",
    "                    # Direction accuracy\n",
    "                    window_dir_true = np.sign(window_true.diff().fillna(0))\n",
    "                    window_dir_pred = np.sign(window_pred.diff().fillna(0))\n",
    "                    \n",
    "                    nonzero_mask = window_dir_true != 0\n",
    "                    if nonzero_mask.any():\n",
    "                        window_dir_acc = np.mean(window_dir_true[nonzero_mask] == window_dir_pred[nonzero_mask])\n",
    "                    else:\n",
    "                        window_dir_acc = np.nan\n",
    "                    \n",
    "                    rolling_rmse.append(rmse)\n",
    "                    rolling_mae.append(mae)\n",
    "                    rolling_dir_acc.append(window_dir_acc)\n",
    "                \n",
    "                # Add to metrics\n",
    "                model_metrics['rolling_rmse'] = pd.Series(\n",
    "                    rolling_rmse, \n",
    "                    index=y_true.index[rolling_window-1:len(rolling_rmse)+rolling_window-1]\n",
    "                )\n",
    "                \n",
    "                model_metrics['rolling_mae'] = pd.Series(\n",
    "                    rolling_mae, \n",
    "                    index=y_true.index[rolling_window-1:len(rolling_mae)+rolling_window-1]\n",
    "                )\n",
    "                \n",
    "                model_metrics['rolling_dir_acc'] = pd.Series(\n",
    "                    rolling_dir_acc,\n",
    "                    index=y_true.index[rolling_window-1:len(rolling_dir_acc)+rolling_window-1]\n",
    "                )\n",
    "            \n",
    "            # Calculate coverage metrics if quantiles are available\n",
    "            if isinstance(preds, pd.DataFrame) and any(col.startswith('q') for col in preds.columns):\n",
    "                # Get lower and upper quantiles\n",
    "                lower_q = min(self.quantiles)\n",
    "                upper_q = max(self.quantiles)\n",
    "                \n",
    "                lower_col = f\"q{int(lower_q*100)}\"\n",
    "                upper_col = f\"q{int(upper_q*100)}\"\n",
    "                \n",
    "                if lower_col in preds.columns and upper_col in preds.columns:\n",
    "                    lower_pred = preds[lower_col].loc[common_idx]\n",
    "                    upper_pred = preds[upper_col].loc[common_idx]\n",
    "                    \n",
    "                    # Calculate coverage\n",
    "                    coverage = np.mean((y_true >= lower_pred) & (y_true <= upper_pred))\n",
    "                    model_metrics['prediction_interval_coverage'] = coverage\n",
    "                    \n",
    "                    # Calculate interval width\n",
    "                    interval_width = np.mean(upper_pred - lower_pred)\n",
    "                    model_metrics['prediction_interval_width'] = interval_width\n",
    "                    \n",
    "                    # Calculate interval efficiency (coverage / width)\n",
    "                    if interval_width > 0:\n",
    "                        model_metrics['interval_efficiency'] = coverage / interval_width\n",
    "                    else:\n",
    "                        model_metrics['interval_efficiency'] = np.nan\n",
    "            \n",
    "            # Add to metrics dictionary\n",
    "            metrics[model_name] = model_metrics\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def plot_forecasts(self, predictions=None, actuals=None, figsize=(12, 8), include_quantiles=True):\n",
    "        \"\"\"\n",
    "        Plot forecast results.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        predictions : dict, optional\n",
    "            Dictionary of predictions\n",
    "        actuals : Series, optional\n",
    "            Actual values\n",
    "        figsize : tuple\n",
    "            Figure size\n",
    "        include_quantiles : bool\n",
    "            Whether to include quantile bands\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            Forecast plot\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        # Get predictions if not provided\n",
    "        if predictions is None:\n",
    "            predictions = self.predict(return_quantiles=include_quantiles)\n",
    "        \n",
    "        # Get actuals if not provided\n",
    "        if actuals is None:\n",
    "            actuals = self.test_data['target']\n",
    "        \n",
    "        # Create figure\n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Plot actual values\n",
    "        plt.plot(actuals.index, actuals, 'k-', linewidth=2, label='Actual GDP')\n",
    "        \n",
    "        # Plot predictions for each model\n",
    "        colors = plt.cm.tab10.colors\n",
    "        \n",
    "        for i, (model_name, preds) in enumerate(predictions.items()):\n",
    "            color = colors[i % len(colors)]\n",
    "            \n",
    "            # Get point prediction\n",
    "            if 'median' in preds.columns:\n",
    "                point_pred = preds['median']\n",
    "            elif isinstance(preds, pd.Series):\n",
    "                point_pred = preds\n",
    "            else:\n",
    "                # Use the first column as fallback\n",
    "                point_pred = preds.iloc[:, 0]\n",
    "            \n",
    "            # Plot point prediction\n",
    "            plt.plot(point_pred.index, point_pred, 'o-', color=color, linewidth=1.5, label=f'{model_name}')\n",
    "            \n",
    "            # Add quantile bands if available and requested\n",
    "            if include_quantiles and isinstance(preds, pd.DataFrame) and any(col.startswith('q') for col in preds.columns):\n",
    "                # Get quantiles\n",
    "                lower_q = min(self.quantiles)\n",
    "                upper_q = max(self.quantiles)\n",
    "                \n",
    "                lower_col = f\"q{int(lower_q*100)}\"\n",
    "                upper_col = f\"q{int(upper_q*100)}\"\n",
    "                \n",
    "                if lower_col in preds.columns and upper_col in preds.columns:\n",
    "                    plt.fill_between(\n",
    "                        point_pred.index,\n",
    "                        preds[lower_col],\n",
    "                        preds[upper_col],\n",
    "                        color=color,\n",
    "                        alpha=0.2,\n",
    "                        label=f'{model_name} {int(lower_q*100)}-{int(upper_q*100)} Percentile'\n",
    "                    )\n",
    "        \n",
    "        # Add recession shading if available\n",
    "        try:\n",
    "            from pandas_datareader.data import DataReader\n",
    "            from pandas_datareader._utils import RemoteDataError\n",
    "            \n",
    "            try:\n",
    "                # Get US recession data from FRED\n",
    "                all_dates = pd.DatetimeIndex(sorted(list(set(actuals.index) | \n",
    "                                               set(predictions[list(predictions.keys())[0]].index))))\n",
    "                                               \n",
    "                start_date = all_dates[0]\n",
    "                end_date = all_dates[-1]\n",
    "                \n",
    "                recession = DataReader('USREC', 'fred', start=start_date, end=end_date)\n",
    "                \n",
    "                # Create shaded regions for recessions\n",
    "                last_date = None\n",
    "                for date, value in recession.itertuples():\n",
    "                    if value == 1.0:  # Recession period\n",
    "                        if last_date is None:\n",
    "                            last_date = date\n",
    "                    elif last_date is not None:\n",
    "                        # End of recession period\n",
    "                        plt.axvspan(last_date, date, alpha=0.2, color='gray')\n",
    "                        last_date = None\n",
    "                \n",
    "                # Handle case where we're still in a recession at the end\n",
    "                if last_date is not None:\n",
    "                    plt.axvspan(last_date, all_dates[-1], alpha=0.2, color='gray')\n",
    "            \n",
    "            except RemoteDataError:\n",
    "                print(\"Could not retrieve recession data from FRED\")\n",
    "        \n",
    "        except ImportError:\n",
    "            print(\"pandas_datareader not available for recession shading\")\n",
    "        \n",
    "        # Add legend, grid, labels, etc.\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('GDP Growth (%)')\n",
    "        plt.title('GDP Growth: Actual vs Predicted')\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Format y-axis to show percentage\n",
    "        plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.1f}%'))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return plt.gcf()\n",
    "    \n",
    "    def plot_feature_importance(self, figsize=(10, 12)):\n",
    "        \"\"\"\n",
    "        Plot feature importance.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        figsize : tuple\n",
    "            Figure size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            Feature importance plot\n",
    "        \"\"\"\n",
    "        return self.feature_selector.plot_feature_importance(figsize=figsize)\n",
    "    \n",
    "    def economic_value_of_forecasts(self, predictions=None, actuals=None, risk_aversion=5):\n",
    "        \"\"\"\n",
    "        Calculate the economic value of GDP forecasts.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        predictions : dict, optional\n",
    "            Dictionary of predictions\n",
    "        actuals : Series, optional\n",
    "            Actual values\n",
    "        risk_aversion : float\n",
    "            Coefficient of risk aversion\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Economic performance metrics\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        # Get predictions if not provided\n",
    "        if predictions is None:\n",
    "            predictions = self.predict(return_quantiles=True)\n",
    "        \n",
    "        # Get actuals if not provided\n",
    "        if actuals is None:\n",
    "            actuals = self.test_data['target']\n",
    "        \n",
    "        # Initialize results\n",
    "        econ_value = {}\n",
    "        \n",
    "        # Calculate metrics for each model\n",
    "        for model_name, preds in predictions.items():\n",
    "            # Get point prediction\n",
    "            if 'median' in preds.columns:\n",
    "                point_pred = preds['median']\n",
    "            elif isinstance(preds, pd.Series):\n",
    "                point_pred = preds\n",
    "            else:\n",
    "                # Use the first column as fallback\n",
    "                point_pred = preds.iloc[:, 0]\n",
    "            \n",
    "            # Align predictions with actuals\n",
    "            common_idx = actuals.index.intersection(point_pred.index)\n",
    "            y_true = actuals.loc[common_idx]\n",
    "            y_pred = point_pred.loc[common_idx]\n",
    "            \n",
    "            # Calculate directional accuracy\n",
    "            actual_dir = np.sign(y_true.diff().fillna(0))\n",
    "            pred_dir = np.sign(y_pred.diff().fillna(0))\n",
    "            \n",
    "            # Ignore zero changes\n",
    "            nonzero_mask = actual_dir != 0\n",
    "            if nonzero_mask.any():\n",
    "                dir_acc = np.mean(actual_dir[nonzero_mask] == pred_dir[nonzero_mask])\n",
    "            else:\n",
    "                dir_acc = np.nan\n",
    "            \n",
    "            # Simulate investment strategy based on forecasts\n",
    "            returns = []\n",
    "            \n",
    "            for t in range(len(y_true) - 1):  # -1 because we need next period's actual\n",
    "                # Use forecast to decide allocation\n",
    "                forecast = y_pred.iloc[t]\n",
    "                \n",
    "                # Simple rule: if forecast > 0, invest proportionally to forecast\n",
    "                if forecast > 0:\n",
    "                    allocation = min(1.0, forecast / 2.0)  # Cap allocation at 100%\n",
    "                else:\n",
    "                    allocation = 0.0\n",
    "                    \n",
    "                # Calculate realized return\n",
    "                realized_growth = y_true.iloc[t+1]\n",
    "                period_return = allocation * realized_growth\n",
    "                returns.append(period_return)\n",
    "            \n",
    "            # Calculate performance metrics\n",
    "            returns = np.array(returns)\n",
    "            mean_return = np.mean(returns)\n",
    "            vol_return = np.std(returns)\n",
    "            \n",
    "            # Calculate Sharpe ratio if variance is positive\n",
    "            sharpe_ratio = mean_return / vol_return if vol_return > 0 else 0\n",
    "            \n",
    "            # Calculate utility\n",
    "            utility = mean_return - 0.5 * risk_aversion * vol_return**2\n",
    "            \n",
    "            # Store results\n",
    "            econ_value[model_name] = {\n",
    "                'mean_return': mean_return,\n",
    "                'volatility': vol_return,\n",
    "                'sharpe_ratio': sharpe_ratio,\n",
    "                'utility': utility,\n",
    "                'directional_accuracy': dir_acc\n",
    "            }\n",
    "        \n",
    "        return econ_value\n",
    "    \n",
    "    def generate_report(self, output_file=None, predictions=None, actuals=None):\n",
    "        \"\"\"\n",
    "        Generate comprehensive evaluation report.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_file : str, optional\n",
    "            Path to save report\n",
    "        predictions : dict, optional\n",
    "            Dictionary of predictions\n",
    "        actuals : Series, optional\n",
    "            Actual values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            Report content\n",
    "        \"\"\"\n",
    "        # Get predictions if not provided\n",
    "        if predictions is None:\n",
    "            predictions = self.predict(return_quantiles=True)\n",
    "        \n",
    "        # Get actuals if not provided\n",
    "        if actuals is None:\n",
    "            actuals = self.test_data['target']\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = self.evaluate(predictions, actuals)\n",
    "        econ_value = self.economic_value_of_forecasts(predictions, actuals)\n",
    "        \n",
    "        # Start building report\n",
    "        report = \"# Advanced GDP Forecasting System Evaluation Report\\n\\n\"\n",
    "        report += f\"Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\\n\\n\"\n",
    "        \n",
    "        # Add model summary\n",
    "        report += \"## Models Evaluated\\n\\n\"\n",
    "        report += f\"Number of models: {len(predictions)}\\n\"\n",
    "        report += f\"Evaluation period: {actuals.index[0]} to {actuals.index[-1]}\\n\"\n",
    "        report += f\"Number of observations: {len(actuals)}\\n\\n\"\n",
    "        \n",
    "        # Add performance metrics table\n",
    "        report += \"## Statistical Performance Metrics\\n\\n\"\n",
    "        report += \"| Model | RMSE | MAE | R² | Direction Accuracy |\\n\"\n",
    "        report += \"|-------|------|-----|----|-----------------|\\n\"\n",
    "        \n",
    "        for model_name, model_metrics in metrics.items():\n",
    "            report += (\n",
    "                f\"| {model_name} | \"\n",
    "                f\"{model_metrics['rmse']:.4f} | \"\n",
    "                f\"{model_metrics['mae']:.4f} | \"\n",
    "                f\"{model_metrics['r2']:.4f} | \"\n",
    "                f\"{model_metrics['direction_accuracy']:.4f} |\\n\"\n",
    "            )\n",
    "        \n",
    "        report += \"\\n\"\n",
    "        \n",
    "        # Add economic value table\n",
    "        report += \"## Economic Value Metrics\\n\\n\"\n",
    "        report += \"| Model | Mean Return | Volatility | Sharpe Ratio | Utility |\\n\"\n",
    "        report += \"|-------|-------------|------------|--------------|--------|\\n\"\n",
    "        \n",
    "        for model_name, model_metrics in econ_value.items():\n",
    "            report += (\n",
    "                f\"| {model_name} | \"\n",
    "                f\"{model_metrics['mean_return']:.4f} | \"\n",
    "                f\"{model_metrics['volatility']:.4f} | \"\n",
    "                f\"{model_metrics['sharpe_ratio']:.4f} | \"\n",
    "                f\"{model_metrics['utility']:.4f} |\\n\"\n",
    "            )\n",
    "        \n",
    "        report += \"\\n\"\n",
    "        \n",
    "        # Add prediction interval coverage if available\n",
    "        has_intervals = False\n",
    "        for model_metrics in metrics.values():\n",
    "            if 'prediction_interval_coverage' in model_metrics:\n",
    "                has_intervals = True\n",
    "                break\n",
    "        \n",
    "        if has_intervals:\n",
    "            report += \"## Uncertainty Quantification Metrics\\n\\n\"\n",
    "            report += \"| Model | PI Coverage | PI Width | Interval Efficiency |\\n\"\n",
    "            report += \"|-------|-------------|----------|---------------------|\\n\"\n",
    "            \n",
    "            for model_name, model_metrics in metrics.items():\n",
    "                if 'prediction_interval_coverage' in model_metrics:\n",
    "                    report += (\n",
    "                        f\"| {model_name} | \"\n",
    "                        f\"{model_metrics['prediction_interval_coverage']:.4f} | \"\n",
    "                        f\"{model_metrics['prediction_interval_width']:.4f} | \"\n",
    "                        f\"{model_metrics['interval_efficiency']:.4f} |\\n\"\n",
    "                    )\n",
    "            \n",
    "            report += \"\\n\"\n",
    "        \n",
    "        # Add feature importance section\n",
    "        if self.features_info is not None:\n",
    "            report += \"## Feature Importance\\n\\n\"\n",
    "            report += \"### Top 10 Features\\n\\n\"\n",
    "            \n",
    "            top_features = self.feature_selector.feature_importance.sort_values(ascending=False).head(10)\n",
    "            \n",
    "            for feature, importance in top_features.items():\n",
    "                report += f\"- **{feature}**: {importance:.4f}\\n\"\n",
    "            \n",
    "            report += \"\\n\"\n",
    "        \n",
    "        # Add conclusion based on metrics\n",
    "        report += \"## Conclusion\\n\\n\"\n",
    "        \n",
    "        # Find best model by RMSE\n",
    "        best_rmse_model = min(metrics.items(), key=lambda x: x[1]['rmse'])[0]\n",
    "        \n",
    "        # Find best model by direction accuracy\n",
    "        best_dir_model = max(metrics.items(), key=lambda x: x[1]['direction_accuracy'])[0]\n",
    "        \n",
    "        # Find best model by economic utility\n",
    "        best_econ_model = max(econ_value.items(), key=lambda x: x[1]['utility'])[0]\n",
    "        \n",
    "        report += f\"- Based on **RMSE**, the best performing model is **{best_rmse_model}** with RMSE of {metrics[best_rmse_model]['rmse']:.4f}.\\n\"\n",
    "        report += f\"- Based on **directional accuracy**, the best performing model is **{best_dir_model}** with accuracy of {metrics[best_dir_model]['direction_accuracy']:.2%}.\\n\"\n",
    "        report += f\"- Based on **economic utility**, the best performing model is **{best_econ_model}** with utility of {econ_value[best_econ_model]['utility']:.4f}.\\n\\n\"\n",
    "        \n",
    "        # Add summary recommendation\n",
    "        if best_rmse_model == best_dir_model and best_rmse_model == best_econ_model:\n",
    "            report += f\"The **{best_rmse_model}** model outperforms across all metrics and is recommended for GDP forecasting.\"\n",
    "        else:\n",
    "            report += \"Different models excel at different metrics.\\n\\n\"\n",
    "            \n",
    "            if has_intervals:\n",
    "                # Find model with best coverage\n",
    "                best_coverage_model = max(\n",
    "                    [(model, metrics[model]['prediction_interval_coverage']) \n",
    "                     for model in metrics \n",
    "                     if 'prediction_interval_coverage' in metrics[model]],\n",
    "                    key=lambda x: x[1]\n",
    "                )[0]\n",
    "                \n",
    "                report += f\"For point forecasts, **{best_rmse_model}** is recommended, while **{best_coverage_model}** provides the most reliable uncertainty estimates.\"\n",
    "            else:\n",
    "                report += f\"For overall performance, **{best_econ_model}** is recommended based on economic utility.\"\n",
    "        \n",
    "        # Save report to file if specified\n",
    "        if output_file:\n",
    "            os.makedirs(os.path.dirname(os.path.abspath(output_file)), exist_ok=True)\n",
    "            with open(output_file, 'w') as f:\n",
    "                f.write(report)\n",
    "            print(f\"Report saved to {output_file}\")\n",
    "        \n",
    "        return report\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Main Workflow Function\n",
    "#-----------------------------------------------------------------------------\n",
    "def run_advanced_gdp_forecast_workflow(\n",
    "    data_folder,\n",
    "    output_folder='./output',\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    test_size=0.2,\n",
    "    max_features=50,\n",
    "    midas_lags=12,\n",
    "    n_trees=500,\n",
    "    selection_method='rf_importance',\n",
    "    random_state=42,\n",
    "    save_models=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a complete GDP forecasting workflow with advanced methods.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_folder : str\n",
    "        Path to the data folder\n",
    "    output_folder : str\n",
    "        Path to the output folder\n",
    "    start_date : str, optional\n",
    "        Start date for analysis\n",
    "    end_date : str, optional\n",
    "        End date for analysis\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    max_features : int\n",
    "        Maximum number of features to select\n",
    "    midas_lags : int\n",
    "        Maximum lag periods for MIDAS\n",
    "    n_trees : int\n",
    "        Number of trees for Quantile Regression Forest\n",
    "    selection_method : str\n",
    "        Feature selection method ('boruta', 'rf_importance', 'mutual_info')\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    save_models : bool\n",
    "        Whether to save the models\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (forecast_system, predictions, metrics)\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Set up logging\n",
    "    log_file = os.path.join(output_folder, 'advanced_workflow_log.txt')\n",
    "    def log(message):\n",
    "        \"\"\"Log message to file and print to console.\"\"\"\n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(f\"{pd.Timestamp.now()}: {message}\\n\")\n",
    "        print(message)\n",
    "    \n",
    "    log(\"=\" * 80)\n",
    "    log(f\"Starting Advanced GDP Forecasting Workflow at {pd.Timestamp.now()}\")\n",
    "    log(\"=\" * 80)\n",
    "    \n",
    "    # 1. Configuration\n",
    "    log(\"\\n1. Setting up configuration...\")\n",
    "    \n",
    "    # Monthly data configuration\n",
    "    monthly_config = {\n",
    "        'monthly': {\n",
    "            'files': {\n",
    "                'CPI_mon_monthly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1955-01-01'\n",
    "                },\n",
    "                'Unemployment_monthly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'diff']},\n",
    "                    'start_date': '1948-01-01'\n",
    "                },\n",
    "                'InterestRate_monthly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'diff']},\n",
    "                    'start_date': '1954-01-01'\n",
    "                },\n",
    "                'HousingStarts_monthly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1959-01-01'\n",
    "                },\n",
    "                'Heavy_Truck_Sales.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1967-01-01'\n",
    "                },\n",
    "                'Manufacturing_Production_Motor_and_Vehicle_Parts.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1972-01-01'\n",
    "                },\n",
    "                'Consumer_Confidence.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'diff']},\n",
    "                    'start_date': '1960-01-01'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Quarterly data configuration\n",
    "    quarterly_config = {\n",
    "        'quarterly': {\n",
    "            'files': {\n",
    "                'GDP_quaterly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1947-01-01'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Combine configurations\n",
    "    data_config = {}\n",
    "    data_config.update(monthly_config)\n",
    "    data_config.update(quarterly_config)\n",
    "    \n",
    "    log(f\"Configuration set up with {len(monthly_config['monthly']['files'])} monthly files, \" +\n",
    "        f\"{len(quarterly_config['quarterly']['files'])} quarterly files\")\n",
    "    \n",
    "    # 2. Data Preprocessing\n",
    "    log(\"\\n2. Data Preprocessing...\")\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    #from MultiFrequencyPreprocessor import MultiFrequencyPreprocessor  # Import from original codebase\n",
    "    \n",
    "    preprocessor = MultiFrequencyPreprocessor(data_folder)\n",
    "    preprocessor.set_config(data_config)\n",
    "    \n",
    "    # Set date range if provided\n",
    "    if start_date is not None:\n",
    "        preprocessor.set_date_range(start_date=start_date)\n",
    "    if end_date is not None:\n",
    "        preprocessor.set_date_range(end_date=end_date)\n",
    "    \n",
    "    # Process data\n",
    "    monthly_df = preprocessor.process_frequency_data('monthly')\n",
    "    trace_nans(\"Raw monthly data\", monthly_df)\n",
    "    quarterly_df = preprocessor.process_frequency_data('quarterly')\n",
    "    trace_nans(\"Raw quarterly data\", quarterly_df)\n",
    "    \n",
    "    log(f\"Processed data: monthly={monthly_df.shape}, quarterly={quarterly_df.shape}\")\n",
    "    \n",
    "    # 3. Advanced GDP Forecasting System\n",
    "    log(\"\\n3. Initializing Advanced GDP Forecasting System...\")\n",
    "    \n",
    "    # Initialize the forecasting system\n",
    "    forecast_system = AdvancedGDPForecastSystem(\n",
    "        max_features=max_features,\n",
    "        midas_lags=midas_lags,\n",
    "        n_trees=n_trees,\n",
    "        selection_method=selection_method,\n",
    "        quantiles=[0.1, 0.25, 0.5, 0.75, 0.9],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Set GDP target column\n",
    "    gdp_target_column = 'GDP_quaterly_Value_pct_change'\n",
    "    \n",
    "    # Fit the system\n",
    "    forecast_system.fit(\n",
    "        monthly_df=monthly_df,\n",
    "        quarterly_df=quarterly_df,\n",
    "        target_column=gdp_target_column,\n",
    "        test_size=test_size\n",
    "    )\n",
    "    \n",
    "    # 4. Generate forecasts\n",
    "    log(\"\\n4. Generating GDP forecasts...\")\n",
    "    \n",
    "    predictions = forecast_system.predict(return_quantiles=True)\n",
    "    \n",
    "    # 5. Evaluate forecasts\n",
    "    log(\"\\n5. Evaluating forecast performance...\")\n",
    "    \n",
    "    metrics = forecast_system.evaluate(predictions)\n",
    "    econ_value = forecast_system.economic_value_of_forecasts(predictions)\n",
    "    \n",
    "    # Print key metrics\n",
    "    log(\"\\nKey Statistical Metrics:\")\n",
    "    log(\"-\" * 80)\n",
    "    log(f\"{'Model':<15} {'RMSE':>10} {'MAE':>10} {'Dir Acc':>10}\")\n",
    "    log(\"-\" * 80)\n",
    "    \n",
    "    for model_name, model_metrics in metrics.items():\n",
    "        log(f\"{model_name:<15} {model_metrics['rmse']:>10.4f} {model_metrics['mae']:>10.4f} {model_metrics['direction_accuracy']:>10.4f}\")\n",
    "    \n",
    "    log(\"\\nEconomic Value Metrics:\")\n",
    "    log(\"-\" * 80)\n",
    "    log(f\"{'Model':<15} {'Return':>10} {'Sharpe':>10} {'Utility':>10}\")\n",
    "    log(\"-\" * 80)\n",
    "    \n",
    "    for model_name, model_metrics in econ_value.items():\n",
    "        log(f\"{model_name:<15} {model_metrics['mean_return']:>10.4f} {model_metrics['sharpe_ratio']:>10.4f} {model_metrics['utility']:>10.4f}\")\n",
    "    \n",
    "    # 6. Generate plots\n",
    "    log(\"\\n6. Creating visualization plots...\")\n",
    "    \n",
    "    # Forecast plot\n",
    "    forecast_plot = forecast_system.plot_forecasts(predictions)\n",
    "    forecast_plot.savefig(os.path.join(output_folder, 'advanced_gdp_forecasts.png'))\n",
    "    plt.close(forecast_plot)\n",
    "    log(f\"Forecast plot saved to {os.path.join(output_folder, 'advanced_gdp_forecasts.png')}\")\n",
    "    \n",
    "    # Feature importance plot\n",
    "    importance_plot = forecast_system.plot_feature_importance()\n",
    "    importance_plot.savefig(os.path.join(output_folder, 'feature_importance.png'))\n",
    "    plt.close(importance_plot)\n",
    "    log(f\"Feature importance plot saved to {os.path.join(output_folder, 'feature_importance.png')}\")\n",
    "    \n",
    "    # 7. Generate report\n",
    "    log(\"\\n7. Generating comprehensive report...\")\n",
    "    \n",
    "    report_path = os.path.join(output_folder, 'advanced_gdp_forecast_report.md')\n",
    "    report = forecast_system.generate_report(report_path, predictions)\n",
    "    log(f\"Comprehensive report saved to {report_path}\")\n",
    "    \n",
    "    # 8. Save models if requested\n",
    "    #if save_models:\n",
    "    #    log(\"\\n8. Saving models...\")\n",
    "    #    \n",
    "    #    model_path = os.path.join(output_folder, 'advanced_forecast_system.pkl')\n",
    "    #    with open(model_path, 'wb') as f:\n",
    "    #        pickle.dump(forecast_system, f)\n",
    "    #    log(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    # 9. Conclusion\n",
    "    log(\"\\n9. Workflow completed\")\n",
    "    log(\"=\" * 80)\n",
    "    log(f\"Advanced GDP Forecasting Workflow completed at {pd.Timestamp.now()}\")\n",
    "    log(\"=\" * 80)\n",
    "    \n",
    "    return forecast_system, predictions, metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set parameters\n",
    "    DATA_FOLDER = \"./Project_Data\"\n",
    "    OUTPUT_FOLDER = \"./output/advanced\"\n",
    "    \n",
    "    # Run the workflow\n",
    "    forecast_system, predictions, metrics = run_advanced_gdp_forecast_workflow(\n",
    "        data_folder=DATA_FOLDER,\n",
    "        output_folder=OUTPUT_FOLDER,\n",
    "        start_date='1980-01-01',\n",
    "        end_date=None,\n",
    "        test_size=0.2,\n",
    "        max_features=50,\n",
    "        midas_lags=12,\n",
    "        n_trees=500,\n",
    "        selection_method='rf_importance',\n",
    "        random_state=42,\n",
    "        save_models=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4324b1-153b-4cfe-a433-b35e097b3ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
