{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc68ece-d46f-45be-82d0-9f38bee308f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting Monthly-Only GDP Forecasting Workflow at 2025-05-09 14:09:33.062144\n",
      "================================================================================\n",
      "\n",
      "1. Setting up configuration...\n",
      "Configuration set up with 7 monthly files, 1 quarterly files\n",
      "\n",
      "2. Data Preprocessing...\n",
      "Found 18 files in ./Project_Data\n",
      "Processing monthly data...\n",
      "Processed CPI_mon_monthly.csv: 829 observations, 2 features\n",
      "Processed Unemployment_monthly.csv: 925 observations, 2 features\n",
      "Processed InterestRate_monthly.csv: 847 observations, 2 features\n",
      "Processed HousingStarts_monthly.csv: 792 observations, 2 features\n",
      "Processed Heavy_Truck_Sales.csv: 698 observations, 2 features\n",
      "Processed Manufacturing_Production_Motor_and_Vehicle_Parts.csv: 637 observations, 2 features\n",
      "Processed Consumer_Confidence.csv: 768 observations, 2 features\n",
      "Final monthly dataset: 926 observations, 14 features\n",
      "Processing quarterly data...\n",
      "Processed GDP_quaterly.csv: 311 observations, 2 features\n",
      "Final quarterly dataset: 311 observations, 2 features\n",
      "Processed data: monthly=(926, 14), quarterly=(311, 2)\n",
      "Data overview saved to ./output/data_overview.png\n",
      "\n",
      "3. Calculating Technical Indicators...\n",
      "Applied indicators to CPI_mon_monthly_Value_raw: 45 new features\n",
      "Applied indicators to CPI_mon_monthly_Value_pct_change: 45 new features\n",
      "Applied indicators to Unemployment_monthly_Value_raw: 45 new features\n",
      "Applied indicators to Unemployment_monthly_Value_diff: 45 new features\n",
      "Applied indicators to InterestRate_monthly_Value_raw: 45 new features\n",
      "Applied indicators to InterestRate_monthly_Value_diff: 45 new features\n",
      "Applied indicators to HousingStarts_monthly_Value_raw: 45 new features\n",
      "Applied indicators to HousingStarts_monthly_Value_pct_change: 45 new features\n",
      "Applied indicators to Heavy_Truck_Sales_Value_raw: 45 new features\n",
      "Applied indicators to Heavy_Truck_Sales_Value_pct_change: 45 new features\n",
      "Applied indicators to Manufacturing_Production_Motor_and_Vehicle_Parts_Value_raw: 45 new features\n",
      "Applied indicators to Manufacturing_Production_Motor_and_Vehicle_Parts_Value_pct_change: 45 new features\n",
      "Applied indicators to Consumer_Confidence_Value_raw: 45 new features\n",
      "Applied indicators to Consumer_Confidence_Value_diff: 45 new features\n",
      "Applied indicators to GDP_quaterly_Value_raw: 45 new features\n",
      "Applied indicators to GDP_quaterly_Value_pct_change: 45 new features\n",
      "Calculated technical indicators: monthly=(926, 630), quarterly=(311, 90)\n",
      "\n",
      "4. Aligning Data for Model...\n",
      "Aligned monthly to quarterly: (311, 630)\n",
      "\n",
      "5. Creating Train-Test Split...\n",
      "Train-test split at 2009-03-31 00:00:00: train=248, test=63\n",
      "\n",
      "6. Building Models...\n",
      "Building Monthly-to-Quarterly Model...\n",
      "Fitting monthly model with 630 features\n",
      "Fitting monthly model with 630 features\n",
      "Extracted 3 monthly factors\n",
      "Auto-detected start date: 1948-03-31 00:00:00 (first quarter with complete data)\n",
      "Using 244 quarters of data\n",
      "Fitting MIDAS model with 3 monthly factors, 4 GDP lags\n",
      "Using 238 observations after trimming 6 periods with lag-induced NaNs\n",
      "Monthly_MIDAS Model successfully built\n",
      "Monthly_MIDAS Model saved to ./output/monthly_midas_model.pkl\n",
      "Building Baseline Models...\n",
      "AR Baseline Model successfully built\n",
      "MA-4 Baseline Model defined\n",
      "\n",
      "7. Evaluating Models...\n",
      "Generated predictions for Monthly_MIDAS: 63 quarters\n",
      "Generated predictions for AR_Baseline: 63 quarters\n",
      "Generated predictions for MA_4_Baseline: 63 quarters\n",
      "Calculating evaluation metrics...\n",
      "\n",
      "Key Performance Metrics:\n",
      "--------------------------------------------------------------------------------\n",
      "Model                           RMSE        MAE    Dir Acc\n",
      "--------------------------------------------------------------------------------\n",
      "Monthly_MIDAS                 1.8460     0.9276     0.4194\n",
      "AR_Baseline                   1.9823     0.8485     0.4516\n",
      "MA_4_Baseline                 1.9151     0.9072     0.4032\n",
      "\n",
      "Generating evaluation plots...\n",
      "pandas_datareader not available for recession shading\n",
      "Forecasts plot saved to ./output/gdp_forecasts.png\n",
      "Error distribution plot saved to ./output/error_distribution.png\n",
      "Rolling metrics plot saved to ./output/rolling_metrics.png\n",
      "\n",
      "8. Generating Final Report...\n",
      "pandas_datareader not available for recession shading\n",
      "Comprehensive evaluation report saved to ./output/gdp_forecast_evaluation.md\n",
      "\n",
      "9. Workflow Completed\n",
      "================================================================================\n",
      "Monthly-Only GDP Forecasting Workflow completed at 2025-05-09 14:09:48.479672\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import copy\n",
    "import warnings\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Silence warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def trace_nans(name, df, threshold=0):\n",
    "    \"\"\"\n",
    "    Comprehensive NaN tracing function for pandas DataFrames.\n",
    "    \"\"\"\n",
    "    if isinstance(df, pd.Series):\n",
    "        nan_count = df.isna().sum()\n",
    "        total = len(df)\n",
    "        if nan_count > 0:\n",
    "            print(f\"WARNING: {name} Series contains {nan_count}/{total} NaNs ({nan_count/total:.2%})\")\n",
    "        return\n",
    "        \n",
    "    nan_count = df.isna().sum().sum()\n",
    "    if nan_count > 0:\n",
    "        rows, cols = df.shape\n",
    "        total_cells = rows * cols\n",
    "        \n",
    "        print(f\"WARNING: {name} contains {nan_count}/{total_cells} NaNs ({nan_count/total_cells:.2%})\")\n",
    "        \n",
    "        cols_with_nans = df.columns[df.isna().sum() > threshold]\n",
    "        if len(cols_with_nans) > 0:\n",
    "            print(f\"  Columns with > {threshold} NaNs:\")\n",
    "            for col in cols_with_nans:\n",
    "                col_nans = df[col].isna().sum()\n",
    "                print(f\"    {col}: {col_nans}/{rows} NaNs ({col_nans/rows:.2%})\")\n",
    "        \n",
    "        row_nan_counts = df.isna().sum(axis=1)\n",
    "        rows_with_many_nans = row_nan_counts[row_nan_counts > cols//4].sort_values(ascending=False)\n",
    "        if len(rows_with_many_nans) > 0:\n",
    "            print(f\"  Rows with significant NaNs:\")\n",
    "            for idx, count in rows_with_many_nans.head(5).items():\n",
    "                print(f\"    Row at {idx}: {count}/{cols} NaNs ({count/cols:.2%})\")\n",
    "        \n",
    "        first_rows_nan_pct = df.head(rows//10).isna().sum().sum() / (rows//10 * cols)\n",
    "        last_rows_nan_pct = df.tail(rows//10).isna().sum().sum() / (rows//10 * cols)\n",
    "        if first_rows_nan_pct > 0.1:\n",
    "            print(f\"  First 10% of rows have {first_rows_nan_pct:.2%} NaNs - possible lag/window effect\")\n",
    "        if last_rows_nan_pct > 0.1:\n",
    "            print(f\"  Last 10% of rows have {last_rows_nan_pct:.2%} NaNs - possible trailing window effect\")\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Module 1: Multi-Frequency Data Preprocessor\n",
    "#-----------------------------------------------------------------------------\n",
    "class MultiFrequencyPreprocessor:\n",
    "    \"\"\"\n",
    "    Enhanced data preprocessor for multi-frequency economic data.\n",
    "    This class handles different time frequencies (daily, weekly, monthly, quarterly)\n",
    "    and ensures proper alignment and processing for hierarchical modeling.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_folder):\n",
    "        \"\"\"\n",
    "        Initialize the MultiFrequencyPreprocessor with the folder containing CSV files.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_folder: str\n",
    "            Path to the folder containing CSV files\n",
    "        \"\"\"\n",
    "        self.data_folder = data_folder\n",
    "        self.available_files = self._get_available_files()\n",
    "        self.data_config = {}\n",
    "        self.frequency_data = {\n",
    "            'daily': None,\n",
    "            'weekly': None,\n",
    "            'monthly': None,\n",
    "            'quarterly': None\n",
    "        }\n",
    "        self.start_date = None\n",
    "        self.end_date = None\n",
    "        # Dictionaries to store processed data and factors\n",
    "        self.processed_data = {}\n",
    "        self.factors = {}\n",
    "        print(f\"Found {len(self.available_files)} files in {data_folder}\")\n",
    "    \n",
    "    def _get_available_files(self):\n",
    "        \"\"\"List all CSV files in the data folder.\"\"\"\n",
    "        # Normalize path to handle both forward and backward slashes\n",
    "        norm_path = os.path.normpath(self.data_folder)\n",
    "        files = glob.glob(os.path.join(norm_path, '*.csv'))\n",
    "        return [os.path.basename(f) for f in files]\n",
    "    \n",
    "    def set_config(self, data_config):\n",
    "        \"\"\"\n",
    "        Set the configuration for data loading and preprocessing.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_config: dict\n",
    "            Configuration dictionary for data loading\n",
    "        \"\"\"\n",
    "        self.data_config = data_config\n",
    "    \n",
    "    def set_date_range(self, start_date=None, end_date=None):\n",
    "        \"\"\"\n",
    "        Set the global date range for data processing.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        start_date: str or datetime\n",
    "            Start date for data processing (format: 'YYYY-MM-DD')\n",
    "        end_date: str or datetime\n",
    "            End date for data processing (format: 'YYYY-MM-DD')\n",
    "        \"\"\"\n",
    "        if start_date:\n",
    "            self.start_date = pd.to_datetime(start_date) if isinstance(start_date, str) else start_date\n",
    "        if end_date:\n",
    "            self.end_date = pd.to_datetime(end_date) if isinstance(end_date, str) else end_date\n",
    "    \n",
    "    def _load_csv(self, file_name, frequency):\n",
    "        \"\"\"\n",
    "        Load a CSV file and parse the date column.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        file_name: str\n",
    "            Name of the CSV file\n",
    "        frequency: str\n",
    "            Data frequency ('daily', 'weekly', 'monthly', 'quarterly')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            Loaded dataframe with date index\n",
    "        \"\"\"\n",
    "        # Normalize path\n",
    "        norm_path = os.path.normpath(self.data_folder)\n",
    "        file_path = os.path.join(norm_path, file_name)\n",
    "        \n",
    "        try:\n",
    "            # First try standard CSV loading\n",
    "            df = pd.read_csv(file_path, parse_dates=[0], index_col=0)\n",
    "            \n",
    "            # Check if index is datetime\n",
    "            if not pd.api.types.is_datetime64_any_dtype(df.index):\n",
    "                # Convert index to datetime\n",
    "                df.index = pd.to_datetime(df.index)\n",
    "            \n",
    "            # Apply frequency-specific processing\n",
    "            if frequency == 'daily':\n",
    "                # For daily data, ensure the index is business days\n",
    "                df = df.asfreq('B', method='ffill')\n",
    "            elif frequency == 'weekly':\n",
    "                # For weekly data, use end of week\n",
    "                df = df.asfreq('W-FRI', method='ffill')\n",
    "            elif frequency == 'monthly':\n",
    "                # For monthly data, use end of month\n",
    "                df = df.asfreq('M', method='ffill')\n",
    "            elif frequency == 'quarterly':\n",
    "                # For quarterly data, use end of quarter\n",
    "                df = df.asfreq('Q', method='ffill')\n",
    "            \n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_name}: {e}\")\n",
    "            \n",
    "            # Try alternative approach\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                date_col = df.columns[0]\n",
    "                \n",
    "                # Try different date formats\n",
    "                try:\n",
    "                    df[date_col] = pd.to_datetime(df[date_col])\n",
    "                except:\n",
    "                    for date_format in ['%Y-%m-%d', '%d/%m/%Y', '%m/%d/%Y', '%Y/%m/%d']:\n",
    "                        try:\n",
    "                            df[date_col] = pd.to_datetime(df[date_col], format=date_format)\n",
    "                            break\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                \n",
    "                df.set_index(date_col, inplace=True)\n",
    "                \n",
    "                # Apply frequency-specific processing\n",
    "                if frequency == 'daily':\n",
    "                    df = df.asfreq('B', method='ffill')\n",
    "                elif frequency == 'weekly':\n",
    "                    df = df.asfreq('W-FRI', method='ffill')\n",
    "                elif frequency == 'monthly':\n",
    "                    df = df.asfreq('M', method='ffill')\n",
    "                elif frequency == 'quarterly':\n",
    "                    df = df.asfreq('Q', method='ffill')\n",
    "                \n",
    "                return df\n",
    "            \n",
    "            except Exception as nested_e:\n",
    "                print(f\"Failed to load {file_name} after multiple attempts: {nested_e}\")\n",
    "                raise\n",
    "    \n",
    "    def _apply_transformation(self, df, column, transformation):\n",
    "        \"\"\"\n",
    "        Apply the specified transformation to a column.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df: pd.DataFrame\n",
    "            DataFrame containing the column\n",
    "        column: str\n",
    "            Column name to transform\n",
    "        transformation: str or list\n",
    "            Transformation type ('raw', 'pct_change', 'log_return', 'diff') or list\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list of tuples\n",
    "            List of (column_name, transformed_series) tuples\n",
    "        \"\"\"\n",
    "        if column not in df.columns:\n",
    "            print(f\"Warning: Column {column} not found in DataFrame\")\n",
    "            return []\n",
    "        \n",
    "        # Handle list of transformations\n",
    "        if isinstance(transformation, list):\n",
    "            result = []\n",
    "            for t in transformation:\n",
    "                column_name = f\"{column}_{t}\"\n",
    "                series = self._apply_single_transformation(df, column, t)\n",
    "                result.append((column_name, series))\n",
    "            return result\n",
    "        else:\n",
    "            # Handle single transformation\n",
    "            column_name = f\"{column}_{transformation}\" if transformation != 'raw' else column\n",
    "            series = self._apply_single_transformation(df, column, transformation)\n",
    "            return [(column_name, series)]\n",
    "    \n",
    "    def _apply_single_transformation(self, df, column, transformation):\n",
    "        \"\"\"\n",
    "        Apply a single transformation to a column with robust handling of edge cases.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df: pd.DataFrame\n",
    "            DataFrame containing the column\n",
    "        column: str\n",
    "            Column name to transform\n",
    "        transformation: str\n",
    "            Transformation type ('raw', 'pct_change', 'log_return', 'diff', 'yoy')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.Series\n",
    "            Transformed series\n",
    "        \"\"\"\n",
    "        if transformation == 'raw':\n",
    "            return df[column]\n",
    "        elif transformation == 'pct_change':\n",
    "            # Calculate percentage change with correct usage\n",
    "            pct = df[column].ffill().pct_change() * 100\n",
    "            # Fill first value with 0 for continuity\n",
    "            if len(pct) > 0:\n",
    "                pct.iloc[0] = 0\n",
    "            return pct\n",
    "        elif transformation == 'log_return':\n",
    "            # Calculate log return (continuously compounded return)\n",
    "            log_ret = (np.log(df[column]) - np.log(df[column].shift(1))) * 100\n",
    "            # Fill first value with 0 for continuity\n",
    "            if len(log_ret) > 0:\n",
    "                log_ret.iloc[0] = 0\n",
    "            return log_ret\n",
    "        elif transformation == 'diff':\n",
    "            # Calculate first difference\n",
    "            diff = df[column].diff()\n",
    "            # Fill first value with 0 for continuity\n",
    "            if len(diff) > 0:\n",
    "                diff.iloc[0] = 0\n",
    "            return diff\n",
    "        elif transformation == 'yoy':\n",
    "            # Calculate year-over-year percentage change\n",
    "            yoy = df[column].ffill().pct_change(periods=12) * 100\n",
    "            # Forward fill NaN values\n",
    "            yoy = yoy.ffill()\n",
    "            return yoy\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown transformation: {transformation}\")\n",
    "    \n",
    "    def _calculate_ratios(self, data_dict, ratio_config):\n",
    "        \"\"\"\n",
    "        Calculate financial ratios from base time series.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_dict: dict\n",
    "            Dictionary of DataFrames\n",
    "        ratio_config: dict\n",
    "            Configuration for ratio calculation\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary with ratio DataFrames added\n",
    "        \"\"\"\n",
    "        result_dict = data_dict.copy()\n",
    "        \n",
    "        for ratio_name, config in ratio_config.items():\n",
    "            try:\n",
    "                numerator_key = config['numerator']\n",
    "                denominator_key = config['denominator']\n",
    "                transformations = config.get('transformations', ['raw'])\n",
    "                \n",
    "                # Get the component series\n",
    "                if numerator_key in data_dict and denominator_key in data_dict:\n",
    "                    numerator = data_dict[numerator_key].iloc[:, 0]  # Assume first column\n",
    "                    denominator = data_dict[denominator_key].iloc[:, 0]  # Assume first column\n",
    "                    \n",
    "                    # Calculate the ratio\n",
    "                    ratio = numerator / denominator\n",
    "                    ratio_df = pd.DataFrame({f\"{ratio_name}_raw\": ratio})\n",
    "                    \n",
    "                    # Apply transformations\n",
    "                    for transform in transformations:\n",
    "                        if transform != 'raw':\n",
    "                            transformed_series = self._apply_single_transformation(ratio_df, f\"{ratio_name}_raw\", transform)\n",
    "                            ratio_df[f\"{ratio_name}_{transform}\"] = transformed_series\n",
    "                    \n",
    "                    # Add to result\n",
    "                    result_dict[ratio_name] = ratio_df\n",
    "                    print(f\"Created ratio: {ratio_name} with {len(ratio_df)} observations\")\n",
    "                else:\n",
    "                    print(f\"Warning: Could not create ratio {ratio_name}. Missing component series.\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating ratio {ratio_name}: {e}\")\n",
    "        \n",
    "        return result_dict\n",
    "    \n",
    "    def process_frequency_data(self, frequency):\n",
    "        \"\"\"\n",
    "        Process data for a specific frequency.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        frequency: str\n",
    "            Data frequency ('daily', 'weekly', 'monthly', 'quarterly')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            Processed DataFrame for the specified frequency\n",
    "        \"\"\"\n",
    "        if frequency not in self.data_config:\n",
    "            print(f\"No configuration found for {frequency} data\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Processing {frequency} data...\")\n",
    "        freq_config = self.data_config[frequency]\n",
    "        \n",
    "        # Load and transform individual files\n",
    "        data_dict = {}\n",
    "        \n",
    "        for file_name, config in freq_config.get('files', {}).items():\n",
    "            if file_name not in self.available_files:\n",
    "                print(f\"Warning: {file_name} not found, skipping\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Load CSV file\n",
    "                df = self._load_csv(file_name, frequency)\n",
    "                \n",
    "                # Apply date filtering if specified\n",
    "                if 'start_date' in config:\n",
    "                    df = df[df.index >= pd.to_datetime(config['start_date'])]\n",
    "                elif self.start_date:\n",
    "                    df = df[df.index >= self.start_date]\n",
    "                \n",
    "                if self.end_date:\n",
    "                    df = df[df.index <= self.end_date]\n",
    "                \n",
    "                # Apply transformations\n",
    "                transformed_columns = []\n",
    "                for column in config['columns']:\n",
    "                    # Get transformation type\n",
    "                    transformation = config['transformations'].get(column, 'raw')\n",
    "                    \n",
    "                    # Apply transformation\n",
    "                    results = self._apply_transformation(df, column, transformation)\n",
    "                    \n",
    "                    # Store results\n",
    "                    for col_name, series in results:\n",
    "                        # Create descriptive name: filename_column_transformation\n",
    "                        file_prefix = file_name.split('.')[0]  # Remove extension\n",
    "                        prefixed_name = f\"{file_prefix}_{col_name}\"\n",
    "                        transformed_columns.append((prefixed_name, series))\n",
    "                \n",
    "                # Create DataFrame from transformed columns\n",
    "                if transformed_columns:\n",
    "                    processed_df = pd.DataFrame({name: series for name, series in transformed_columns})\n",
    "                    processed_df.index = df.index\n",
    "                    \n",
    "                    # Store in data dictionary\n",
    "                    key = file_name.split('.')[0]  # Use filename without extension\n",
    "                    data_dict[key] = processed_df\n",
    "                    \n",
    "                    print(f\"Processed {file_name}: {len(processed_df)} observations, {len(processed_df.columns)} features\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        # Calculate ratios if configured\n",
    "        if 'ratios' in freq_config:\n",
    "            data_dict = self._calculate_ratios(data_dict, freq_config['ratios'])\n",
    "        \n",
    "        # Merge all DataFrames\n",
    "        if data_dict:\n",
    "            merged_df = None\n",
    "            for _, df in data_dict.items():\n",
    "                if merged_df is None:\n",
    "                    merged_df = df.copy()\n",
    "                else:\n",
    "                    merged_df = merged_df.join(df, how='outer')\n",
    "            \n",
    "            # Handle missing values\n",
    "            if merged_df is not None:\n",
    "                # Forward fill for continuity\n",
    "                merged_df = merged_df.ffill()\n",
    "                # Then backward fill any remaining NaNs at the beginning\n",
    "                merged_df = merged_df.bfill()\n",
    "                \n",
    "                # Store in processed data dictionary\n",
    "                self.processed_data[frequency] = merged_df\n",
    "                \n",
    "                print(f\"Final {frequency} dataset: {len(merged_df)} observations, {len(merged_df.columns)} features\")\n",
    "                return merged_df\n",
    "            else:\n",
    "                print(f\"No valid data found for {frequency} frequency\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"No data processed for {frequency} frequency\")\n",
    "            return None\n",
    "    \n",
    "    def process_all_frequencies(self):\n",
    "        \"\"\"\n",
    "        Process data for all configured frequencies.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of processed DataFrames for each frequency\n",
    "        \"\"\"\n",
    "        for frequency in self.data_config.keys():\n",
    "            self.process_frequency_data(frequency)\n",
    "        \n",
    "        return self.processed_data\n",
    "    \n",
    "    def align_to_dates(self, source_df, target_dates, method='last'):\n",
    "        \"\"\"\n",
    "        Align source DataFrame to target dates using specified method.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        source_df: pd.DataFrame\n",
    "            Source DataFrame to align\n",
    "        target_dates: pd.DatetimeIndex\n",
    "            Target dates to align to\n",
    "        method: str\n",
    "            Method for alignment ('last', 'nearest', 'linear')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            Aligned DataFrame\n",
    "        \"\"\"\n",
    "        # Initialize aligned DataFrame with the same columns as source_df\n",
    "        aligned_df = pd.DataFrame(index=target_dates, columns=source_df.columns)\n",
    "        \n",
    "        if method == 'last':\n",
    "            # For each target date, find the last available observation\n",
    "            for date in target_dates:\n",
    "                prev_data = source_df[source_df.index <= date]\n",
    "                if not prev_data.empty:\n",
    "                    # Get the last row as a Series and assign values column by column\n",
    "                    last_row = prev_data.iloc[-1]\n",
    "                    for col in source_df.columns:\n",
    "                        aligned_df.loc[date, col] = last_row[col]\n",
    "        \n",
    "        elif method == 'nearest':\n",
    "            # For each target date, find the nearest observation\n",
    "            for date in target_dates:\n",
    "                # Calculate absolute difference in days\n",
    "                source_dates = source_df.index\n",
    "                if len(source_dates) > 0:\n",
    "                    # Convert to numpy arrays for vectorized operations\n",
    "                    days_diff = np.abs((source_dates - date).days.values)\n",
    "                    nearest_idx = np.argmin(days_diff)\n",
    "                    \n",
    "                    # Assign values column by column\n",
    "                    nearest_row = source_df.iloc[nearest_idx]\n",
    "                    for col in source_df.columns:\n",
    "                        aligned_df.loc[date, col] = nearest_row[col]\n",
    "        \n",
    "        elif method == 'linear':\n",
    "            # This method can be implemented directly with pandas reindex\n",
    "            aligned_df = source_df.reindex(index=sorted(list(source_df.index) + list(target_dates)))\n",
    "            \n",
    "            # Apply linear interpolation\n",
    "            aligned_df = aligned_df.interpolate(method='linear')\n",
    "            \n",
    "            # Extract only the target dates\n",
    "            aligned_df = aligned_df.reindex(target_dates)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown alignment method: {method}\")\n",
    "        \n",
    "        # Handle any remaining NaNs by forward filling, then backward filling\n",
    "        aligned_df = aligned_df.ffill().bfill()\n",
    "        \n",
    "        return aligned_df\n",
    "    \n",
    "    def generate_hierarchical_dataset(self, target_frequency='quarterly'):\n",
    "        \"\"\"\n",
    "        Generate hierarchical dataset with higher-frequency data aligned to lower frequency.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        target_frequency: str\n",
    "            Target frequency for alignment ('quarterly', 'monthly', 'weekly')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of aligned datasets for hierarchical modeling\n",
    "        \"\"\"\n",
    "        hierarchical_data = {}\n",
    "        \n",
    "        # Define frequency hierarchy\n",
    "        freq_hierarchy = {\n",
    "            'quarterly': ['monthly', 'weekly', 'daily'],\n",
    "            'monthly': ['weekly', 'daily'],\n",
    "            'weekly': ['daily']\n",
    "        }\n",
    "        \n",
    "        # Get target dates\n",
    "        if target_frequency not in self.processed_data:\n",
    "            raise ValueError(f\"No processed data found for {target_frequency} frequency\")\n",
    "        \n",
    "        target_dates = self.processed_data[target_frequency].index\n",
    "        hierarchical_data[target_frequency] = self.processed_data[target_frequency]\n",
    "        \n",
    "        # Align higher frequency data to target dates\n",
    "        for higher_freq in freq_hierarchy.get(target_frequency, []):\n",
    "            if higher_freq in self.processed_data:\n",
    "                aligned_df = self.align_to_dates(\n",
    "                    self.processed_data[higher_freq],\n",
    "                    target_dates,\n",
    "                    method='last'  # Use last available observation\n",
    "                )\n",
    "                hierarchical_data[f\"{higher_freq}_aligned\"] = aligned_df\n",
    "        \n",
    "        return hierarchical_data\n",
    "    \n",
    "    def plot_data_overview(self, frequency=None):\n",
    "        \"\"\"\n",
    "        Plot an overview of the processed data to help with visualization.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        frequency: str or None\n",
    "            Frequency to plot, or None to plot all\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            Matplotlib figure object\n",
    "        \"\"\"\n",
    "        if frequency:\n",
    "            if frequency not in self.processed_data:\n",
    "                raise ValueError(f\"No processed data found for {frequency} frequency\")\n",
    "            frequencies = [frequency]\n",
    "        else:\n",
    "            frequencies = list(self.processed_data.keys())\n",
    "        \n",
    "        n_freqs = len(frequencies)\n",
    "        fig, axes = plt.subplots(n_freqs, 1, figsize=(15, 6*n_freqs))\n",
    "        \n",
    "        if n_freqs == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, freq in enumerate(frequencies):\n",
    "            df = self.processed_data[freq]\n",
    "            \n",
    "            # Select a subset of columns if there are too many\n",
    "            max_cols = 10\n",
    "            if len(df.columns) > max_cols:\n",
    "                # Choose evenly spaced columns\n",
    "                indices = np.linspace(0, len(df.columns)-1, max_cols, dtype=int)\n",
    "                plot_cols = [df.columns[i] for i in indices]\n",
    "            else:\n",
    "                plot_cols = df.columns\n",
    "            \n",
    "            # Plot each column\n",
    "            for col in plot_cols:\n",
    "                axes[i].plot(df.index, df[col], label=col)\n",
    "            \n",
    "            axes[i].set_title(f\"{freq.capitalize()} Data Overview\")\n",
    "            axes[i].set_xlabel('Date')\n",
    "            axes[i].set_ylabel('Value')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            axes[i].legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Module 2: Technical Indicators for Multi-Frequency Data\n",
    "#-----------------------------------------------------------------------------\n",
    "class MultiFrequencyTechnicalIndicators:\n",
    "    \"\"\"\n",
    "    Technical indicators calculation for multi-frequency economic data.\n",
    "    This class implements SMA, RSI, and ROC with frequency-appropriate parameters\n",
    "    and enhanced metrics for economic time series.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def get_frequency_params(frequency):\n",
    "        \"\"\"\n",
    "        Get appropriate technical indicator parameters for each frequency.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        frequency: str\n",
    "            Data frequency ('daily', 'weekly', 'monthly', 'quarterly')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of parameters for each indicator type\n",
    "        \"\"\"\n",
    "        if frequency == 'daily':\n",
    "            # For daily data - standard financial parameters\n",
    "            return {\n",
    "                'sma': [5, 20, 60, 200],  # Short, medium, quarter, year\n",
    "                'rsi': [14, 21],  # Standard and extended\n",
    "                'roc': [1, 5, 20, 60]  # Daily, weekly, monthly, quarter\n",
    "            }\n",
    "        elif frequency == 'weekly':\n",
    "            # For weekly data - adjusted to weekly scale\n",
    "            return {\n",
    "                'sma': [4, 12, 26, 52],  # Month, quarter, half-year, year\n",
    "                'rsi': [8, 12],  # ~1.5-2 months\n",
    "                'roc': [1, 4, 13, 26]  # Week, month, quarter, half-year\n",
    "            }\n",
    "        elif frequency == 'monthly':\n",
    "            # For monthly data - adjusted to monthly scale\n",
    "            return {\n",
    "                'sma': [3, 6, 12, 24],  # Quarter, half-year, year, two years\n",
    "                'rsi': [6, 9],  # Half-year, three quarters\n",
    "                'roc': [1, 3, 6, 12]  # Month, quarter, half-year, year\n",
    "            }\n",
    "        elif frequency == 'quarterly':\n",
    "            # For quarterly data - adjusted to quarterly scale\n",
    "            return {\n",
    "                'sma': [2, 4, 8, 12],  # Half-year, year, two years, three years\n",
    "                'rsi': [4, 6],  # Year, year and half\n",
    "                'roc': [1, 2, 4, 8]  # Quarter, half-year, year, two years\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown frequency: {frequency}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _calculate_trend_direction(series, periods=1):\n",
    "        \"\"\"\n",
    "        Calculate trend direction for a series with proper handling of zeros and NaNs.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        series: pandas.Series\n",
    "            Series to calculate trend direction for\n",
    "        periods: int\n",
    "            Number of periods to look back\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.Series\n",
    "            Series containing trend direction values:\n",
    "            1 for rising, -1 for falling, 0 for no change\n",
    "        \"\"\"\n",
    "        # Calculate direction safely\n",
    "        diff = series.diff(periods)\n",
    "        \n",
    "        # Initialize direction series\n",
    "        direction = pd.Series(0, index=series.index)\n",
    "        \n",
    "        # Positive direction\n",
    "        direction[diff > 0] = 1\n",
    "        \n",
    "        # Negative direction\n",
    "        direction[diff < 0] = -1\n",
    "        \n",
    "        # For zero-diff values, carry forward previous direction to avoid flicker\n",
    "        # but only where series values are valid\n",
    "        zero_mask = (diff == 0) & series.notna()\n",
    "        if zero_mask.any():\n",
    "            # Forward-fill only zero-diff positions\n",
    "            direction_filled = direction.copy()\n",
    "            direction_filled[zero_mask] = np.nan\n",
    "            direction_filled = direction_filled.ffill()\n",
    "            \n",
    "            # Update direction where diff was zero\n",
    "            direction[zero_mask] = direction_filled[zero_mask]\n",
    "        \n",
    "        return direction\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_sma(df, column, windows=None, include_trend=True, include_crossovers=True):\n",
    "        \"\"\"\n",
    "        Calculate Simple Moving Averages with enhanced metrics.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df: pandas.DataFrame\n",
    "            DataFrame containing the data\n",
    "        column: str\n",
    "            Column name to calculate SMA for\n",
    "        windows: list\n",
    "            List of window sizes for SMA calculation\n",
    "        include_trend: bool\n",
    "            Whether to include trend direction\n",
    "        include_crossovers: bool\n",
    "            Whether to include crossover signals\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with SMA values and enhanced metrics\n",
    "        \"\"\"\n",
    "        if windows is None:\n",
    "            # Default parameters - will be overridden by frequency-specific ones\n",
    "            windows = [5, 20, 60, 200]\n",
    "        \n",
    "        result_df = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # Calculate SMAs for each window\n",
    "        for window in windows:\n",
    "            # Calculate SMA with proper min_periods\n",
    "            min_periods = max(1, window // 4)\n",
    "            sma = df[column].rolling(window=window, min_periods=min_periods).mean()\n",
    "            \n",
    "            sma_name = f\"{column}_SMA_{window}\"\n",
    "            result_df[sma_name] = sma\n",
    "            \n",
    "            # Calculate percentage difference from SMA\n",
    "            valid_mask = (sma != 0) & sma.notna() & df[column].notna()\n",
    "            pct_diff = pd.Series(index=df.index, dtype=float)\n",
    "            pct_diff[valid_mask] = (df[column][valid_mask] - sma[valid_mask]) / sma[valid_mask] * 100\n",
    "            result_df[f\"{sma_name}_pct_diff\"] = pct_diff\n",
    "            \n",
    "            # Calculate trend if requested\n",
    "            if include_trend:\n",
    "                trend = MultiFrequencyTechnicalIndicators._calculate_trend_direction(sma)\n",
    "                result_df[f\"{sma_name}_trend\"] = trend\n",
    "        \n",
    "        # Calculate crossovers if requested and we have at least two windows\n",
    "        if include_crossovers and len(windows) >= 2:\n",
    "            # Sort windows to ensure correct fast/slow designation\n",
    "            sorted_windows = sorted(windows)\n",
    "            \n",
    "            # Calculate crossovers between adjacent SMAs\n",
    "            for i in range(len(sorted_windows) - 1):\n",
    "                fast_window = sorted_windows[i]\n",
    "                slow_window = sorted_windows[i+1]\n",
    "                \n",
    "                fast_sma = result_df[f\"{column}_SMA_{fast_window}\"]\n",
    "                slow_sma = result_df[f\"{column}_SMA_{slow_window}\"]\n",
    "                \n",
    "                # Calculate difference between fast and slow SMAs\n",
    "                diff = fast_sma - slow_sma\n",
    "                \n",
    "                # Calculate crossover signal\n",
    "                crossover = pd.Series(0, index=df.index)\n",
    "                \n",
    "                # Find where diff changes sign\n",
    "                diff_sign = np.sign(diff)\n",
    "                sign_change = diff_sign.diff().fillna(0)\n",
    "                \n",
    "                # 1 for bullish crossover (fast crosses above slow)\n",
    "                crossover[sign_change > 0] = 1\n",
    "                \n",
    "                # -1 for bearish crossover (fast crosses below slow)\n",
    "                crossover[sign_change < 0] = -1\n",
    "                \n",
    "                crossover_name = f\"{column}_SMA_{fast_window}_{slow_window}_crossover\"\n",
    "                result_df[crossover_name] = crossover\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_rsi(df, column, windows=None, include_trend=True, include_zones=True):\n",
    "        \"\"\"\n",
    "        Calculate Relative Strength Index with enhanced metrics.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df: pandas.DataFrame\n",
    "            DataFrame containing the data\n",
    "        column: str\n",
    "            Column name to calculate RSI for\n",
    "        windows: list\n",
    "            List of window sizes for RSI calculation\n",
    "        include_trend: bool\n",
    "            Whether to include trend direction\n",
    "        include_zones: bool\n",
    "            Whether to include overbought/oversold zone indicators\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with RSI values and enhanced metrics\n",
    "        \"\"\"\n",
    "        if windows is None:\n",
    "            # Default parameters - will be overridden by frequency-specific ones\n",
    "            windows = [14, 21]\n",
    "        \n",
    "        result_df = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        for window in windows:\n",
    "            # Calculate price changes\n",
    "            delta = df[column].diff()\n",
    "            \n",
    "            # Create separate gain and loss series with proper dtype\n",
    "            gain = pd.Series(0.0, index=delta.index)  # Use float dtype\n",
    "            loss = pd.Series(0.0, index=delta.index)  # Use float dtype\n",
    "            \n",
    "            # Set values for gain and loss series using .loc for proper assignment\n",
    "            gain.loc[delta > 0] = delta[delta > 0]\n",
    "            loss.loc[delta < 0] = -delta[delta < 0]  # Make losses positive\n",
    "            \n",
    "            # First values are NaN\n",
    "            gain.iloc[0] = 0.0\n",
    "            loss.iloc[0] = 0.0\n",
    "            \n",
    "            # Calculate RSI using Wilder's method\n",
    "            # First calculate simple averages for initial periods\n",
    "            avg_gain = gain.rolling(window=window, min_periods=1).mean()\n",
    "            avg_loss = loss.rolling(window=window, min_periods=1).mean()\n",
    "            \n",
    "            # Then use the Wilder's smoothing method\n",
    "            for i in range(window, len(gain)):\n",
    "                avg_gain.iloc[i] = (avg_gain.iloc[i-1] * (window-1) + gain.iloc[i]) / window\n",
    "                avg_loss.iloc[i] = (avg_loss.iloc[i-1] * (window-1) + loss.iloc[i]) / window\n",
    "            \n",
    "            # Calculate RS and RSI\n",
    "            # Avoid division by zero with epsilon\n",
    "            epsilon = np.finfo(float).eps\n",
    "            rs = avg_gain / avg_loss.replace(0, epsilon)\n",
    "            rsi = 100 - (100 / (1 + rs))\n",
    "            \n",
    "            # Ensure RSI is within [0, 100] bounds\n",
    "            rsi = np.clip(rsi, 0, 100)\n",
    "            \n",
    "            rsi_name = f\"{column}_RSI_{window}\"\n",
    "            result_df[rsi_name] = rsi\n",
    "            \n",
    "            # Calculate trend if requested\n",
    "            if include_trend:\n",
    "                trend = MultiFrequencyTechnicalIndicators._calculate_trend_direction(rsi)\n",
    "                result_df[f\"{rsi_name}_trend\"] = trend\n",
    "            \n",
    "            # Add overbought/oversold indicators if requested\n",
    "            if include_zones:\n",
    "                # Overbought zone (RSI > 70)\n",
    "                result_df[f\"{rsi_name}_overbought\"] = (rsi > 70).astype(int)\n",
    "                \n",
    "                # Oversold zone (RSI < 30)\n",
    "                result_df[f\"{rsi_name}_oversold\"] = (rsi < 30).astype(int)\n",
    "                \n",
    "                # Initialize divergence column\n",
    "                result_df[f\"{rsi_name}_divergence\"] = 0\n",
    "                \n",
    "                # Calculate divergence between price and RSI\n",
    "                # Instead of using chained assignment, we'll create and assign a complete array\n",
    "                divergence_window = max(5, window // 3)\n",
    "                divergence_values = np.zeros(len(df))\n",
    "                \n",
    "                # Process in batches to improve performance\n",
    "                batch_size = 1000  # Process in batches\n",
    "                for start_idx in range(divergence_window, len(df), batch_size):\n",
    "                    end_idx = min(start_idx + batch_size, len(df))\n",
    "                    \n",
    "                    for i in range(start_idx, end_idx):\n",
    "                        # Get windows for analysis\n",
    "                        price_window = df[column].iloc[i-divergence_window:i+1]\n",
    "                        rsi_window = rsi.iloc[i-divergence_window:i+1]\n",
    "                        \n",
    "                        # Skip if windows contain NaN\n",
    "                        if price_window.isna().any() or rsi_window.isna().any():\n",
    "                            continue\n",
    "                        \n",
    "                        # Check for bearish divergence\n",
    "                        # Price higher high but RSI lower high\n",
    "                        if (price_window.iloc[-1] > price_window.iloc[:-1].max() and\n",
    "                            rsi_window.iloc[-1] < rsi_window.iloc[:-1].max()):\n",
    "                            divergence_values[i] = -1  # Bearish\n",
    "                        \n",
    "                        # Check for bullish divergence\n",
    "                        # Price lower low but RSI higher low\n",
    "                        elif (price_window.iloc[-1] < price_window.iloc[:-1].min() and\n",
    "                            rsi_window.iloc[-1] > rsi_window.iloc[:-1].min()):\n",
    "                            divergence_values[i] = 1  # Bullish\n",
    "                \n",
    "                # Assign the complete divergence array at once (avoids chained assignment)\n",
    "                result_df.loc[:, f\"{rsi_name}_divergence\"] = divergence_values\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_roc(df, column, windows=None, include_trend=True, include_signal=True):\n",
    "        \"\"\"\n",
    "        Calculate Rate of Change with enhanced metrics.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df: pandas.DataFrame\n",
    "            DataFrame containing the data\n",
    "        column: str\n",
    "            Column name to calculate ROC for\n",
    "        windows: list\n",
    "            List of window sizes for ROC calculation\n",
    "        include_trend: bool\n",
    "            Whether to include trend direction\n",
    "        include_signal: bool\n",
    "            Whether to include signal line\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with ROC values and enhanced metrics\n",
    "        \"\"\"\n",
    "        if windows is None:\n",
    "            # Default parameters - will be overridden by frequency-specific ones\n",
    "            windows = [1, 5, 20, 60]\n",
    "        \n",
    "        result_df = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        for window in windows:\n",
    "            # Calculate ROC (percentage change over the specified window)\n",
    "            roc = df[column].pct_change(periods=window) * 100\n",
    "            \n",
    "            # Fill first value with 0 for continuity\n",
    "            roc.iloc[:window] = 0\n",
    "            \n",
    "            roc_name = f\"{column}_ROC_{window}\"\n",
    "            result_df[roc_name] = roc\n",
    "            \n",
    "            # Calculate trend if requested\n",
    "            if include_trend:\n",
    "                trend = MultiFrequencyTechnicalIndicators._calculate_trend_direction(roc)\n",
    "                result_df[f\"{roc_name}_trend\"] = trend\n",
    "            \n",
    "            # Calculate signal line if requested\n",
    "            if include_signal:\n",
    "                # Signal line is typically a moving average of the ROC\n",
    "                signal_window = max(5, window // 4)\n",
    "                signal = roc.rolling(window=signal_window, min_periods=1).mean()\n",
    "                result_df[f\"{roc_name}_signal\"] = signal\n",
    "                \n",
    "                # Calculate crossover signal\n",
    "                crossover = pd.Series(0, index=df.index)\n",
    "                \n",
    "                # ROC crossing above signal line = bullish\n",
    "                crossover[(roc.shift(1) <= signal.shift(1)) & (roc > signal)] = 1\n",
    "                \n",
    "                # ROC crossing below signal line = bearish\n",
    "                crossover[(roc.shift(1) >= signal.shift(1)) & (roc < signal)] = -1\n",
    "                \n",
    "                result_df[f\"{roc_name}_crossover\"] = crossover\n",
    "                \n",
    "                # Calculate histogram (difference between ROC and signal)\n",
    "                histogram = roc - signal\n",
    "                result_df[f\"{roc_name}_histogram\"] = histogram\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_indicators(df, frequency='daily'):\n",
    "        \"\"\"\n",
    "        Apply all technical indicators with frequency-appropriate parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df: pandas.DataFrame\n",
    "            DataFrame containing the data\n",
    "        frequency: str\n",
    "            Data frequency ('daily', 'weekly', 'monthly', 'quarterly')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with all technical indicators\n",
    "        \"\"\"\n",
    "        # Get frequency-specific parameters\n",
    "        params = MultiFrequencyTechnicalIndicators.get_frequency_params(frequency)\n",
    "        \n",
    "        # Store all indicators in a dictionary first to avoid DataFrame fragmentation\n",
    "        all_indicators = {}\n",
    "        \n",
    "        # Process each column in the DataFrame\n",
    "        for column in df.columns:\n",
    "            try:\n",
    "                # Calculate SMA\n",
    "                sma_df = MultiFrequencyTechnicalIndicators.calculate_sma(\n",
    "                    df, column, windows=params['sma'],\n",
    "                    include_trend=True, include_crossovers=True\n",
    "                )\n",
    "                \n",
    "                # Calculate RSI\n",
    "                rsi_df = MultiFrequencyTechnicalIndicators.calculate_rsi(\n",
    "                    df, column, windows=params['rsi'],\n",
    "                    include_trend=True, include_zones=True\n",
    "                )\n",
    "                \n",
    "                # Calculate ROC\n",
    "                roc_df = MultiFrequencyTechnicalIndicators.calculate_roc(\n",
    "                    df, column, windows=params['roc'],\n",
    "                    include_trend=True, include_signal=True\n",
    "                )\n",
    "                \n",
    "                # Combine all indicators into the dictionary\n",
    "                for col in sma_df.columns:\n",
    "                    all_indicators[f\"{column}_{col}\"] = sma_df[col]\n",
    "                \n",
    "                for col in rsi_df.columns:\n",
    "                    all_indicators[f\"{column}_{col}\"] = rsi_df[col]\n",
    "                \n",
    "                for col in roc_df.columns:\n",
    "                    all_indicators[f\"{column}_{col}\"] = roc_df[col]\n",
    "                \n",
    "                print(f\"Applied indicators to {column}: {len(sma_df.columns) + len(rsi_df.columns) + len(roc_df.columns)} new features\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error applying indicators to {column}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        # Create the result DataFrame in one go to avoid fragmentation\n",
    "        result_df = pd.DataFrame(all_indicators, index=df.index)\n",
    "        \n",
    "        # Handle NaN values properly\n",
    "        if result_df.isna().any().any():\n",
    "            # Use proper forward fill and backward fill\n",
    "            result_df = result_df.ffill().bfill()\n",
    "            # If still have NaNs, fill with zeros\n",
    "            result_df = result_df.fillna(0)\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Module 6: GDP Forecast Evaluator\n",
    "#-----------------------------------------------------------------------------\n",
    "class GDPForecastEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluation framework for GDP forecasting models.\n",
    "    This class provides comprehensive evaluation metrics and visualizations\n",
    "    for GDP forecasting performance.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the evaluator.\"\"\"\n",
    "        self.results = {}\n",
    "        self.models = {}\n",
    "        self.actual = None\n",
    "    \n",
    "    def add_model(self, name, predictions, actual=None):\n",
    "        \"\"\"\n",
    "        Add a model's predictions for evaluation.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        name: str\n",
    "            Model name\n",
    "        predictions: pandas.Series\n",
    "            Predicted GDP values\n",
    "        actual: pandas.Series, optional\n",
    "            Actual GDP values (if not already set)\n",
    "        \"\"\"\n",
    "        self.models[name] = predictions\n",
    "        \n",
    "        if actual is not None and self.actual is None:\n",
    "            self.actual = actual\n",
    "    \n",
    "    def calculate_metrics(self, rolling_window=None):\n",
    "        \"\"\"\n",
    "        Calculate evaluation metrics for all models.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        rolling_window: int, optional\n",
    "            Window size for rolling metrics calculation\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        if self.actual is None:\n",
    "            raise ValueError(\"Actual values not set. Provide actual values when adding a model.\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for model_name, predictions in self.models.items():\n",
    "            # Align predictions with actual values\n",
    "            common_index = self.actual.index.intersection(predictions.index)\n",
    "            y_true = self.actual.loc[common_index]\n",
    "            y_pred = predictions.loc[common_index]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = self._calculate_model_metrics(y_true, y_pred, model_name)\n",
    "            \n",
    "            # Add rolling metrics if requested\n",
    "            if rolling_window is not None and len(y_true) > rolling_window:\n",
    "                rolling_metrics = self._calculate_rolling_metrics(y_true, y_pred, rolling_window)\n",
    "                metrics.update(rolling_metrics)\n",
    "            \n",
    "            results[model_name] = metrics\n",
    "        \n",
    "        self.results = results\n",
    "        return results\n",
    "    \n",
    "    def _calculate_model_metrics(self, y_true, y_pred, model_name):\n",
    "        \"\"\"\n",
    "        Calculate comprehensive evaluation metrics for a model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_true: pandas.Series\n",
    "            Actual values\n",
    "        y_pred: pandas.Series\n",
    "            Predicted values\n",
    "        model_name: str\n",
    "            Model name\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "        \n",
    "        # Calculate basic error metrics\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        # Calculate directional accuracy\n",
    "        direction_true = np.sign(y_true.diff().fillna(0))\n",
    "        direction_pred = np.sign(y_pred.diff().fillna(0))\n",
    "        \n",
    "        # Ignore zero changes\n",
    "        nonzero_mask = direction_true != 0\n",
    "        if nonzero_mask.any():\n",
    "            direction_accuracy = np.mean(direction_true[nonzero_mask] == direction_pred[nonzero_mask])\n",
    "        else:\n",
    "            direction_accuracy = np.nan\n",
    "        \n",
    "        # Calculate mean absolute percentage error\n",
    "        # Use a safe version to handle zeros\n",
    "        nonzero_mask = y_true != 0\n",
    "        if nonzero_mask.any():\n",
    "            mape = np.mean(np.abs((y_true[nonzero_mask] - y_pred[nonzero_mask]) / y_true[nonzero_mask])) * 100\n",
    "        else:\n",
    "            mape = np.nan\n",
    "        \n",
    "        # Calculate Theil's U statistic\n",
    "        # U = sqrt(MSE(model)) / sqrt(MSE(naive))\n",
    "        # Naive forecast is previous value (no change)\n",
    "        naive_pred = y_true.shift(1).fillna(method='bfill')\n",
    "        naive_mse = mean_squared_error(y_true[1:], naive_pred[1:])\n",
    "        \n",
    "        if naive_mse > 0:\n",
    "            theils_u = np.sqrt(mse) / np.sqrt(naive_mse)\n",
    "        else:\n",
    "            theils_u = np.nan\n",
    "        \n",
    "        # Calculate advanced forecast accuracy metrics\n",
    "        # Mean Directional Accuracy (MDA)\n",
    "        actual_changes = y_true.diff().fillna(0)\n",
    "        predicted_changes = y_pred.diff().fillna(0)\n",
    "        mda = np.mean((actual_changes * predicted_changes) > 0)\n",
    "        \n",
    "        # Confusion matrix for directional forecasts\n",
    "        direction_true_binary = (actual_changes > 0).astype(int)\n",
    "        direction_pred_binary = (predicted_changes > 0).astype(int)\n",
    "        \n",
    "        true_pos = np.sum((direction_true_binary == 1) & (direction_pred_binary == 1))\n",
    "        false_pos = np.sum((direction_true_binary == 0) & (direction_pred_binary == 1))\n",
    "        true_neg = np.sum((direction_true_binary == 0) & (direction_pred_binary == 0))\n",
    "        false_neg = np.sum((direction_true_binary == 1) & (direction_pred_binary == 0))\n",
    "        \n",
    "        # Hit rate (% of positive changes correctly predicted)\n",
    "        if (true_pos + false_neg) > 0:\n",
    "            hit_rate = true_pos / (true_pos + false_neg)\n",
    "        else:\n",
    "            hit_rate = np.nan\n",
    "        \n",
    "        # False alarm rate (% of negative changes incorrectly predicted as positive)\n",
    "        if (false_pos + true_neg) > 0:\n",
    "            false_alarm_rate = false_pos / (false_pos + true_neg)\n",
    "        else:\n",
    "            false_alarm_rate = np.nan\n",
    "        \n",
    "        # Calculate over/underprediction bias\n",
    "        bias = np.mean(y_pred - y_true)\n",
    "        \n",
    "        # Create results dictionary\n",
    "        metrics = {\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'mape': mape,\n",
    "            'r2': r2,\n",
    "            'direction_accuracy': direction_accuracy,\n",
    "            'theils_u': theils_u,\n",
    "            'mean_directional_accuracy': mda,\n",
    "            'hit_rate': hit_rate,\n",
    "            'false_alarm_rate': false_alarm_rate,\n",
    "            'bias': bias,\n",
    "            'confusion_matrix': {\n",
    "                'true_pos': true_pos,\n",
    "                'false_pos': false_pos,\n",
    "                'true_neg': true_neg,\n",
    "                'false_neg': false_neg\n",
    "            },\n",
    "            'forecast_errors': y_pred - y_true\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _calculate_rolling_metrics(self, y_true, y_pred, window):\n",
    "        \"\"\"\n",
    "        Calculate rolling evaluation metrics.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_true: pandas.Series\n",
    "            Actual values\n",
    "        y_pred: pandas.Series\n",
    "            Predicted values\n",
    "        window: int\n",
    "            Rolling window size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of rolling metrics\n",
    "        \"\"\"\n",
    "        # Initialize rolling metrics\n",
    "        rolling_rmse = []\n",
    "        rolling_mae = []\n",
    "        rolling_direction_accuracy = []\n",
    "        \n",
    "        # Loop through rolling windows\n",
    "        for i in range(len(y_true) - window + 1):\n",
    "            window_true = y_true.iloc[i:i+window]\n",
    "            window_pred = y_pred.iloc[i:i+window]\n",
    "            \n",
    "            # Calculate metrics for this window\n",
    "            mse = np.mean((window_true - window_pred) ** 2)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = np.mean(np.abs(window_true - window_pred))\n",
    "            \n",
    "            # Calculate directional accuracy\n",
    "            direction_true = np.sign(window_true.diff().fillna(0))\n",
    "            direction_pred = np.sign(window_pred.diff().fillna(0))\n",
    "            \n",
    "            # Ignore zero changes\n",
    "            nonzero_mask = direction_true != 0\n",
    "            if nonzero_mask.any():\n",
    "                direction_accuracy = np.mean(direction_true[nonzero_mask] == direction_pred[nonzero_mask])\n",
    "            else:\n",
    "                direction_accuracy = np.nan\n",
    "            \n",
    "            # Add to lists\n",
    "            rolling_rmse.append(rmse)\n",
    "            rolling_mae.append(mae)\n",
    "            rolling_direction_accuracy.append(direction_accuracy)\n",
    "        \n",
    "        # Convert to pandas Series with appropriate index\n",
    "        index = y_true.index[window-1:]\n",
    "        rolling_metrics = {\n",
    "            'rolling_rmse': pd.Series(rolling_rmse, index=index[:len(rolling_rmse)]),\n",
    "            'rolling_mae': pd.Series(rolling_mae, index=index[:len(rolling_mae)]),\n",
    "            'rolling_direction_accuracy': pd.Series(rolling_direction_accuracy, index=index[:len(rolling_direction_accuracy)])\n",
    "        }\n",
    "        \n",
    "        return rolling_metrics\n",
    "    \n",
    "    def diebold_mariano_test(self, model1, model2, alternative='two-sided'):\n",
    "        \"\"\"\n",
    "        Perform Diebold-Mariano test to compare forecast accuracy.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model1: str\n",
    "            First model name\n",
    "        model2: str\n",
    "            Second model name\n",
    "        alternative: str\n",
    "            Alternative hypothesis ('two-sided', 'less', 'greater')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            DM statistic and p-value\n",
    "        \"\"\"\n",
    "        import statsmodels.api as sm\n",
    "        from scipy.stats import norm\n",
    "        \n",
    "        if model1 not in self.models or model2 not in self.models:\n",
    "            raise ValueError(f\"Models {model1} and/or {model2} not found\")\n",
    "        \n",
    "        # Get predictions\n",
    "        pred1 = self.models[model1]\n",
    "        pred2 = self.models[model2]\n",
    "        \n",
    "        # Align predictions with actual values\n",
    "        common_index = self.actual.index.intersection(pred1.index).intersection(pred2.index)\n",
    "        y_true = self.actual.loc[common_index]\n",
    "        y_pred1 = pred1.loc[common_index]\n",
    "        y_pred2 = pred2.loc[common_index]\n",
    "        \n",
    "        # Calculate squared errors\n",
    "        error1 = (y_true - y_pred1) ** 2\n",
    "        error2 = (y_true - y_pred2) ** 2\n",
    "        \n",
    "        # Calculate loss differential\n",
    "        d = error1 - error2\n",
    "        \n",
    "        # Calculate DM statistic\n",
    "        n = len(d)\n",
    "        if n <= 1:\n",
    "            return np.nan, np.nan\n",
    "        \n",
    "        # Estimate lag-1 autocorrelation of loss differential\n",
    "        acf_result = sm.tsa.acf(d, nlags=1, fft=False)\n",
    "        gamma_0 = acf_result[0]  # This is the variance of d\n",
    "        gamma_1 = acf_result[1] * gamma_0  # Autocovariance at lag 1\n",
    "        \n",
    "        # Calculate long-run variance with Newey-West correction for autocorrelation\n",
    "        lrvar = gamma_0 + 2 * gamma_1\n",
    "        \n",
    "        # Calculate DM statistic\n",
    "        dm_stat = d.mean() / np.sqrt(lrvar / n)\n",
    "        \n",
    "        # Calculate p-value based on alternative hypothesis\n",
    "        if alternative == 'two-sided':\n",
    "            p_value = 2 * (1 - norm.cdf(np.abs(dm_stat)))\n",
    "        elif alternative == 'less':\n",
    "            p_value = norm.cdf(dm_stat)\n",
    "        elif alternative == 'greater':\n",
    "            p_value = 1 - norm.cdf(dm_stat)\n",
    "        else:\n",
    "            raise ValueError(\"alternative must be 'two-sided', 'less', or 'greater'\")\n",
    "        \n",
    "        return dm_stat, p_value\n",
    "    \n",
    "    def plot_forecasts(self, start_date=None, end_date=None, figsize=(12, 6)):\n",
    "        \"\"\"\n",
    "        Plot actual vs predicted GDP.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        start_date: str or datetime, optional\n",
    "            Start date for plot\n",
    "        end_date: str or datetime, optional\n",
    "            End date for plot\n",
    "        figsize: tuple\n",
    "            Figure size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            Figure object\n",
    "        \"\"\"\n",
    "        if self.actual is None:\n",
    "            raise ValueError(\"Actual values not set\")\n",
    "        \n",
    "        # Filter by date range if provided\n",
    "        actual = self.actual\n",
    "        if start_date is not None:\n",
    "            actual = actual[actual.index >= pd.to_datetime(start_date)]\n",
    "        if end_date is not None:\n",
    "            actual = actual[actual.index <= pd.to_datetime(end_date)]\n",
    "        \n",
    "        # Create plot\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        # Plot actual values\n",
    "        ax.plot(actual.index, actual, 'k-', linewidth=2, label='Actual GDP')\n",
    "        \n",
    "        # Plot predictions for each model\n",
    "        colors = plt.cm.tab10.colors\n",
    "        for i, (model_name, predictions) in enumerate(self.models.items()):\n",
    "            # Filter predictions by date range\n",
    "            pred = predictions\n",
    "            if start_date is not None:\n",
    "                pred = pred[pred.index >= pd.to_datetime(start_date)]\n",
    "            if end_date is not None:\n",
    "                pred = pred[pred.index <= pd.to_datetime(end_date)]\n",
    "            \n",
    "            # Only use shared dates\n",
    "            common_index = actual.index.intersection(pred.index)\n",
    "            pred = pred.loc[common_index]\n",
    "            \n",
    "            color = colors[i % len(colors)]\n",
    "            ax.plot(pred.index, pred, 'o-', color=color, linewidth=1.5, label=f'{model_name}')\n",
    "        \n",
    "        # Add recession shading if available\n",
    "        try:\n",
    "            from pandas_datareader.data import DataReader\n",
    "            from pandas_datareader._utils import RemoteDataError\n",
    "            \n",
    "            try:\n",
    "                # Get US recession data from FRED\n",
    "                recession = DataReader('USREC', 'fred', start=actual.index[0], end=actual.index[-1])\n",
    "                \n",
    "                # Create shaded regions for recessions\n",
    "                last_date = None\n",
    "                for date, value in recession.itertuples():\n",
    "                    if value == 1.0:  # Recession period\n",
    "                        if last_date is None:\n",
    "                            last_date = date\n",
    "                    elif last_date is not None:\n",
    "                        # End of recession period\n",
    "                        ax.axvspan(last_date, date, alpha=0.2, color='gray')\n",
    "                        last_date = None\n",
    "                \n",
    "                # Handle case where we're still in a recession at the end of the data\n",
    "                if last_date is not None:\n",
    "                    ax.axvspan(last_date, actual.index[-1], alpha=0.2, color='gray')\n",
    "            \n",
    "            except RemoteDataError:\n",
    "                print(\"Could not retrieve recession data from FRED\")\n",
    "        \n",
    "        except ImportError:\n",
    "            print(\"pandas_datareader not available for recession shading\")\n",
    "        \n",
    "        # Add legend, grid, labels, etc.\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('GDP Growth (%)')\n",
    "        ax.set_title('GDP Growth: Actual vs Predicted')\n",
    "        ax.legend(loc='best')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Format y-axis to show percentage\n",
    "        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.1f}%'))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def plot_error_distribution(self, figsize=(12, 8)):\n",
    "        \"\"\"\n",
    "        Plot error distributions for all models.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        figsize: tuple\n",
    "            Figure size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            Figure object\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            self.calculate_metrics()\n",
    "        \n",
    "        n_models = len(self.models)\n",
    "        fig, axes = plt.subplots(n_models, 2, figsize=figsize)\n",
    "        \n",
    "        # Handle case with single model\n",
    "        if n_models == 1:\n",
    "            axes = axes.reshape(1, 2)\n",
    "        \n",
    "        # Iterate through models\n",
    "        for i, (model_name, metrics) in enumerate(self.results.items()):\n",
    "            errors = metrics['forecast_errors']\n",
    "            \n",
    "            # Histogram of errors\n",
    "            bins = min(20, max(5, int(np.sqrt(len(errors)))))\n",
    "            axes[i, 0].hist(errors, bins=bins, alpha=0.7, edgecolor='black')\n",
    "            axes[i, 0].axvline(x=0, color='r', linestyle='--')\n",
    "            axes[i, 0].set_title(f'{model_name}: Error Distribution')\n",
    "            axes[i, 0].set_xlabel('Forecast Error (Predicted - Actual)')\n",
    "            axes[i, 0].set_ylabel('Frequency')\n",
    "            \n",
    "            # Add metrics to plot\n",
    "            metrics_text = (\n",
    "                f\"RMSE: {metrics['rmse']:.4f}\\n\"\n",
    "                f\"MAE: {metrics['mae']:.4f}\\n\"\n",
    "                f\"Bias: {metrics['bias']:.4f}\\n\"\n",
    "                f\"Dir. Acc: {metrics['direction_accuracy']:.2f}\"\n",
    "            )\n",
    "            \n",
    "            axes[i, 0].annotate(\n",
    "                metrics_text, xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                va='top', ha='left', bbox=dict(boxstyle='round', fc='white', alpha=0.7)\n",
    "            )\n",
    "            \n",
    "            # Q-Q plot\n",
    "            from scipy import stats\n",
    "            \n",
    "            # Get z-scores for normal distribution\n",
    "            z = (errors - errors.mean()) / errors.std()\n",
    "            \n",
    "            # Create Q-Q plot\n",
    "            stats.probplot(z, dist=\"norm\", plot=axes[i, 1])\n",
    "            axes[i, 1].set_title(f'{model_name}: Q-Q Plot')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def plot_rolling_metrics(self, window=8, figsize=(12, 15)):\n",
    "        \"\"\"\n",
    "        Plot rolling metrics for all models.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        window: int\n",
    "            Rolling window size\n",
    "        figsize: tuple\n",
    "            Figure size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "            Figure object\n",
    "        \"\"\"\n",
    "        # Ensure we have rolling metrics\n",
    "        if not self.results or 'rolling_rmse' not in next(iter(self.results.values())):\n",
    "            self.calculate_metrics(rolling_window=window)\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 1, figsize=figsize)\n",
    "        \n",
    "        # Plot rolling RMSE\n",
    "        for model_name, metrics in self.results.items():\n",
    "            axes[0].plot(\n",
    "                metrics['rolling_rmse'].index,\n",
    "                metrics['rolling_rmse'],\n",
    "                'o-',\n",
    "                label=model_name\n",
    "            )\n",
    "        \n",
    "        axes[0].set_title(f'Rolling RMSE ({window}-quarter window)')\n",
    "        axes[0].set_ylabel('RMSE')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].legend(loc='best')\n",
    "        \n",
    "        # Plot rolling MAE\n",
    "        for model_name, metrics in self.results.items():\n",
    "            axes[1].plot(\n",
    "                metrics['rolling_mae'].index,\n",
    "                metrics['rolling_mae'],\n",
    "                'o-',\n",
    "                label=model_name\n",
    "            )\n",
    "        \n",
    "        axes[1].set_title(f'Rolling MAE ({window}-quarter window)')\n",
    "        axes[1].set_ylabel('MAE')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].legend(loc='best')\n",
    "        \n",
    "        # Plot rolling direction accuracy\n",
    "        for model_name, metrics in self.results.items():\n",
    "            axes[2].plot(\n",
    "                metrics['rolling_direction_accuracy'].index,\n",
    "                metrics['rolling_direction_accuracy'],\n",
    "                'o-',\n",
    "                label=model_name\n",
    "            )\n",
    "        \n",
    "        axes[2].set_title(f'Rolling Direction Accuracy ({window}-quarter window)')\n",
    "        axes[2].set_ylabel('Direction Accuracy')\n",
    "        axes[2].set_ylim(0, 1)\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        axes[2].legend(loc='best')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def generate_report(self, output_file=None, include_plots=True):\n",
    "        \"\"\"\n",
    "        Generate comprehensive evaluation report.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_file: str, optional\n",
    "            Path to save report (HTML or markdown)\n",
    "        include_plots: bool\n",
    "            Whether to include plots in the report\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            Report content\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            self.calculate_metrics()\n",
    "        \n",
    "        # Start building report\n",
    "        report = \"# GDP Forecasting Model Evaluation Report\\n\\n\"\n",
    "        report += f\"Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\\n\\n\"\n",
    "        \n",
    "        # Add model summary\n",
    "        report += \"## Models Evaluated\\n\\n\"\n",
    "        report += f\"Number of models: {len(self.models)}\\n\"\n",
    "        report += f\"Evaluation period: {self.actual.index[0]} to {self.actual.index[-1]}\\n\"\n",
    "        report += f\"Number of observations: {len(self.actual)}\\n\\n\"\n",
    "        \n",
    "        # Add performance metrics table\n",
    "        report += \"## Performance Metrics\\n\\n\"\n",
    "        report += \"| Model | RMSE | MAE | MAPE | R | Direction Accuracy | Theil's U | Bias |\\n\"\n",
    "        report += \"|-------|------|-----|------|----|--------------------|-----------|------|\\n\"\n",
    "        \n",
    "        for model_name, metrics in self.results.items():\n",
    "            report += (\n",
    "                f\"| {model_name} | \"\n",
    "                f\"{metrics['rmse']:.4f} | \"\n",
    "                f\"{metrics['mae']:.4f} | \"\n",
    "                f\"{metrics['mape']:.2f}% | \"\n",
    "                f\"{metrics['r2']:.4f} | \"\n",
    "                f\"{metrics['direction_accuracy']:.2f} | \"\n",
    "                f\"{metrics['theils_u']:.4f} | \"\n",
    "                f\"{metrics['bias']:.4f} |\\n\"\n",
    "            )\n",
    "        \n",
    "        report += \"\\n\"\n",
    "        \n",
    "        # Add detailed analysis for each model\n",
    "        report += \"## Detailed Model Analysis\\n\\n\"\n",
    "        \n",
    "        for model_name, metrics in self.results.items():\n",
    "            report += f\"### {model_name}\\n\\n\"\n",
    "            \n",
    "            # Confusion matrix\n",
    "            cm = metrics['confusion_matrix']\n",
    "            report += \"#### Directional Forecast Confusion Matrix\\n\\n\"\n",
    "            report += \"| | Predicted Up | Predicted Down |\\n\"\n",
    "            report += \"|------------|--------------|----------------|\\n\"\n",
    "            report += f\"| **Actual Up** | {cm['true_pos']} | {cm['false_neg']} |\\n\"\n",
    "            report += f\"| **Actual Down** | {cm['false_pos']} | {cm['true_neg']} |\\n\\n\"\n",
    "            \n",
    "            # Additional metrics\n",
    "            report += \"#### Additional Metrics\\n\\n\"\n",
    "            report += f\"* Mean Directional Accuracy: {metrics['mean_directional_accuracy']:.4f}\\n\"\n",
    "            report += f\"* Hit Rate (% of Up movements correctly predicted): {metrics['hit_rate']:.4f}\\n\"\n",
    "            report += f\"* False Alarm Rate: {metrics['false_alarm_rate']:.4f}\\n\"\n",
    "            report += f\"* Bias (Average overestimation): {metrics['bias']:.4f}\\n\\n\"\n",
    "        \n",
    "        # Add model comparison using Diebold-Mariano test\n",
    "        if len(self.models) > 1:\n",
    "            report += \"## Model Comparison: Diebold-Mariano Test\\n\\n\"\n",
    "            report += \"| Model 1 | Model 2 | DM Statistic | p-value | Conclusion |\\n\"\n",
    "            report += \"|---------|---------|--------------|---------|------------|\\n\"\n",
    "            \n",
    "            models = list(self.models.keys())\n",
    "            for i in range(len(models)):\n",
    "                for j in range(i+1, len(models)):\n",
    "                    dm_stat, p_value = self.diebold_mariano_test(models[i], models[j])\n",
    "                    \n",
    "                    # Determine conclusion\n",
    "                    if p_value < 0.01:\n",
    "                        significance = \"***\"\n",
    "                    elif p_value < 0.05:\n",
    "                        significance = \"**\"\n",
    "                    elif p_value < 0.1:\n",
    "                        significance = \"*\"\n",
    "                    else:\n",
    "                        significance = \"\"\n",
    "                    \n",
    "                    if np.isnan(dm_stat) or np.isnan(p_value):\n",
    "                        conclusion = \"Insufficient data\"\n",
    "                    elif p_value < 0.05:\n",
    "                        if dm_stat > 0:\n",
    "                            conclusion = f\"Model 2 is more accurate {significance}\"\n",
    "                        else:\n",
    "                            conclusion = f\"Model 1 is more accurate {significance}\"\n",
    "                    else:\n",
    "                        conclusion = \"No significant difference\"\n",
    "                    \n",
    "                    report += (\n",
    "                        f\"| {models[i]} | {models[j]} | \"\n",
    "                        f\"{dm_stat:.4f} | {p_value:.4f} | {conclusion} |\\n\"\n",
    "                    )\n",
    "            \n",
    "            report += \"\\n*Significance levels: *** = 1%, ** = 5%, * = 10%\\n\\n\"\n",
    "        \n",
    "        # Add conclusion\n",
    "        report += \"## Conclusion\\n\\n\"\n",
    "        \n",
    "        # Determine best model based on metrics\n",
    "        rmse_ranking = {model: metrics['rmse'] for model, metrics in self.results.items()}\n",
    "        best_rmse = min(rmse_ranking.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        dir_acc_ranking = {model: metrics['direction_accuracy'] for model, metrics in self.results.items()}\n",
    "        best_dir_acc = max(dir_acc_ranking.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        report += f\"Based on RMSE, the best performing model is **{best_rmse}**.\\n\\n\"\n",
    "        report += f\"Based on directional accuracy, the best performing model is **{best_dir_acc}**.\\n\\n\"\n",
    "        \n",
    "        # If plots are included and an output file is specified\n",
    "        if include_plots and output_file:\n",
    "            # Save plots to files\n",
    "            import os\n",
    "            \n",
    "            output_dir = os.path.dirname(output_file)\n",
    "            if output_dir and not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            \n",
    "            # Base file name without extension\n",
    "            base_name = os.path.splitext(output_file)[0]\n",
    "            \n",
    "            # Forecast plot\n",
    "            forecast_plot_path = f\"{base_name}_forecasts.png\"\n",
    "            fig = self.plot_forecasts()\n",
    "            fig.savefig(forecast_plot_path)\n",
    "            plt.close(fig)\n",
    "            \n",
    "            # Error distribution plot\n",
    "            error_plot_path = f\"{base_name}_errors.png\"\n",
    "            fig = self.plot_error_distribution()\n",
    "            fig.savefig(error_plot_path)\n",
    "            plt.close(fig)\n",
    "            \n",
    "            # Rolling metrics plot\n",
    "            rolling_plot_path = f\"{base_name}_rolling.png\"\n",
    "            fig = self.plot_rolling_metrics()\n",
    "            fig.savefig(rolling_plot_path)\n",
    "            plt.close(fig)\n",
    "            \n",
    "            # Add images to report\n",
    "            report += \"## Visualizations\\n\\n\"\n",
    "            report += \"### Forecast Comparison\\n\\n\"\n",
    "            report += f\"![Forecast Comparison]({os.path.basename(forecast_plot_path)})\\n\\n\"\n",
    "            report += \"### Error Distribution\\n\\n\"\n",
    "            report += f\"![Error Distribution]({os.path.basename(error_plot_path)})\\n\\n\"\n",
    "            report += \"### Rolling Metrics\\n\\n\"\n",
    "            report += f\"![Rolling Metrics]({os.path.basename(rolling_plot_path)})\\n\\n\"\n",
    "        \n",
    "        # Save report to file if specified\n",
    "        if output_file:\n",
    "            with open(output_file, 'w') as f:\n",
    "                f.write(report)\n",
    "        \n",
    "        return report\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Monthly Factor Model\n",
    "#-----------------------------------------------------------------------------\n",
    "class MonthlyFactorModel:\n",
    "    \"\"\"\n",
    "    Simplified Dynamic Factor Model for working with monthly data only.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_factors=3, max_iter=100, tol=1e-4, random_state=None):\n",
    "        \"\"\"\n",
    "        Initialize the Monthly Factor Model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_factors: int\n",
    "            Number of factors to extract\n",
    "        max_iter: int\n",
    "            Maximum number of iterations\n",
    "        tol: float\n",
    "            Tolerance for convergence\n",
    "        random_state: int or None\n",
    "            Random state for initialization\n",
    "        \"\"\"\n",
    "        self.n_factors = n_factors\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        self.loadings = None\n",
    "        self.factors = None\n",
    "        self.column_names = None\n",
    "        self.index = None\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the model using PCA instead of full DFM to avoid memory issues.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: DataFrame or ndarray\n",
    "            Data matrix (time  variables)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self\n",
    "            Fitted model instance\n",
    "        \"\"\"\n",
    "        # Convert to numpy array if DataFrame\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.column_names = X.columns\n",
    "            self.index = X.index\n",
    "            X_values = X.values\n",
    "        else:\n",
    "            self.column_names = [f\"Var{i}\" for i in range(X.shape[1])]\n",
    "            self.index = np.arange(X.shape[0])\n",
    "            X_values = X\n",
    "        \n",
    "        # Standardize data\n",
    "        X_std = (X_values - np.nanmean(X_values, axis=0)) / np.nanstd(X_values, axis=0)\n",
    "        \n",
    "        # Handle missing values\n",
    "        X_filled = np.nan_to_num(X_std, nan=0.0)\n",
    "        \n",
    "        # Use PCA to extract factors (simpler and memory efficient)\n",
    "        pca = PCA(n_components=self.n_factors, random_state=self.random_state)\n",
    "        self.factors = pca.fit_transform(X_filled)\n",
    "        self.loadings = pca.components_.T\n",
    "        \n",
    "        # Create factors DataFrame\n",
    "        self.factors_df = pd.DataFrame(\n",
    "            self.factors, \n",
    "            index=self.index,\n",
    "            columns=[f\"Factor{i+1}\" for i in range(self.n_factors)]\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X=None):\n",
    "        \"\"\"\n",
    "        Extract factors from data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: DataFrame or ndarray, optional\n",
    "            New data to transform. If None, use the data used for fitting.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        ndarray\n",
    "            Extracted factors\n",
    "        \"\"\"\n",
    "        if X is None:\n",
    "            # Return factors estimated during fitting\n",
    "            return self.factors\n",
    "        \n",
    "        # Convert to numpy array if DataFrame\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X_values = X.values\n",
    "        else:\n",
    "            X_values = X\n",
    "        \n",
    "        # Standardize data\n",
    "        X_std = (X_values - np.nanmean(X_values, axis=0)) / np.nanstd(X_values, axis=0)\n",
    "        \n",
    "        # Handle missing values\n",
    "        X_filled = np.nan_to_num(X_std, nan=0.0)\n",
    "        \n",
    "        # Project data onto loadings\n",
    "        factors = X_filled @ self.loadings\n",
    "        \n",
    "        return factors\n",
    "    \n",
    "    def get_factor_loadings(self):\n",
    "        \"\"\"\n",
    "        Get factor loadings as a DataFrame.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            Factor loadings\n",
    "        \"\"\"\n",
    "        if self.loadings is None:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        factor_names = [f\"Factor{i+1}\" for i in range(self.n_factors)]\n",
    "        return pd.DataFrame(self.loadings, index=self.column_names, columns=factor_names)\n",
    "    \n",
    "    def get_factors(self):\n",
    "        \"\"\"\n",
    "        Get extracted factors as a DataFrame.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            Extracted factors\n",
    "        \"\"\"\n",
    "        if self.factors is None:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        factor_names = [f\"Factor{i+1}\" for i in range(self.n_factors)]\n",
    "        return pd.DataFrame(self.factors, index=self.index, columns=factor_names)\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Simplified MIDAS Regressor\n",
    "#-----------------------------------------------------------------------------\n",
    "class SimplifiedMIDASRegressor:\n",
    "    \"\"\"\n",
    "    Simplified MIDAS regression for mixed-frequency time series.\n",
    "    \"\"\"\n",
    "    def __init__(self, weight_function='exponential_almon', max_lags=12,\n",
    "                n_weight_params=2, ar_lags=4, regularization=0.0,\n",
    "                max_iter=1000, tol=1e-6, random_state=None):\n",
    "        \"\"\"\n",
    "        Initialize simplified MIDAS regressor.\n",
    "        \"\"\"\n",
    "        self.max_lags = max_lags\n",
    "        self.n_weight_params = n_weight_params\n",
    "        self.ar_lags = ar_lags\n",
    "        self.regularization = regularization\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Set weighting function\n",
    "        if weight_function == 'exponential_almon':\n",
    "            self.weight_function = self._exponential_almon_weights\n",
    "        elif callable(weight_function):\n",
    "            self.weight_function = weight_function\n",
    "        else:\n",
    "            raise ValueError(\"weight_function must be 'exponential_almon' or a callable\")\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.weight_params = None\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.fit_intercept = True\n",
    "        \n",
    "        # Initialize random number generator\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "    \n",
    "    def _exponential_almon_weights(self, lag, params):\n",
    "        \"\"\"\n",
    "        Exponential Almon lag polynomial weighting function.\n",
    "        \"\"\"\n",
    "        if len(params) < 2:\n",
    "            # Need at least two parameters\n",
    "            params = np.array([params[0], 0.0])\n",
    "        \n",
    "        # Normalize lags to [0, 1]\n",
    "        x = lag / (self.max_lags - 1) if self.max_lags > 1 else 0\n",
    "        \n",
    "        # Calculate weights\n",
    "        exponent = params[0] * x + params[1] * x**2\n",
    "        weights = np.exp(exponent)\n",
    "        \n",
    "        # Normalize weights to sum to 1\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def _aggregate_high_frequency(self, X_hf, weight_params):\n",
    "        \"\"\"\n",
    "        Aggregate high-frequency variables using weighting function with explicit type handling.\n",
    "        \"\"\"\n",
    "        n_samples = X_hf[0].shape[0]\n",
    "        n_hf_vars = len(X_hf)\n",
    "        \n",
    "        # Initialize aggregated variables\n",
    "        X_aggregated = np.zeros((n_samples, n_hf_vars), dtype=np.float64)\n",
    "        \n",
    "        # Calculate weights\n",
    "        lags = np.arange(self.max_lags)\n",
    "        weights = self.weight_function(lags, weight_params)\n",
    "        weights = np.asarray(weights, dtype=np.float64)\n",
    "        \n",
    "        # Apply weights to each high-frequency variable\n",
    "        for i, X_var in enumerate(X_hf):\n",
    "            # Ensure X_var is float64\n",
    "            X_var_float = np.asarray(X_var, dtype=np.float64)\n",
    "            \n",
    "            # Weighted sum across lags\n",
    "            X_aggregated[:, i] = np.sum(X_var_float * weights, axis=1)\n",
    "        \n",
    "        return X_aggregated\n",
    "    \n",
    "    def _objective_function(self, weight_params, X_hf, X_ar, y):\n",
    "        \"\"\"\n",
    "        Objective function for MIDAS parameter optimization with NaN tracing.\n",
    "        \"\"\"\n",
    "        # Add debugging for arrays\n",
    "        for i, X in enumerate(X_hf):\n",
    "            if np.isnan(X).any():\n",
    "                print(f\"WARNING: X_hf[{i}] has {np.isnan(X).sum()} NaNs in objective function\")\n",
    "        \n",
    "        if X_ar is not None and np.isnan(X_ar).any():\n",
    "            print(f\"WARNING: X_ar has {np.isnan(X_ar).sum()} NaNs in objective function\")\n",
    "        \n",
    "        X_hf_processed = []\n",
    "        for X in X_hf:\n",
    "            X_hf_processed.append(np.asarray(X, dtype=np.float64))\n",
    "        \n",
    "        if X_ar is not None:\n",
    "            X_ar = np.asarray(X_ar, dtype=np.float64)\n",
    "        \n",
    "        # Aggregate high-frequency variables with type-safe arrays\n",
    "        X_midas = self._aggregate_high_frequency(X_hf_processed, weight_params)\n",
    "        \n",
    "        # Continue with regular processing...\n",
    "        if X_ar is not None:\n",
    "            X = np.column_stack([X_ar, X_midas])\n",
    "        else:\n",
    "            X = X_midas\n",
    "        \n",
    "        # Add intercept\n",
    "        if self.fit_intercept:\n",
    "            X = np.column_stack([np.ones(X.shape[0]), X])\n",
    "        \n",
    "        # Regularized regression\n",
    "        try:\n",
    "            reg_term = self.regularization * np.eye(X.shape[1])\n",
    "            XtX = X.T @ X + reg_term\n",
    "            Xty = X.T @ y\n",
    "            coef = np.linalg.solve(XtX, Xty)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # If direct solve fails, use pseudoinverse\n",
    "            coef = np.linalg.pinv(XtX) @ Xty\n",
    "        \n",
    "        # Calculate predictions\n",
    "        y_pred = X @ coef\n",
    "        \n",
    "        # Calculate MSE\n",
    "        mse = np.mean((y - y_pred) ** 2)\n",
    "        return mse\n",
    "    \n",
    "    def fit(self, X_hf, y, X_ar=None):\n",
    "        \"\"\"\n",
    "        Fit MIDAS regression model with proper type handling.\n",
    "        \"\"\"\n",
    "        # Convert target to numeric array\n",
    "        if isinstance(y, pd.Series) or isinstance(y, pd.DataFrame):\n",
    "            y = y.values.flatten().astype(np.float64)\n",
    "        else:\n",
    "            y = np.asarray(y, dtype=np.float64).flatten()\n",
    "        \n",
    "        # Process high-frequency variables with explicit type conversion\n",
    "        X_hf_arrays = []\n",
    "        for X in X_hf:\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X_hf_arrays.append(X.values.astype(np.float64))\n",
    "            else:\n",
    "                X_hf_arrays.append(np.asarray(X, dtype=np.float64))\n",
    "        \n",
    "        # Process autoregressive features with explicit type conversion\n",
    "        if X_ar is not None:\n",
    "            if isinstance(X_ar, pd.DataFrame):\n",
    "                X_ar = X_ar.values.astype(np.float64)\n",
    "            else:\n",
    "                X_ar = np.asarray(X_ar, dtype=np.float64)\n",
    "        \n",
    "        # Initialize weight parameters\n",
    "        init_params = self.rng.normal(0, 0.01, self.n_weight_params)\n",
    "        \n",
    "        # Use bounded optimization\n",
    "        bounds = [(-5, 5)] * self.n_weight_params\n",
    "        \n",
    "        # Try different optimization methods\n",
    "        optimization_methods = ['BFGS', 'Nelder-Mead', 'Powell']\n",
    "        \n",
    "        for method in optimization_methods:\n",
    "            try:\n",
    "                # Use a method that doesn't rely on SVD for gradients\n",
    "                result = minimize(\n",
    "                    self._objective_function,\n",
    "                    init_params,\n",
    "                    args=(X_hf_arrays, X_ar, y),\n",
    "                    method=method,\n",
    "                    options={'maxiter': self.max_iter, 'gtol': self.tol}\n",
    "                )\n",
    "                \n",
    "                if result.success:\n",
    "                    self.weight_params = result.x\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Optimization with {method} failed: {e}.\")\n",
    "        \n",
    "        # If all optimization methods failed, use a simple exponential decay\n",
    "        if not hasattr(self, 'weight_params') or self.weight_params is None:\n",
    "            print(\"All optimization methods failed. Using default exponential decay weights.\")\n",
    "            self.weight_params = np.array([-1.0, -0.5])  # Simple exponential decay\n",
    "        \n",
    "        # Calculate final weights\n",
    "        lags = np.arange(self.max_lags)\n",
    "        self.weights_ = self.weight_function(lags, self.weight_params)\n",
    "        \n",
    "        # Aggregate high-frequency variables with optimized weights\n",
    "        X_midas = self._aggregate_high_frequency(X_hf_arrays, self.weight_params)\n",
    "        \n",
    "        # Combine with autoregressive features\n",
    "        if X_ar is not None:\n",
    "            X = np.column_stack([X_ar, X_midas])\n",
    "        else:\n",
    "            X = X_midas\n",
    "        \n",
    "        # Ensure final regression matrix is float64\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        \n",
    "        # Fit Ridge regression\n",
    "        ridge = Ridge(alpha=max(self.regularization, 1e-5), fit_intercept=self.fit_intercept)\n",
    "        ridge.fit(X, y)\n",
    "        \n",
    "        # Store coefficients\n",
    "        if self.fit_intercept:\n",
    "            self.intercept_ = ridge.intercept_\n",
    "            self.coef_ = ridge.coef_\n",
    "        else:\n",
    "            self.intercept_ = 0.0\n",
    "            self.coef_ = ridge.coef_\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_hf, X_ar=None):\n",
    "        \"\"\"\n",
    "        Make predictions with fitted model.\n",
    "        \"\"\"\n",
    "        if self.weight_params is None:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        # Convert inputs to numpy arrays if needed\n",
    "        X_hf_arrays = []\n",
    "        for X in X_hf:\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X_hf_arrays.append(X.values)\n",
    "            else:\n",
    "                X_hf_arrays.append(np.asarray(X))\n",
    "        \n",
    "        if X_ar is not None:\n",
    "            if isinstance(X_ar, pd.DataFrame):\n",
    "                X_ar = X_ar.values\n",
    "            else:\n",
    "                X_ar = np.asarray(X_ar)\n",
    "        \n",
    "        # Aggregate high-frequency variables with fitted weights\n",
    "        X_midas = self._aggregate_high_frequency(X_hf_arrays, self.weight_params)\n",
    "        \n",
    "        # Combine with autoregressive features\n",
    "        if X_ar is not None:\n",
    "            X = np.column_stack([X_ar, X_midas])\n",
    "        else:\n",
    "            X = X_midas\n",
    "        \n",
    "        # Make predictions\n",
    "        if self.fit_intercept:\n",
    "            y_pred = self.intercept_ + X @ self.coef_\n",
    "        else:\n",
    "            y_pred = X @ self.coef_\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def get_midas_weights(self):\n",
    "        \"\"\"\n",
    "        Get the MIDAS weighting function parameters and weights.\n",
    "        \"\"\"\n",
    "        if self.weight_params is None:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        lags = np.arange(self.max_lags)\n",
    "        weights = self.weight_function(lags, self.weight_params)\n",
    "        \n",
    "        return {\n",
    "            'parameters': self.weight_params,\n",
    "            'weights': weights,\n",
    "            'lags': lags\n",
    "        }\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Monthly GDP Predictor\n",
    "#-----------------------------------------------------------------------------\n",
    "class MonthlyGDPPredictor:\n",
    "    \"\"\"\n",
    "    Simplified GDP prediction system that only uses monthly data.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                monthly_factors=3,\n",
    "                gdp_ar_lags=4,\n",
    "                use_midas=True, \n",
    "                midas_max_lags=6, \n",
    "                random_state=None):\n",
    "        \"\"\"\n",
    "        Initialize the GDP prediction system.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        monthly_factors: int\n",
    "            Number of factors to extract from monthly data\n",
    "        gdp_ar_lags: int\n",
    "            Autoregressive lags for GDP\n",
    "        use_midas: bool\n",
    "            Whether to use MIDAS for the final GDP prediction\n",
    "        midas_max_lags: int\n",
    "            Maximum number of lags for MIDAS\n",
    "        random_state: int or None\n",
    "            Random state for reproducibility\n",
    "        \"\"\"\n",
    "        self.monthly_factors = monthly_factors\n",
    "        self.gdp_ar_lags = gdp_ar_lags\n",
    "        self.use_midas = use_midas\n",
    "        self.midas_max_lags = midas_max_lags\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Initialize models\n",
    "        self.monthly_model = MonthlyFactorModel(\n",
    "            n_factors=monthly_factors,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Initialize GDP model based on configuration\n",
    "        if use_midas:\n",
    "            self.gdp_model = SimplifiedMIDASRegressor(\n",
    "                weight_function='exponential_almon',\n",
    "                max_lags=midas_max_lags,\n",
    "                n_weight_params=2,\n",
    "                ar_lags=gdp_ar_lags,\n",
    "                regularization=0.01,\n",
    "                random_state=random_state\n",
    "            )\n",
    "        else:\n",
    "            # Use Ridge regression as fallback\n",
    "            self.gdp_model = Ridge(alpha=0.01)\n",
    "        \n",
    "        # Storage for fitted factors\n",
    "        self.monthly_factors_df = None\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit_monthly_model(self, monthly_df):\n",
    "        \"\"\"\n",
    "        Fit the monthly factor model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        monthly_df: pandas.DataFrame\n",
    "            Monthly data with technical indicators\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            Extracted monthly factors\n",
    "        \"\"\"\n",
    "        print(f\"Fitting monthly model with {monthly_df.shape[1]} features\")\n",
    "        self.monthly_model.fit(monthly_df)\n",
    "        self.monthly_factors_df = self.monthly_model.get_factors()\n",
    "        \n",
    "        # Add debugging\n",
    "        trace_nans(\"Monthly data before factor extraction\", monthly_df)\n",
    "        \n",
    "        print(f\"Fitting monthly model with {monthly_df.shape[1]} features\")\n",
    "        self.monthly_model.fit(monthly_df)\n",
    "        self.monthly_factors_df = self.monthly_model.get_factors()\n",
    "        \n",
    "        # Add debugging\n",
    "        trace_nans(\"Monthly factors after extraction\", self.monthly_factors_df)\n",
    "        \n",
    "        print(f\"Extracted {self.monthly_factors_df.shape[1]} monthly factors\")\n",
    "        return self.monthly_factors_df\n",
    "    \n",
    "    def fit_gdp_model(self, gdp_series, monthly_factors, use_ar=True):\n",
    "        \"\"\"\n",
    "        Fit the GDP prediction model without using synthetic data.\n",
    "        \"\"\"\n",
    "        # Align indices\n",
    "        common_index = gdp_series.index.intersection(monthly_factors.index)\n",
    "        y = gdp_series.loc[common_index]\n",
    "        X_monthly = monthly_factors.loc[common_index]\n",
    "        \n",
    "        if self.use_midas:\n",
    "            # Prepare data for MIDAS model\n",
    "            X_lags = []\n",
    "            for col in X_monthly.columns:\n",
    "                # Create lag matrix for each factor\n",
    "                lag_matrix = pd.DataFrame(index=X_monthly.index)\n",
    "                for lag in range(self.midas_max_lags):\n",
    "                    lag_matrix[f\"{col}_lag{lag}\"] = X_monthly[col].shift(lag)\n",
    "                X_lags.append(lag_matrix.values)\n",
    "            \n",
    "            # Prepare autoregressive features\n",
    "            if use_ar and self.gdp_ar_lags > 0:\n",
    "                X_ar = pd.DataFrame(index=y.index)\n",
    "                for lag in range(1, self.gdp_ar_lags + 1):\n",
    "                    X_ar[f\"GDP_lag{lag}\"] = y.shift(lag)\n",
    "                \n",
    "                # Determine maximum lag period\n",
    "                max_lag = max(self.midas_max_lags, self.gdp_ar_lags)\n",
    "                \n",
    "                # Skip the first 'max_lag' periods to eliminate all NaNs\n",
    "                # This is the key change - trim data rather than fill NaNs\n",
    "                valid_indices = X_ar.index[max_lag:]\n",
    "                \n",
    "                # Filter all data to these valid indices\n",
    "                y_valid = y.loc[valid_indices]\n",
    "                X_ar_valid = X_ar.loc[valid_indices]\n",
    "                X_lags_valid = [X[max_lag:] for X in X_lags]\n",
    "                \n",
    "                print(f\"Fitting MIDAS model with {len(X_lags_valid)} monthly factors, {X_ar_valid.shape[1]} GDP lags\")\n",
    "                print(f\"Using {len(y_valid)} observations after trimming {max_lag} periods with lag-induced NaNs\")\n",
    "                \n",
    "                self.gdp_model.fit(X_lags_valid, y_valid, X_ar_valid)\n",
    "            else:\n",
    "                # No autoregressive features, but still trim for factor lags\n",
    "                valid_indices = X_monthly.index[self.midas_max_lags:]\n",
    "                y_valid = y.loc[valid_indices]\n",
    "                X_lags_valid = [X[self.midas_max_lags:] for X in X_lags]\n",
    "                \n",
    "                print(f\"Fitting MIDAS model with {len(X_lags_valid)} monthly factors\")\n",
    "                print(f\"Using {len(y_valid)} observations after trimming {self.midas_max_lags} periods with lag-induced NaNs\")\n",
    "                \n",
    "                self.gdp_model.fit(X_lags_valid, y_valid)\n",
    "        else:\n",
    "            # Using standard Ridge regression\n",
    "            # Handle autoregressive features\n",
    "            if use_ar and self.gdp_ar_lags > 0:\n",
    "                X_ar = pd.DataFrame(index=y.index)\n",
    "                for lag in range(1, self.gdp_ar_lags + 1):\n",
    "                    X_ar[f\"GDP_lag{lag}\"] = y.shift(lag)\n",
    "                \n",
    "                # Combine with monthly factors\n",
    "                X_combined = pd.concat([X_monthly, X_ar], axis=1)\n",
    "                \n",
    "                # Skip the first 'gdp_ar_lags' periods to eliminate all NaNs\n",
    "                valid_indices = X_combined.index[self.gdp_ar_lags:]\n",
    "                X_combined_valid = X_combined.loc[valid_indices]\n",
    "                y_valid = y.loc[valid_indices]\n",
    "                \n",
    "                print(f\"Fitting Ridge regression with {X_combined_valid.shape[1]} features\")\n",
    "                print(f\"Using {len(y_valid)} observations after trimming {self.gdp_ar_lags} periods with lag-induced NaNs\")\n",
    "                \n",
    "                self.gdp_model.fit(X_combined_valid, y_valid)\n",
    "            else:\n",
    "                # No lagged features, use data as is\n",
    "                self.gdp_model.fit(X_monthly, y)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def fit(self, monthly_df, gdp_series, align_dates=True, use_ar=True):\n",
    "        \"\"\"\n",
    "        Fit the complete model with auto-detection of valid date range.\n",
    "        \"\"\"\n",
    "        # 1. Extract monthly factors\n",
    "        monthly_factors = self.fit_monthly_model(monthly_df)\n",
    "        \n",
    "        # 2. Align monthly factors to quarterly GDP dates\n",
    "        if align_dates:\n",
    "            # Find quarterly dates\n",
    "            quarterly_dates = gdp_series.index\n",
    "            \n",
    "            # Create aligned DataFrame with the same columns as monthly_factors\n",
    "            aligned_monthly_factors = pd.DataFrame(\n",
    "                index=quarterly_dates, \n",
    "                columns=monthly_factors.columns\n",
    "            )\n",
    "            \n",
    "            # Align monthly factors to quarterly dates\n",
    "            for quarterly_date in quarterly_dates:\n",
    "                monthly_data = monthly_factors[monthly_factors.index <= quarterly_date]\n",
    "                if not monthly_data.empty:\n",
    "                    # Get the last row of data for each column\n",
    "                    for col in monthly_factors.columns:\n",
    "                        aligned_monthly_factors.loc[quarterly_date, col] = monthly_data.iloc[-1][col]\n",
    "            \n",
    "            # Check for NaNs after alignment to auto-detect valid date range\n",
    "            nan_rows = aligned_monthly_factors.isna().any(axis=1)\n",
    "            if nan_rows.any():\n",
    "                # Find first date where all data is available\n",
    "                first_valid_date = aligned_monthly_factors[~nan_rows].index[0]\n",
    "                print(f\"Auto-detected start date: {first_valid_date} (first quarter with complete data)\")\n",
    "                \n",
    "                # Filter to only use data from the valid range\n",
    "                aligned_monthly_factors = aligned_monthly_factors.loc[first_valid_date:]\n",
    "                gdp_series_valid = gdp_series.loc[first_valid_date:]\n",
    "                \n",
    "                print(f\"Using {len(aligned_monthly_factors)} quarters of data\")\n",
    "            else:\n",
    "                gdp_series_valid = gdp_series\n",
    "        else:\n",
    "            aligned_monthly_factors = monthly_factors\n",
    "            gdp_series_valid = gdp_series\n",
    "        \n",
    "        # 3. Fit GDP model with monthly factors\n",
    "        self.fit_gdp_model(gdp_series_valid, aligned_monthly_factors, use_ar)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, monthly_df=None, gdp_history=None, predict_date=None):\n",
    "        \"\"\"\n",
    "        Generate GDP predictions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        monthly_df: pandas.DataFrame, optional\n",
    "            Monthly data for prediction period\n",
    "        gdp_history: pandas.Series, optional\n",
    "            Historical GDP data for autoregressive features\n",
    "        predict_date: datetime or str, optional\n",
    "            Date for which to generate prediction\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.Series\n",
    "            GDP growth predictions\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model has not been fitted yet. Call fit() first.\")\n",
    "        \n",
    "        # 1. Process monthly data and extract factors\n",
    "        if monthly_df is not None:\n",
    "            # Transform monthly data to factors\n",
    "            monthly_factors = pd.DataFrame(\n",
    "                self.monthly_model.transform(monthly_df),\n",
    "                index=monthly_df.index,\n",
    "                columns=[f\"MonthlyFactor{i+1}\" for i in range(self.monthly_factors)]\n",
    "            )\n",
    "        else:\n",
    "            # Use existing monthly factors\n",
    "            monthly_factors = self.monthly_factors_df\n",
    "        \n",
    "        # 2. Filter data if needed\n",
    "        if predict_date is not None:\n",
    "            # Filter data up to predict_date\n",
    "            monthly_factors = monthly_factors[monthly_factors.index <= predict_date]\n",
    "        \n",
    "        # 3. Use monthly factors to predict GDP\n",
    "        if self.use_midas:\n",
    "            # Prepare data for MIDAS model\n",
    "            # We need to create lag structure for monthly factors\n",
    "            X_lags = []\n",
    "            for col in monthly_factors.columns:\n",
    "                # Create lag matrix for each factor\n",
    "                lag_matrix = pd.DataFrame(index=monthly_factors.index)\n",
    "                for lag in range(self.midas_max_lags):\n",
    "                    lag_matrix[f\"{col}_lag{lag}\"] = monthly_factors[col].shift(lag)\n",
    "                # Forward fill any NaNs at the beginning\n",
    "                lag_matrix = lag_matrix.fillna(method='ffill')\n",
    "                X_lags.append(lag_matrix.values)\n",
    "            \n",
    "            # Prepare autoregressive features if needed\n",
    "            if gdp_history is not None and self.gdp_ar_lags > 0:\n",
    "                X_ar = pd.DataFrame(index=monthly_factors.index)\n",
    "                for lag in range(1, self.gdp_ar_lags + 1):\n",
    "                    X_ar[f\"GDP_lag{lag}\"] = gdp_history.shift(lag)\n",
    "                # Forward fill any NaNs at the beginning\n",
    "                X_ar = X_ar.fillna(method='ffill')\n",
    "                \n",
    "                # Make prediction\n",
    "                gdp_pred = self.gdp_model.predict(X_lags, X_ar)\n",
    "            else:\n",
    "                # No autoregressive features\n",
    "                gdp_pred = self.gdp_model.predict(X_lags)\n",
    "        else:\n",
    "            # Using standard Ridge regression\n",
    "            # Prepare autoregressive features if needed\n",
    "            if gdp_history is not None and self.gdp_ar_lags > 0:\n",
    "                X_ar = pd.DataFrame(index=monthly_factors.index)\n",
    "                for lag in range(1, self.gdp_ar_lags + 1):\n",
    "                    X_ar[f\"GDP_lag{lag}\"] = gdp_history.shift(lag)\n",
    "                # Combine with monthly factors\n",
    "                X_combined = pd.concat([monthly_factors, X_ar], axis=1)\n",
    "            else:\n",
    "                X_combined = monthly_factors\n",
    "            \n",
    "            # Handle NaNs\n",
    "            X_combined = X_combined.fillna(method='ffill')\n",
    "            \n",
    "            # Make prediction\n",
    "            gdp_pred = self.gdp_model.predict(X_combined)\n",
    "        \n",
    "        # Convert to pandas Series\n",
    "        gdp_predictions = pd.Series(gdp_pred, index=monthly_factors.index, name=\"GDP_prediction\")\n",
    "        \n",
    "        return gdp_predictions\n",
    "    \n",
    "    def get_factor_loadings(self):\n",
    "        \"\"\"\n",
    "        Get factor loadings.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of factor loadings\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        loadings = {\n",
    "            'monthly': self.monthly_model.get_factor_loadings()\n",
    "        }\n",
    "        \n",
    "        return loadings\n",
    "    \n",
    "    def get_factors(self):\n",
    "        \"\"\"\n",
    "        Get extracted factors.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of factors\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        factors = {\n",
    "            'monthly': self.monthly_factors_df\n",
    "        }\n",
    "        \n",
    "        return factors\n",
    "    \n",
    "    def get_midas_weights(self):\n",
    "        \"\"\"\n",
    "        Get MIDAS weights if using MIDAS model.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of MIDAS weights\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        if not self.use_midas:\n",
    "            return None\n",
    "        \n",
    "        return self.gdp_model.get_midas_weights()\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Main Workflow Function\n",
    "#-----------------------------------------------------------------------------\n",
    "def run_gdp_forecast_workflow_monthly_only(\n",
    "    data_folder,\n",
    "    output_folder='./output',\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    train_test_split=0.8,\n",
    "    use_midas=True,\n",
    "    monthly_factors=3,\n",
    "    gdp_ar_lags=4,\n",
    "    random_state=42,\n",
    "    save_models=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a simplified GDP forecasting workflow using only monthly data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_folder: str\n",
    "        Path to the data folder\n",
    "    output_folder: str\n",
    "        Path to the output folder\n",
    "    start_date: str or None\n",
    "        Start date for analysis\n",
    "    end_date: str or None\n",
    "        End date for analysis\n",
    "    train_test_split: float\n",
    "        Proportion of data to use for training\n",
    "    use_midas: bool\n",
    "        Whether to use MIDAS for GDP prediction\n",
    "    monthly_factors: int\n",
    "        Number of factors to extract from monthly data\n",
    "    gdp_ar_lags: int\n",
    "        Number of autoregressive lags for GDP\n",
    "    random_state: int\n",
    "        Random seed for reproducibility\n",
    "    save_models: bool\n",
    "        Whether to save the models\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (evaluator, models, preprocessor)\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Set up logging\n",
    "    log_file = os.path.join(output_folder, 'workflow_log.txt')\n",
    "    def log(message):\n",
    "        \"\"\"Log message to file and print to console.\"\"\"\n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(f\"{pd.Timestamp.now()}: {message}\\n\")\n",
    "        print(message)\n",
    "    \n",
    "    log(\"=\" * 80)\n",
    "    log(f\"Starting Monthly-Only GDP Forecasting Workflow at {pd.Timestamp.now()}\")\n",
    "    log(\"=\" * 80)\n",
    "    \n",
    "    # 1. Configuration - only monthly and quarterly data\n",
    "    log(\"\\n1. Setting up configuration...\")\n",
    "    \n",
    "    # Monthly data configuration\n",
    "    monthly_config = {\n",
    "        'monthly': {\n",
    "            'files': {\n",
    "                'CPI_mon_monthly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1955-01-01'\n",
    "                },\n",
    "                'Unemployment_monthly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'diff']},\n",
    "                    'start_date': '1948-01-01'\n",
    "                },\n",
    "                'InterestRate_monthly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'diff']},\n",
    "                    'start_date': '1954-01-01'\n",
    "                },\n",
    "                'HousingStarts_monthly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1959-01-01'\n",
    "                },\n",
    "                'Heavy_Truck_Sales.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1967-01-01'\n",
    "                },\n",
    "                'Manufacturing_Production_Motor_and_Vehicle_Parts.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1972-01-01'\n",
    "                },\n",
    "                'Consumer_Confidence.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'diff']},\n",
    "                    'start_date': '1960-01-01'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Quarterly data configuration\n",
    "    quarterly_config = {\n",
    "        'quarterly': {\n",
    "            'files': {\n",
    "                'GDP_quaterly.csv': {\n",
    "                    'columns': ['Value'],\n",
    "                    'transformations': {'Value': ['raw', 'pct_change']},\n",
    "                    'start_date': '1947-01-01'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Combine configurations\n",
    "    data_config = {}\n",
    "    data_config.update(monthly_config)\n",
    "    data_config.update(quarterly_config)\n",
    "    \n",
    "    log(f\"Configuration set up with {len(monthly_config['monthly']['files'])} monthly files, \" +\n",
    "        f\"{len(quarterly_config['quarterly']['files'])} quarterly files\")\n",
    "    \n",
    "    # 2. Data Preprocessing\n",
    "    log(\"\\n2. Data Preprocessing...\")\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = MultiFrequencyPreprocessor(data_folder)\n",
    "    preprocessor.set_config(data_config)\n",
    "    \n",
    "    # Set date range if provided\n",
    "    if start_date is not None:\n",
    "        preprocessor.set_date_range(start_date=start_date)\n",
    "    if end_date is not None:\n",
    "        preprocessor.set_date_range(end_date=end_date)\n",
    "    \n",
    "    # Process data for monthly and quarterly only\n",
    "    monthly_df = preprocessor.process_frequency_data('monthly')\n",
    "    trace_nans(\"Raw monthly data\", monthly_df)\n",
    "    quarterly_df = preprocessor.process_frequency_data('quarterly')\n",
    "    trace_nans(\"Raw quarterly data\", quarterly_df)\n",
    "    \n",
    "    log(f\"Processed data: monthly={monthly_df.shape}, quarterly={quarterly_df.shape}\")\n",
    "    \n",
    "    # Plot data overview\n",
    "    try:\n",
    "        fig = preprocessor.plot_data_overview()\n",
    "        fig.savefig(os.path.join(output_folder, 'data_overview.png'))\n",
    "        plt.close(fig)\n",
    "        log(f\"Data overview saved to {os.path.join(output_folder, 'data_overview.png')}\")\n",
    "    except Exception as e:\n",
    "        log(f\"Warning: Could not create data overview plot: {e}\")\n",
    "    \n",
    "    # 3. Technical Indicators\n",
    "    log(\"\\n3. Calculating Technical Indicators...\")\n",
    "    \n",
    "    # Initialize technical indicators calculator\n",
    "    tech_indicators = MultiFrequencyTechnicalIndicators()\n",
    "    \n",
    "    # Calculate technical indicators for monthly and quarterly\n",
    "    monthly_indicators = tech_indicators.apply_indicators(monthly_df, frequency='monthly')\n",
    "    trace_nans(\"Monthly data before indicators\", monthly_df)\n",
    "    quarterly_indicators = tech_indicators.apply_indicators(quarterly_df, frequency='quarterly')\n",
    "    trace_nans(\"Monthly data after indicators\", monthly_indicators)\n",
    "    \n",
    "    log(f\"Calculated technical indicators: monthly={monthly_indicators.shape}, \" +\n",
    "        f\"quarterly={quarterly_indicators.shape}\")\n",
    "    \n",
    "    # 4. Data Alignment for Model\n",
    "    log(\"\\n4. Aligning Data for Model...\")\n",
    "    \n",
    "    # Get GDP target series\n",
    "    gdp_target = quarterly_df['GDP_quaterly_Value_pct_change']\n",
    "    \n",
    "    # Align monthly data to quarterly dates\n",
    "    monthly_to_quarterly = preprocessor.align_to_dates(monthly_indicators, gdp_target.index, method='last')\n",
    "    trace_nans(\"Monthly data after alignment to quarterly\", monthly_to_quarterly)\n",
    "    log(f\"Aligned monthly to quarterly: {monthly_to_quarterly.shape}\")\n",
    "    \n",
    "    # 5. Train-Test Split\n",
    "    log(\"\\n5. Creating Train-Test Split...\")\n",
    "    \n",
    "    # Determine split point\n",
    "    n_quarters = len(gdp_target)\n",
    "    n_train = int(n_quarters * train_test_split)\n",
    "    split_date = gdp_target.index[n_train]\n",
    "    \n",
    "    # Split GDP data\n",
    "    train_gdp = gdp_target.iloc[:n_train]\n",
    "    test_gdp = gdp_target.iloc[n_train:]\n",
    "    \n",
    "    # Split aligned data\n",
    "    train_monthly_aligned = monthly_to_quarterly.loc[train_gdp.index]\n",
    "    test_monthly_aligned = monthly_to_quarterly.loc[test_gdp.index]\n",
    "\n",
    "    trace_nans(\"Training monthly aligned data\", train_monthly_aligned)\n",
    "    trace_nans(\"Training GDP data\", train_gdp)\n",
    "    \n",
    "    log(f\"Train-test split at {split_date}: train={len(train_gdp)}, test={len(test_gdp)}\")\n",
    "    \n",
    "    # 6. Model Building\n",
    "    log(\"\\n6. Building Models...\")\n",
    "    \n",
    "    # Initialize models dictionary\n",
    "    models = {}\n",
    "    \n",
    "    # 6.1. Monthly-to-Quarterly Model with MIDAS option\n",
    "    log(\"Building Monthly-to-Quarterly Model...\")\n",
    "    try:\n",
    "        # Initialize predictor\n",
    "        monthly_model = MonthlyGDPPredictor(\n",
    "            monthly_factors=monthly_factors,\n",
    "            gdp_ar_lags=gdp_ar_lags,\n",
    "            use_midas=use_midas,\n",
    "            midas_max_lags=6,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Fit the model with monthly data\n",
    "        monthly_model.fit(\n",
    "            monthly_df=monthly_indicators,\n",
    "            gdp_series=train_gdp,\n",
    "            align_dates=True,\n",
    "            use_ar=True\n",
    "        )\n",
    "        \n",
    "        # Store in models dictionary\n",
    "        model_name = \"Monthly_MIDAS\" if use_midas else \"Monthly_Direct\"\n",
    "        models[model_name] = monthly_model\n",
    "        \n",
    "        log(f\"{model_name} Model successfully built\")\n",
    "        \n",
    "        # Save model if requested\n",
    "        if save_models:\n",
    "            model_path = os.path.join(output_folder, f'{model_name.lower()}_model.pkl')\n",
    "            with open(model_path, 'wb') as f:\n",
    "                pickle.dump(monthly_model, f)\n",
    "            log(f\"{model_name} Model saved to {model_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        log(f\"Error building Monthly-to-Quarterly Model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # 6.2. Baseline Models\n",
    "    log(\"Building Baseline Models...\")\n",
    "    \n",
    "    # 6.2.1. AR Model (autoregressive)\n",
    "    try:\n",
    "        # Create lag features\n",
    "        X_ar = pd.DataFrame(index=train_gdp.index)\n",
    "        for lag in range(1, gdp_ar_lags + 1):\n",
    "            X_ar[f'lag_{lag}'] = train_gdp.shift(lag)\n",
    "        \n",
    "        # Drop rows with NaN values\n",
    "        valid_rows = ~X_ar.isna().any(axis=1)\n",
    "        X_ar_valid = X_ar[valid_rows]\n",
    "        y_ar_valid = train_gdp[valid_rows]\n",
    "        \n",
    "        # Fit AR model\n",
    "        ar_model = Ridge(alpha=0.1, random_state=random_state)\n",
    "        ar_model.fit(X_ar_valid, y_ar_valid)\n",
    "        \n",
    "        # Store model\n",
    "        models['AR_Baseline'] = ar_model\n",
    "        log(\"AR Baseline Model successfully built\")\n",
    "        \n",
    "        # Save lag columns for prediction\n",
    "        models['AR_lag_columns'] = X_ar.columns.tolist()\n",
    "    except Exception as e:\n",
    "        log(f\"Error building AR Baseline Model: {e}\")\n",
    "    \n",
    "    # 6.2.2. MA Model (moving average of previous quarters)\n",
    "    try:\n",
    "        # Create different MA versions\n",
    "        ma_windows = [4]  # 1-year moving average\n",
    "        for window in ma_windows:\n",
    "            ma_model = {'window': window}\n",
    "            models[f'MA_{window}_Baseline'] = ma_model\n",
    "            log(f\"MA-{window} Baseline Model defined\")\n",
    "    except Exception as e:\n",
    "        log(f\"Error defining MA Baseline Models: {e}\")\n",
    "    \n",
    "    # 7. Model Evaluation\n",
    "    log(\"\\n7. Evaluating Models...\")\n",
    "    \n",
    "    # Create evaluator\n",
    "    evaluator = GDPForecastEvaluator()\n",
    "    \n",
    "    # Set actual values\n",
    "    evaluator.add_model('Actual', test_gdp, test_gdp)\n",
    "    \n",
    "    # Generate predictions for each model\n",
    "    for model_name, model in models.items():\n",
    "        try:\n",
    "            if model_name in [\"Monthly_MIDAS\", \"Monthly_Direct\"]:\n",
    "                # Generate predictions using the monthly model\n",
    "                predictions = model.predict(\n",
    "                    monthly_df=monthly_indicators,\n",
    "                    gdp_history=gdp_target,\n",
    "                    predict_date=None  # Use all data\n",
    "                )\n",
    "                \n",
    "                # Filter to test period\n",
    "                test_predictions = predictions.loc[test_gdp.index]\n",
    "                evaluator.add_model(model_name, test_predictions)\n",
    "                log(f\"Generated predictions for {model_name}: {len(test_predictions)} quarters\")\n",
    "            \n",
    "            elif model_name == 'AR_Baseline':\n",
    "                # Create features for test period\n",
    "                X_ar_test = pd.DataFrame(index=test_gdp.index)\n",
    "                for lag, col in enumerate(models['AR_lag_columns'], 1):\n",
    "                    X_ar_test[col] = gdp_target.shift(lag).loc[test_gdp.index]\n",
    "                \n",
    "                # Make predictions\n",
    "                ar_predictions = pd.Series(\n",
    "                    model.predict(X_ar_test),\n",
    "                    index=test_gdp.index,\n",
    "                    name=model_name\n",
    "                )\n",
    "                evaluator.add_model(model_name, ar_predictions)\n",
    "                log(f\"Generated predictions for {model_name}: {len(ar_predictions)} quarters\")\n",
    "            \n",
    "            elif 'MA_' in model_name:\n",
    "                # Get window size from model\n",
    "                window = model['window']\n",
    "                \n",
    "                # Calculate moving average for each test point\n",
    "                ma_predictions = pd.Series(index=test_gdp.index)\n",
    "                for i, date in enumerate(test_gdp.index):\n",
    "                    # Get previous window periods\n",
    "                    hist_data = gdp_target[gdp_target.index < date]\n",
    "                    if len(hist_data) >= window:\n",
    "                        ma_predictions[date] = hist_data[-window:].mean()\n",
    "                    else:\n",
    "                        # Use all available data if less than window\n",
    "                        ma_predictions[date] = hist_data.mean() if len(hist_data) > 0 else np.nan\n",
    "                \n",
    "                # Fill any missing values\n",
    "                ma_predictions = ma_predictions.fillna(method='ffill').fillna(0)\n",
    "                evaluator.add_model(model_name, ma_predictions)\n",
    "                log(f\"Generated predictions for {model_name}: {len(ma_predictions)} quarters\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            log(f\"Error generating predictions for {model_name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    log(\"Calculating evaluation metrics...\")\n",
    "    metrics = evaluator.calculate_metrics(rolling_window=8)\n",
    "    \n",
    "    # Output key metrics\n",
    "    log(\"\\nKey Performance Metrics:\")\n",
    "    log(\"-\" * 80)\n",
    "    log(f\"{'Model':<25} {'RMSE':>10} {'MAE':>10} {'Dir Acc':>10}\")\n",
    "    log(\"-\" * 80)\n",
    "    for model_name, model_metrics in metrics.items():\n",
    "        if model_name != 'Actual':\n",
    "            log(f\"{model_name:<25} {model_metrics['rmse']:>10.4f} {model_metrics['mae']:>10.4f} {model_metrics['direction_accuracy']:>10.4f}\")\n",
    "    \n",
    "    # Create plots\n",
    "    log(\"\\nGenerating evaluation plots...\")\n",
    "    try:\n",
    "        # Forecasts plot\n",
    "        fig = evaluator.plot_forecasts()\n",
    "        fig.savefig(os.path.join(output_folder, 'gdp_forecasts.png'))\n",
    "        plt.close(fig)\n",
    "        log(f\"Forecasts plot saved to {os.path.join(output_folder, 'gdp_forecasts.png')}\")\n",
    "        \n",
    "        # Error distribution plot\n",
    "        fig = evaluator.plot_error_distribution()\n",
    "        fig.savefig(os.path.join(output_folder, 'error_distribution.png'))\n",
    "        plt.close(fig)\n",
    "        log(f\"Error distribution plot saved to {os.path.join(output_folder, 'error_distribution.png')}\")\n",
    "        \n",
    "        # Rolling metrics plot\n",
    "        fig = evaluator.plot_rolling_metrics()\n",
    "        fig.savefig(os.path.join(output_folder, 'rolling_metrics.png'))\n",
    "        plt.close(fig)\n",
    "        log(f\"Rolling metrics plot saved to {os.path.join(output_folder, 'rolling_metrics.png')}\")\n",
    "    except Exception as e:\n",
    "        log(f\"Error generating evaluation plots: {e}\")\n",
    "    \n",
    "    # 8. Generate comprehensive report\n",
    "    log(\"\\n8. Generating Final Report...\")\n",
    "    try:\n",
    "        report_path = os.path.join(output_folder, 'gdp_forecast_evaluation.md')\n",
    "        report_content = evaluator.generate_report(report_path, include_plots=True)\n",
    "        log(f\"Comprehensive evaluation report saved to {report_path}\")\n",
    "    except Exception as e:\n",
    "        log(f\"Error generating evaluation report: {e}\")\n",
    "    \n",
    "    # 9. Conclusion\n",
    "    log(\"\\n9. Workflow Completed\")\n",
    "    log(\"=\" * 80)\n",
    "    log(f\"Monthly-Only GDP Forecasting Workflow completed at {pd.Timestamp.now()}\")\n",
    "    log(\"=\" * 80)\n",
    "    \n",
    "    return evaluator, models, preprocessor\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set parameters\n",
    "    DATA_FOLDER = \"./Project_Data\"\n",
    "    OUTPUT_FOLDER = \"./output\"\n",
    "    \n",
    "    # Run the workflow\n",
    "    evaluator, models, preprocessor = run_gdp_forecast_workflow_monthly_only(\n",
    "        data_folder=DATA_FOLDER,\n",
    "        output_folder=OUTPUT_FOLDER,\n",
    "        start_date='1980-01-01',  # Start date for analysis\n",
    "        end_date=None,  # End date (use None for all available data)\n",
    "        train_test_split=0.8,  # Use 80% of data for training\n",
    "        use_midas=True,  # Use MIDAS for final GDP prediction\n",
    "        monthly_factors=3,  # Number of monthly factors\n",
    "        gdp_ar_lags=4,  # Number of AR lags for GDP\n",
    "        random_state=42,  # For reproducibility\n",
    "        save_models=True  # Save models to files\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4930dd51-1b32-46aa-b360-35ec8f820cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting Advanced GDP Forecasting Workflow at 2025-05-09 14:19:10.866593\n",
      "================================================================================\n",
      "\n",
      "1. Setting up configuration...\n",
      "Configuration set up with 7 monthly files, 1 quarterly files\n",
      "\n",
      "2. Data Preprocessing...\n",
      "Found 18 files in ./Project_Data\n",
      "Processing monthly data...\n",
      "Processed CPI_mon_monthly.csv: 829 observations, 2 features\n",
      "Processed Unemployment_monthly.csv: 925 observations, 2 features\n",
      "Processed InterestRate_monthly.csv: 847 observations, 2 features\n",
      "Processed HousingStarts_monthly.csv: 792 observations, 2 features\n",
      "Processed Heavy_Truck_Sales.csv: 698 observations, 2 features\n",
      "Processed Manufacturing_Production_Motor_and_Vehicle_Parts.csv: 637 observations, 2 features\n",
      "Processed Consumer_Confidence.csv: 768 observations, 2 features\n",
      "Final monthly dataset: 926 observations, 14 features\n",
      "Processing quarterly data...\n",
      "Processed GDP_quaterly.csv: 311 observations, 2 features\n",
      "Final quarterly dataset: 311 observations, 2 features\n",
      "Processed data: monthly=(926, 14), quarterly=(311, 2)\n",
      "\n",
      "3. Initializing Advanced GDP Forecasting System...\n",
      "TensorFlow not available. Will use RandomForest only.\n",
      "\n",
      "================================================================================\n",
      "Advanced GDP Forecasting System - Training Phase\n",
      "================================================================================\n",
      "\n",
      "1. Data Overview:\n",
      "   - Monthly features: 14 variables, 926 time periods\n",
      "   - Quarterly features: 1 variables, 311 time periods\n",
      "   - Target variable: GDP_quaterly_Value_pct_change with 311 observations\n",
      "\n",
      "2. Train-Test Split:\n",
      "   - Training period: 1947-03-31 00:00:00 to 2009-03-31 00:00:00 (249 quarters)\n",
      "   - Testing period: 2009-06-30 00:00:00 to 2024-09-30 00:00:00 (62 quarters)\n",
      "\n",
      "3. Feature Selection:\n",
      "Processing 14 monthly features and 1 quarterly features\n",
      "Cleaning data by handling infinities and outliers...\n",
      "  Column CPI_mon_monthly_Value_pct_change_last contains 24 infinity values - replacing\n",
      "  Column CPI_mon_monthly_Value_pct_change_mean contains 54 infinity values - replacing\n",
      "Applying rf_importance feature selection method...\n",
      "Selected 49 monthly-derived features and 1 quarterly features\n",
      "   - Selected 50 features in total\n",
      "\n",
      "4. Model Training:\n",
      "   - Training Quantile Regression Forest...\n",
      "\n",
      "Training completed successfully.\n",
      "\n",
      "4. Generating GDP forecasts...\n",
      "\n",
      "5. Evaluating forecast performance...\n",
      "\n",
      "Key Statistical Metrics:\n",
      "--------------------------------------------------------------------------------\n",
      "Model                 RMSE        MAE    Dir Acc\n",
      "--------------------------------------------------------------------------------\n",
      "qrf                 1.7048     0.7959     0.7049\n",
      "\n",
      "Economic Value Metrics:\n",
      "--------------------------------------------------------------------------------\n",
      "Model               Return     Sharpe    Utility\n",
      "--------------------------------------------------------------------------------\n",
      "qrf                 0.6157     0.7013    -1.3111\n",
      "\n",
      "6. Creating visualization plots...\n",
      "pandas_datareader not available for recession shading\n",
      "Forecast plot saved to ./output/advanced/advanced_gdp_forecasts.png\n",
      "Feature importance plot saved to ./output/advanced/feature_importance.png\n",
      "\n",
      "7. Generating comprehensive report...\n",
      "Report saved to ./output/advanced/advanced_gdp_forecast_report.md\n",
      "Comprehensive report saved to ./output/advanced/advanced_gdp_forecast_report.md\n",
      "\n",
      "9. Workflow completed\n",
      "================================================================================\n",
      "Advanced GDP Forecasting Workflow completed at 2025-05-09 14:19:19.913576\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
